## Bayesian Updating for Sampled Nonlinear Models

### Introdução
Este capítulo aprofunda a aplicação do *Bayesian updating* em modelos não lineares amostrados, um tema crucial em *adaptive learning*. Construindo sobre os conceitos de *belief models*, *frequentist view* e *Bayesian view* [^31, ^32, ^33, ^34], exploraremos como o *Bayesian updating* pode ser aplicado para refinar nossas estimativas de parâmetros em modelos onde a relação entre as variáveis não é linear. O *Bayesian updating* fornece uma estrutura para incorporar conhecimento prévio (*prior distribution*) com novas observações para formar uma estimativa *posterior* mais precisa [^31, ^34]. Este capítulo irá detalhar as equações de atualização para modelos independentes e correlacionados, culminando na aplicação em modelos não lineares amostrados [^35, ^36, ^37, ^38, ^39].

### Conceitos Fundamentais
O *Bayesian updating* é uma técnica estatística que permite a atualização iterativa de crenças sobre parâmetros desconhecidos com base em novas evidências [^31, ^34]. A essência do *Bayesian updating* reside no teorema de Bayes, que quantifica como as probabilidades devem ser atualizadas à luz de novas informações [^11].

**Teorema de Bayes:**
$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$
onde:
*   $P(A|B)$ é a probabilidade *posterior* do evento A dado o evento B
*   $P(B|A)$ é a *likelihood* do evento B dado o evento A
*   $P(A)$ é a probabilidade *prior* do evento A
*   $P(B)$ é a probabilidade marginal do evento B

**Atualização Bayesiana para Modelos Independentes:**
Em muitos casos, assume-se que as observações são independentes. As equações de atualização para modelos independentes são apresentadas em [^35]:

$$\mu^{n+1} = \frac{\beta^n \mu^n + \beta_W W^{n+1}}{\beta^n + \beta_W}$$

$$\beta^{n+1} = \beta^n + \beta_W$$

onde:
*   $\mu^n$ é a média estimada do parâmetro $\mu$ após $n$ observações
*   $\beta^n$ é a precisão da estimativa de $\mu$ após $n$ observações
*   $W^{n+1}$ é a $(n+1)$-ésima observação
*   $\beta_W$ é a precisão da observação $W^{n+1}$

**Atualização Bayesiana para Priors Normais Correlacionados:**
Quando as crenças sobre diferentes escolhas são correlacionadas, a atualização bayesiana torna-se mais complexa [^37, ^38]. A equação para atualizar o vetor de crenças $\mu^n$ na presença de crenças correlacionadas é:

$$\mu^{n+1} = (B^{n+1})^{-1} (B^n \mu^n + \beta_W W^{n+1} e_{x_n})$$

onde:
*   $B^n$ é a matriz de precisão (inversa da matriz de covariância)
*   $e_{x_n}$ é um vetor coluna com 1 no elemento correspondente à alternativa medida $x_n$ e 0 em outros lugares
*   $\beta_W$ é a precisão do resultado experimental $W$ [^38]

A matriz de precisão também é atualizada:
$$B^{n+1} = B^n + \beta_W e_{x_n} (e_{x_n})^T$$

Essas equações levam em conta a correlação entre as crenças sobre diferentes alternativas, permitindo que a informação de uma observação atualize as crenças sobre alternativas relacionadas [^38].

**Modelos Não Lineares Amostrados:**
Muitas aplicações exigem modelos não lineares. A seção 2.4 [^41] introduz o conceito de *sampled belief model* para lidar com modelos não lineares. Em um *sampled belief model*, assume-se que o parâmetro $\theta$ pertence a um conjunto discreto $\Theta = \{\theta_1, ..., \theta_K\}$. A probabilidade de $\theta = \theta_k$ é denotada por $P[\theta = \theta_k] = p_k$. As equações de atualização para o vetor de probabilidade $p^n$ após observar $y^{n+1}$ são dadas pelo teorema de Bayes condicionado na história $H^n$:

$$P[\theta = \theta_k | y^{n+1} = y, H^n] = \frac{P[y^{n+1} = y | \theta_k, H^n] P[\theta = \theta_k | H^n]}{P[y^{n+1} = y | H^n]}$$

Esta equação pode ser reescrita como [^11, ^12]:

$$p_k^{n+1} = \frac{f_y(y^{n+1} = y | \theta_k) p_k^n}{\sum_{k=1}^{K} f_y(y^{n+1} = y | \theta_k) p_k^n}$$

onde $f_y(y^{n+1} = y | \theta_k)$ é a função de distribuição da observação aleatória $y$ dado $\theta_k$.

### Conclusão

Este capítulo forneceu uma visão aprofundada do *Bayesian updating* e sua aplicação em modelos não lineares amostrados. A combinação do conhecimento prévio com novas observações usando o teorema de Bayes permite aprimorar iterativamente as estimativas de parâmetros [^31, ^34]. As equações de atualização para modelos independentes e correlacionados fornecem uma estrutura flexível para lidar com uma ampla gama de problemas [^35, ^36, ^37, ^38, ^39]. O conceito de *sampled belief model* permite aplicar o *Bayesian updating* a modelos não lineares, tornando-o uma ferramenta poderosa para problemas complexos de *adaptive learning* [^41].

### Referências
[^31]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 31)
[^32]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 32)
[^33]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 33)
[^34]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 34)
[^35]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 35)
[^36]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 36)
[^37]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 37)
[^38]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 38)
[^39]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 39)
[^41]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 41)
[^11]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 11)
[^12]: Powell, W. B., & Ryzhov, I. O. (2018). *Optimal Learning*. John Wiley & Sons, Inc. (p. 12)
$\blacksquare$
<!-- END -->