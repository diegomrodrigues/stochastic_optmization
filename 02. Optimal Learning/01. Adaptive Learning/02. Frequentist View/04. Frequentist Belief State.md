## A Visão Frequentista da Função de Crença

### Introdução
Este capítulo explora a visão frequentista da função de crença, contrastando-a com a abordagem Bayesiana, que será abordada em capítulos posteriores. A visão frequentista, conforme introduzida no Capítulo 2 [^31], parte do princípio de que não possuímos conhecimento prévio sobre os parâmetros de um sistema e que nossas crenças são formadas exclusivamente a partir dos resultados de experimentos repetidos. A representação da crença, portanto, evolui com base nas observações coletadas ao longo do tempo [^33].

### Conceitos Fundamentais
Na visão frequentista, o estado de crença é capturado por meio de estatísticas descritivas, como a média, a variância e o tamanho da amostra [^34]. Formalmente, o estado de crença frequentista no instante *n* é representado como $B_{freq,n} = (\mu_n, \hat{\sigma}^2_n, n)$ [^34], onde:

*   $\mu_n$ representa a **média** das observações até o instante *n*, calculada como:

    $$\
    \mu_n = \frac{1}{n} \sum_{m=1}^{n} W_m
    $$

    em que $W_m$ é a *m*-ésima observação [^33].
*   $\hat{\sigma}^2_n$ representa a **variância** amostral das observações, que quantifica a dispersão dos dados em torno da média [^33]. Ela é calculada como:

    $$\
    \hat{\sigma}^2_n = \frac{1}{n-1} \sum_{m=1}^{n} (W_m - \mu_n)^2
    $$
*   *n* representa o **tamanho da amostra**, ou seja, o número de observações utilizadas para estimar os parâmetros [^34].

**Observação Importante:** É crucial notar que $\mu_n$ e $\hat{\sigma}^2_n$ são *estimativas* dos verdadeiros valores da média e da variância da variável aleatória *W*.

**Atualização Recursiva:** A média e a variância podem ser atualizadas recursivamente a cada nova observação, evitando a necessidade de recalcular as somas a cada instante [^34]. As equações de atualização recursiva são dadas por:

$$\
\mu_n = \left(1 - \frac{1}{n}\right) \mu_{n-1} + \frac{1}{n} W_n \qquad [^34]
$$

$$\
\hat{\sigma}^2_n =
\begin{cases}
\frac{1}{n-2} \hat{\sigma}^2_{n-1} + \frac{n}{n-1} (W_n - \mu_{n-1})^2, & n > 2 \\\\
0, & n = 2
\end{cases} \qquad [^34]
$$

**Distribuição de Probabilidade:** Embora $B_{freq,n}$ seja composto por estatísticas, ele pode ser interpretado como uma aproximação da distribuição de probabilidade subjacente [^34]. Sob certas condições, como o cumprimento do teorema do limite central (Central Limit Theorem), a distribuição da média amostral $\mu_n$ se aproxima de uma distribuição normal à medida que o tamanho da amostra *n* aumenta [^34]. Assim, podemos aproximar a distribuição de probabilidade da variável aleatória *W* por uma distribuição normal com média $\mu_n$ e variância $\hat{\sigma}^2_n$.

**Melhor Estimativa da Variância do Estimador $\mu_n$**: A melhor estimativa da variância do estimador $\mu_n$ é dada por:

$$\
\hat{\sigma}^2_{\mu_n} = \frac{\hat{\sigma}^2_n}{n} [^33]
$$

**Consistência Assintótica:** É importante notar que, à medida que o tamanho da amostra *n* tende ao infinito, $\hat{\sigma}^2_{\mu_n}$ tende a zero, enquanto $\hat{\sigma}^2_n$ tende à variância verdadeira $\sigma^2$ de *W* [^33].

**Belief State:** Dado um conjunto de observações, o estado de crença é representado como:

$$\
B_{freq,n} = (\mu_n, \hat{\sigma}^2_{\mu_n}, n) [^34]
$$

**Diferenças para a Visão Bayesiana:** Na visão Bayesiana, a verdade $\mu_x$ é tratada como uma variável aleatória, modelada por uma distribuição normal com média $\mu_{0}$ e variância $\hat{\sigma}^2_{0}$ [^34]. Com a coleta de dados, essa distribuição muda, e a variância tende a diminuir [^34]. Já na visão frequentista, a verdade é um número desconhecido, e a estimativa desse número é uma variável aleatória que reflete a variação das observações [^34].

### Conclusão
A visão frequentista oferece uma abordagem pragmática para a formação de crenças, baseada exclusivamente em dados observados. A representação do estado de crença por meio da média, variância e tamanho da amostra permite uma fácil atualização à medida que novas informações se tornam disponíveis. A aproximação da distribuição de probabilidade por uma normal, sob certas condições, facilita a tomada de decisões e a quantificação da incerteza. No entanto, a ausência de conhecimento prévio pode ser uma limitação em cenários com dados escassos ou experimentos dispendiosos, onde a visão Bayesiana pode ser mais vantajosa [^34].

### Referências
[^31]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal learning*. John Wiley & Sons, Inc. Capítulo 2, p. 31.
[^33]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal learning*. John Wiley & Sons, Inc. Capítulo 2, p. 33.
[^34]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal learning*. John Wiley & Sons, Inc. Capítulo 2, p. 34.
<!-- END -->