## Correlated Beliefs in Optimal Learning with Normal Priors

### Introdução
Em problemas de aprendizado ótimo, a capacidade de generalizar informações de uma única observação para outras alternativas não medidas é crucial, especialmente quando lidamos com múltiplas escolhas. **Correlated beliefs** [^38] surgem como uma ferramenta poderosa nesse contexto, permitindo que a informação obtida de uma alternativa influencie as crenças sobre outras, otimizando o processo de tomada de decisão [^38]. Este capítulo explora em profundidade o conceito de *correlated beliefs* no contexto de **Updating for Correlated Normal Priors**, detalhando as formulações matemáticas e as implicações práticas dessa abordagem [^38].

### Conceitos Fundamentais
Em cenários com múltiplas escolhas, as crenças sobre diferentes alternativas frequentemente exibem correlação, exigindo a consideração da covariância entre elas [^38]. Ignorar essas correlações pode levar a decisões subótimas, pois a informação de uma alternativa pode fornecer *insights* valiosos sobre outras [^38].

Para formalizar o conceito, seja $\\mu_x$ a crença sobre a alternativa $x$ após $n$ experimentos [^38]. A **covariância** entre as crenças sobre as alternativas $x$ e $y$ é definida como:

$Cov^n(\\mu_x, \\mu_y)$ [^38].

A **matriz de covariância** $\\Sigma^n$ é uma matriz cujos elementos são as covariâncias entre todas as combinações de alternativas:

$\\Sigma^n_{xy} = Cov^n(\\mu_x, \\mu_y)$ [^38].

A **matriz de precisão** $B^n$ é o inverso da matriz de covariância:

$B^n = (\\Sigma^n)^{-1}$ [^38].

A matriz de precisão desempenha um papel fundamental nas equações de atualização bayesiana para crenças correlacionadas [^38].

Para atualizar as crenças em face de uma nova observação, utilizamos as seguintes equações [^38]:

$\\mu^{n+1} = (B^{n+1})^{-1} (B^n \\mu^n + \\beta_W W^{n+1} e_{x_n})$

$B^{n+1} = B^n + \\beta_W e_{x_n} (e_{x_n})^T$

onde:
*   $W^{n+1}$ é a observação escalar quando medimos a alternativa $x_n$ [^38].
*   $e_{x_n}$ é um vetor coluna de zeros com um 1 no elemento $x_n$ [^38].
*   $\\beta_W$ é a precisão do resultado experimental $W$ [^38].

A atualização da matriz de precisão $B^{n+1}$ envolve a adição de um termo que reflete a informação obtida da observação $W^{n+1}$ na alternativa $x_n$ [^38]. A equação para $\\mu^{n+1}$ incorpora essa informação, ajustando as crenças sobre todas as alternativas com base na covariância entre elas e a alternativa observada [^38].

**Sherman-Morrison Formula**

Para evitar o cálculo direto da inversa da matriz de covariância, podemos utilizar a fórmula de Sherman-Morrison [^38]:

$[A + uu^T]^{-1} = A^{-1} - \\frac{A^{-1}uu^T A^{-1}}{1 + u^T A^{-1} u}$

Aplicando esta fórmula, as equações de atualização podem ser reescritas como:

$\\mu^{n+1}(x) = \\mu^n + \\frac{W^{n+1} - \\mu^n}{\\lambda_W + \\Sigma^n_{x_n x_n}} \\Sigma^n_{x x_n}$

$\\Sigma^{n+1}(x) = \\Sigma^n - \\frac{\\Sigma^n_{x x_n} (e_{x_n})^T \\Sigma^n}{\\lambda_W + \\Sigma^n_{x_n x_n}}$

onde $\\lambda_W = \\sigma^2_W = 1/\\beta_W$ é a variância do resultado experimental [^38].

Essas equações permitem atualizar as crenças e as covariâncias de forma eficiente, sem a necessidade de calcular inversas de matrizes [^38].

**Exemplo Ilustrativo**

Considere um cenário com três alternativas, onde o vetor de médias a priori é:

$\\mu^n = \\begin{bmatrix} 20 \\\\ 16 \\\\ 22 \\end{bmatrix}$

e a matriz de covariância é:

$\\Sigma^n = \\begin{bmatrix} 12 & 6 & 3 \\\\ 6 & 7 & 4 \\\\ 3 & 4 & 15 \\end{bmatrix}$

Assumindo que $\\lambda_W = 9$ e que escolhemos medir a alternativa $x = 3$ e observamos $W^{n+1} = 19$ [^39], podemos atualizar as médias de nossas crenças usando:

$\\mu^{n+1}(3) = \\begin{bmatrix} 20 \\\\ 16 \\\\ 22 \\end{bmatrix} + \\frac{19-22}{9+15} \\begin{bmatrix} 3 \\\\ 4 \\\\ 15 \\end{bmatrix} = \\begin{bmatrix} 19.625 \\\\ 15.500 \\\\ 20.125 \\end{bmatrix}$

De forma similar, a matriz de covariância é atualizada usando a equação correspondente [^40].

### Conclusão
O uso de *correlated beliefs* representa uma abordagem sofisticada para o aprendizado ótimo em problemas com múltiplas escolhas [^38]. Ao considerar a covariância entre as crenças sobre diferentes alternativas, podemos generalizar informações de uma única observação para outras alternativas não medidas, melhorando a eficiência do processo de tomada de decisão [^38]. As equações de atualização bayesiana apresentadas neste capítulo fornecem um arcabouço matemático para implementar essa abordagem, permitindo que os tomadores de decisão incorporem informações de forma eficiente e adaptativa [^38]. A aplicação da fórmula de Sherman-Morrison simplifica os cálculos, tornando a abordagem computacionalmente viável para problemas de grande escala [^38].

### Referências
[^38]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 38.
[^39]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 39.
[^40]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 40.
<!-- END -->