## Atualização Bayesiana para Priors Normais Correlacionados

### Introdução
Este capítulo explora a equação de atualização Bayesiana para crenças correlacionadas, um conceito fundamental em problemas de aprendizado adaptativo onde as crenças sobre múltiplas escolhas estão interligadas. Em continuidade ao conceito de **belief models** [^1, ^2], onde representamos não só a nossa estimativa de como uma função ou sistema irá responder a uma entrada controlável *x*, mas também a incerteza nessa estimativa, este capítulo aprofunda-se em como atualizar estas crenças quando elas são correlacionadas. Como vimos anteriormente, a visão Bayesiana começa com crenças iniciais sobre parâmetros, conhecidas como a **prior distribution** [^1]. Essas crenças são então combinadas com resultados experimentais para formar uma **posterior distribution**. Este capítulo expande este conceito para o caso onde as crenças sobre diferentes alternativas estão correlacionadas.

### Conceitos Fundamentais
A equação Bayesiana para atualizar um vetor de crenças *μ* em presença de crenças correlacionadas é dada por [^8]:
$$\
\mu_{n+1} = (B_{n+1})^{-1} (B_n \mu_n + \beta_W W_{n+1} e_{x_n})
$$
onde:
*   $B_n$ é a **precision matrix** no instante *n*. A matriz de precisão é definida como a inversa da matriz de covariância [^8]: $B^n = (\Sigma^n)^{-1}$.
*   $e_{x_n}$ é um vetor coluna de zeros com um 1 no elemento *x*, representando a alternativa que foi medida. Este vetor é importante para incorporar a observação específica à alternativa *x* [^8].
*   $W_{n+1}$ é a observação escalar quando a alternativa *x* é medida, representando o resultado experimental [^8].
*   $\beta_W = 1/\sigma_W^2$ é a precisão da observação experimental $W_{n+1}$ [^5, ^8].

A matriz $B_{n+1}$ é atualizada da seguinte forma [^8]:
$$\
B_{n+1} = B_n + \beta_W e_{x_n} (e_{x_n})^T
$$
onde $e_{x_n} (e_{x_n})^T$ é uma matriz de zeros com um um na linha *x*, coluna *x*.

**Observações Importantes:**

*   A atualização Bayesiana, neste contexto, incorpora as correlações entre as alternativas através da matriz de precisão $B_n$. Isto permite que a observação de uma alternativa influencie as crenças sobre outras alternativas correlacionadas [^8].
*   A precisão $\beta_W$ reflete a confiança na observação $W_{n+1}$. Uma alta precisão indica uma observação mais confiável [^5].
*   A equação de atualização pode ser computacionalmente intensiva devido à inversão da matriz $B_{n+1}$. No entanto, o Sherman-Morrison formula [^8] pode ser utilizada para evitar a inversão direta, tornando os cálculos mais eficientes. A fórmula de Sherman-Morrison é dada por [^8]:
    $$\
    [A + uu^T]^{-1} = A^{-1} - \frac{A^{-1}uu^T A^{-1}}{1 + u^T A^{-1} u}
    $$
    onde *A* é uma matriz invertível e *u* é um vetor coluna.

**Ilustração:**
Para ilustrar a aplicação da equação de atualização Bayesiana, considere o exemplo apresentado no texto [^9], com três alternativas e o seguinte vetor de médias iniciais:
$$\
\mu^n = \begin{bmatrix} 20 \\\\ 16 \\\\ 22 \end{bmatrix}
$$
Assumindo que $\beta_W = 1/\sigma_W^2 = 1/9$ e que a matriz de covariância $\Sigma^n$ é dada por [^9]:
$$\
\Sigma^n = \begin{bmatrix} 12 & 6 & 3 \\\\ 6 & 7 & 4 \\\\ 3 & 4 & 15 \end{bmatrix}
$$
e que escolhemos medir $x=3$ e observamos $W_{n+1} = 19$, então podemos atualizar as médias das nossas crenças usando [^9]:
$$\
\mu^{n+1}(3) = \begin{bmatrix} 20 \\\\ 16 \\\\ 22 \end{bmatrix} + \frac{19-22}{9+15} \begin{bmatrix} 3 \\\\ 4 \\\\ 15 \end{bmatrix} = \begin{bmatrix} 19.625 \\\\ 15.500 \\\\ 20.125 \end{bmatrix}
$$
A matriz de covariância é atualizada usando [^10]:
$$\
\Sigma^{n+1}(3) = \Sigma^n - \frac{\Sigma^n e_3 (e_3)^T \Sigma^n}{\beta_W + \Sigma^n_{33}}
$$

### Conclusão
A equação de atualização Bayesiana para crenças correlacionadas fornece um método para incorporar observações em modelos onde as crenças sobre diferentes alternativas estão interligadas. Este método é particularmente útil em problemas de aprendizado adaptativo, como otimização de preços, seleção de lineups em esportes, e tratamentos médicos, onde a informação obtida de uma alternativa pode ser generalizada para outras alternativas correlacionadas [^7, ^8]. A utilização da fórmula de Sherman-Morrison [^8] pode reduzir a carga computacional associada à inversão da matriz de precisão.

### Referências
[^1]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 31)
[^2]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 32)
[^3]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 33)
[^4]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 34)
[^5]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 35)
[^6]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 36)
[^7]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 37)
[^8]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 38)
[^9]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 39)
[^10]: Powell, W. B., & Ryzhov, I. O. (2012). *Optimal Learning*. John Wiley & Sons, Inc. (p. 40)
<!-- END -->