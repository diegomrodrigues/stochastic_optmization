## Atualização para Priors Normais Correlacionados: A Fórmula de Sherman-Morrison

### Introdução
Em problemas de aprendizado ótimo, uma classe particularmente importante envolve cenários onde existem múltiplas escolhas e nossas crenças sobre essas escolhas são correlacionadas [^37]. Para lidar com essas correlações, necessitamos de métodos eficientes para atualizar nossas crenças à medida que novas informações se tornam disponíveis. A fórmula de Sherman-Morrison surge como uma ferramenta valiosa nesse contexto, permitindo atualizações sem a necessidade de inverter explicitamente a matriz de covariância [^38]. Este capítulo explora o uso da fórmula de Sherman-Morrison na atualização de priors normais correlacionados, detalhando sua derivação e aplicação.

### Conceitos Fundamentais
Para começar, considere $\\mu_x$ como nossa crença sobre a alternativa *x* após *n* experimentos. Definimos a covariância entre as crenças sobre as alternativas *x* e *y* como $Cov^n(\\mu_x, \\mu_y)$ [^38]. A matriz de covariância $\\Sigma_n$ contém esses elementos de covariância, e definimos a matriz de precisão $B_n$ como a inversa da matriz de covariância:

$$B_n = (\\Sigma_n)^{-1}$$ [^38]

Seja $e_x$ um vetor coluna de zeros com um 1 no elemento *x*, e seja $W_{n+1}$ a observação escalar quando decidimos medir a alternativa *x*. Podemos interpretar a observação como um vetor coluna dado por $W_{n+1}e_x$. Mantendo em mente que $\\mu_n$ é um vetor coluna de nossas crenças sobre a expectativa de $\\mu$, a equação Bayesiana para atualizar este vetor na presença de crenças correlacionadas é dada por:

$$\\mu_{n+1} = (B_{n+1})^{-1}(B_n\\mu_n + \\beta_W W_{n+1}e_{x_n})$$ [^38]

onde $B_{n+1}$ é dado por:

$$B_{n+1} = (B_n + \\beta_W e_{x_n}(e_{x_n})^T)$$ [^38]

Aqui, $\\beta_W$ é um escalar que representa a precisão do resultado experimental *W*. Note que $e_x(e_x)^T$ é uma matriz de zeros com um um na linha *x*, coluna *x* [^38].

A **fórmula de Sherman-Morrison** permite realizar essas atualizações sem ter que lidar com a inversa da matriz de covariância [^38]. A fórmula é dada por:

$$[A + uu^T]^{-1} = A^{-1} - \\frac{A^{-1}uu^T A^{-1}}{1 + u^T A^{-1}u}$$ [^38]

onde *A* é uma matriz invertível (como $\\Sigma_n$) e *u* é um vetor coluna (como $e_x$).

Seja $\\sigma_W^2 = 1/\\beta_W$ a variância do resultado experimental $W_{n+1}$. Assumindo que a variância experimental é a mesma para todas as alternativas *x*, podemos substituir $\\beta_W$ por $\\lambda^W$ [^39]. Usando a fórmula de Sherman-Morrison e definindo $x = x_n$, podemos reescrever as equações de atualização como:

$$\\mu_{n+1}(x) = \\mu_n + \\frac{W_{n+1} - \\mu_n}{\\lambda^W + \\Sigma_{xx}} \\Sigma_x$$ [^39]

$$\\Sigma_{n+1}(x) = \\Sigma_n - \\frac{\\Sigma_n e_x(e_x)^T \\Sigma_n}{\\lambda^W + \\Sigma_{xx}}$$ [^39]

onde expressamos a dependência de $\\mu_{n+1}(x)$ e $\\Sigma_{n+1}(x)$ na alternativa *x* que escolhemos medir.

### Conclusão
A fórmula de Sherman-Morrison oferece um meio computacionalmente eficiente para atualizar priors normais correlacionados [^38]. Ao evitar a necessidade de inverter explicitamente a matriz de covariância a cada atualização, a fórmula reduz a complexidade computacional e torna o processo de atualização mais tratável, especialmente em problemas com um grande número de alternativas [^39]. Este método é particularmente útil em problemas de aprendizado ótimo, onde a capacidade de atualizar eficientemente as crenças é crucial para tomar decisões informadas [^37].

### Referências
[^37]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 37.
[^38]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 38.
[^39]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 39.
<!-- END -->