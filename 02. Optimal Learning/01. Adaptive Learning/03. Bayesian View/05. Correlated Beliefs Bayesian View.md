## Causal Inference and Correlated Beliefs in the Bayesian View

### Introdução
Este capítulo explora o conceito de **crenças correlacionadas** e sua aplicação na generalização de resultados de uma única observação para outras alternativas no contexto da visão Bayesiana [^38]. Em continuidade ao que foi discutido anteriormente sobre a atualização de crenças, vamos aprofundar como a matriz de covariância representa as relações entre as crenças sobre diferentes escolhas e como a fórmula de Sherman-Morrison facilita a inversão da matriz de covariância [^1].

### Conceitos Fundamentais

**Crenças correlacionadas** são uma ferramenta poderosa no aprendizado ótimo, permitindo generalizar os resultados de uma única observação para outras alternativas que não foram diretamente medidas [^38]. Isso é particularmente útil em problemas onde as escolhas são inter-relacionadas.

**Matriz de Covariância**: A relação entre as crenças sobre diferentes escolhas é representada pela **matriz de covariância** $\\Sigma^n$ [^38]. O elemento $\\Sigma^n_{xy}$ representa a covariância entre as crenças sobre as alternativas *x* e *y*.

**Matriz de Precisão**: A matriz de precisão $B^n$ é definida como a inversa da matriz de covariância [^38]:
$$\
B^n = (\\Sigma^n)^{-1}
$$\
A precisão representa o grau de certeza em nossas crenças.

**Atualização Bayesiana com Crenças Correlacionadas**: A equação Bayesiana para atualizar as crenças na presença de crenças correlacionadas é dada por [^38]:
$$\
\\mu^{n+1} = (B^{n+1})^{-1} (B^n \\mu^n + \\beta_W W^{n+1} e_{x_n})
$$\
onde:
- $\\mu^{n+1}$ é o vetor de crenças atualizado.
- $B^n$ é a matriz de precisão antes da observação.
- $\\mu^n$ é o vetor de crenças antes da observação.
- $\\beta_W$ é a precisão do resultado experimental *W*.
- $W^{n+1}$ é o resultado experimental da alternativa $x_n$.
- $e_{x_n}$ é um vetor coluna de zeros com um 1 no elemento correspondente à alternativa $x_n$.

**Fórmula de Sherman-Morrison**: Para evitar a inversão direta da matriz de covariância, a **fórmula de Sherman-Morrison** é utilizada [^38]:
$$\
[A + uu^T]^{-1} = A^{-1} - \\frac{A^{-1}uu^T A^{-1}}{1 + u^T A^{-1} u}
$$\
onde *A* é uma matriz invertível e *u* é um vetor coluna. Aplicando esta fórmula à atualização da matriz de precisão:
$$\
B^{n+1} = B^n + \\beta_W e_{x_n} (e_{x_n})^T
$$\
e usando a fórmula de Sherman-Morrison, a atualização do vetor de crenças pode ser expressa como [^39]:
$$\
\\mu^{n+1}(x) = \\mu^n + \\frac{W^{n+1} - \\mu^n}{\\lambda^W + \\Sigma^n_{xx}} \\Sigma^n
$$\
$$\
\\Sigma^{n+1}(x) = \\Sigma^n - \\frac{\\Sigma^n e_x (e_x)^T \\Sigma^n}{\\lambda^W + \\Sigma^n_{xx}}
$$\
onde $\\lambda^W = 1/\\beta_W$ é a variância do resultado experimental *W*.

**Exemplo Ilustrativo**: Considere três alternativas com o vetor de médias iniciais [^39]:
$$\
\\mu^0 = \\begin{bmatrix} 20 \\\\ 16 \\\\ 22 \\end{bmatrix}
$$\
e a matriz de covariância inicial [^39]:
$$\
\\Sigma^0 = \\begin{bmatrix} 12 & 6 & 3 \\\\ 6 & 7 & 4 \\\\ 3 & 4 & 15 \\end{bmatrix}
$$\
Se escolhermos medir a alternativa *x* = 3 e observarmos $W^{n+1}$ = 19, a atualização das médias das crenças é [^39]:
$$\
\\mu^{n+1}(3) = \\begin{bmatrix} 19.625 \\\\ 15.500 \\\\ 20.125 \\end{bmatrix}
$$\
e a atualização da matriz de covariância é [^40]:
$$\
\\Sigma^{n+1}(3) = \\begin{bmatrix} 11.625 & 5.500 & 1.125 \\\\ 5.500 & 6.333 & 1.500 \\\\ 1.125 & 1.500 & 5.625 \\end{bmatrix}
$$\

### Conclusão

A utilização de crenças correlacionadas, juntamente com a matriz de precisão e a fórmula de Sherman-Morrison, oferece um método eficiente para atualizar as crenças sobre múltiplas alternativas com base em uma única observação. Este método é particularmente útil em situações onde as alternativas são inter-relacionadas e a coleta de dados é dispendiosa [^38]. A capacidade de generalizar informações de uma observação para outras alternativas não medidas diretamente aumenta significativamente a eficiência do processo de aprendizado.

### Referências
[^1]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov Copyright© 2018 John Wiley & Sons, Inc.
[^38]: Correlated beliefs are a particularly powerful device in optimal learning, allowing us to generalize the results of a single observation to other alternatives that we have not directly measured. Let un be our belief about alternative x after n experiments. Now let Cου (μα, μy) = the covariance in our belief about με and μy. We let En be the covariance matrix, with element Eny = Covn (μα, μy). Just as we defined the precision Br to be the reciprocal of the variance, we are going to define the precision matrix Bn to be Β" = (Ση)-1. Let ex be a column vector of zeroes with a 1 for element x, and as before we let Wn+1 be the (scalar) observation when we decide to measure alternative x. We could label Wn+1 as Wn+1 to make the dependence on the alternative more explicit. For this discussion, we are going to use the notation that we choose to measure xn and the resulting observation is Wn+1. If we choose to measure xn, we can also interpret the observation as a column vector given by Wn+1ern. Keeping in mind that ū” is a column vector of our beliefs about the expectation of µ, the Bayesian equation for updating this vector in the presence of correlated beliefs is given by μη+1 = (Bn+1)-1 (Βημη + ẞWWn+1exn), (2.15) where Bn+1 is given by Bn+1 = (Bn + BW exn (exn)T). (2.16) Note that ex (ex)T is a matrix of zeroes with a one in row x, column x, whereas BW is a scalar giving the precision of our experimental outcome W.
[^39]: Using the Sherman-Morrison formula, and letting x = xn, we can rewrite the updating equations as μη+1(x) = μη + Wn+1 – μη Ση (2.18) AW + Σ Ση+1(x) Σηεx(ex)ΤΣη (2.19) Ση AW + Σ where we express the dependence of ūn+1(x) and ∑n+1(x) on the alternative x which we have chosen to measure. To illustrate, assume that we have three alternatives with mean vector Assume that XW μη 20 16 22 9 and that our covariance matrix En is given by 12 6 3 Ση = 6 7 4 3 4 15 Assume that we choose to measure x = 3 and observe Wn+1 = Wn+1 = 19. Applying equation (2.18), we update the means of our beliefs using μη+1(3) 20 12 6 3 0 19-22 16 + 6 7 4 0 9+15 22 3 4 15 1 20 3 -3 16 + 4 24 22 15 19.625 15.500 20.125
[^40]: The update of the covariance matrix is computed using 12 6 3 0 12 6 3 6 7 4 0 [001] 6 7 4 3 4 15 1 3 4 15 Ση+1(3) 12 6 3 6 7 4 9+15 3 4 15 12 6 3 9 12 45 1 6 7 4 12 16 60 24 3 4 15 45 60 225 12 6 3 0.375 0.500 1.875 6 7 4 0.500 0.667 2.500 3 4 15 1.875 2.500 9.375 11.625 5.500 1.125 5.500 6.333 1.500 1.125 1.500 5.625

<!-- END -->