## A Bayesian Perspective on Adaptive Learning

### Introdução
Este capítulo explora a visão Bayesiana no contexto do aprendizado adaptativo, com foco na representação da verdade como uma variável aleatória com sua própria distribuição [^34]. Como vimos anteriormente, a modelagem de crenças é fundamental para problemas de aprendizado. A abordagem Bayesiana oferece uma estrutura para incorporar conhecimento prévio e garantir uma redução constante na variância à medida que mais informações são coletadas [^34]. Este capítulo detalha os conceitos fundamentais da abordagem Bayesiana, as equações de atualização para crenças independentes e correlacionadas, e explora casos com priors não-Gaussianos.

### Conceitos Fundamentais
Na visão Bayesiana, o parâmetro verdadeiro $\\mu_\\alpha$ é tratado como uma **variável aleatória** [^34]. Isso contrasta com a visão frequentista, onde $\\mu_\\alpha$ é considerado um número desconhecido e fixo. A abordagem Bayesiana permite que atribuamos uma **distribuição inicial** (prior) a $\\mu_\\alpha$, refletindo nossas crenças prévias sobre seu valor [^34]. Por exemplo, podemos modelar $\\mu_\\alpha$ como uma distribuição normal com média $\\mu_0$ e variância $\\sigma_0^2$, ou seja, $\\mu_\\alpha \\sim N(\\mu_0, \\sigma_0^2)$ [^34].

À medida que coletamos dados (observações), atualizamos nossa distribuição *prior* para obter uma distribuição *posterior*. Este processo de atualização é feito utilizando o Teorema de Bayes, que combina a distribuição *prior* com a função de *likelihood* dos dados [^28].

>A principal vantagem da abordagem Bayesiana é a capacidade de incorporar conhecimento prévio e quantificar a incerteza sobre os parâmetros [^34].

A atualização Bayesiana garante que a variância da distribuição *posterior* diminua constantemente à medida que mais informações são coletadas [^34]. Isso reflete um aumento na certeza sobre o valor do parâmetro [^34].

**Equações de atualização para crenças independentes**
Assumindo que as observações $W$ são normalmente distribuídas com variância $\\sigma_w^2$ e definindo a precisão como $\\beta_w = \\frac{1}{\\sigma_w^2}$ [^35], as equações de atualização para a média $\\mu$ e precisão $\\beta$ após $n$ observações são dadas por [^35]:

$$ \\mu^{n+1} = \\frac{\\beta^n \\mu^n + \\beta_w W^{n+1}}{\\beta^n + \\beta_w} \\qquad (2.6)$$

$$ \\beta^{n+1} = \\beta^n + \\beta_w \\qquad (2.7)$$

Onde $\\mu^{n+1}$ e $\\beta^{n+1}$ são a média e a precisão *posterior*, respectivamente. Note que a equação (2.6) pode ser reescrita como:

$$\\mu^{n+1} = (\\beta^{n+1})^{-1} (\\beta^n \\mu^n + \\beta_w W^{n+1})  \\qquad (2.8)$$

Definindo $\\tilde{\\sigma}^2_n$ como a variância da estimativa de $\\mu^{n+1}$ dado o histórico de $n$ observações, podemos escrever:

$$\\tilde{\\sigma}^{2,n} = Var_n[\\mu^{n+1} - \\mu^n] \\qquad (2.10)$$

$$\\tilde{\\sigma}^{2,n} = \\frac{\\sigma_w^2 \\sigma^{2,n+1}}{\\sigma_w^2 + \\sigma^{2,n}} \\qquad (2.11)$$

Onde $\\sigma^{2,n}$ é a variância *a priori* no tempo $n$.

**Equações de atualização para crenças correlacionadas**

Em muitos problemas de aprendizado adaptativo, as crenças sobre diferentes alternativas são correlacionadas [^37]. Por exemplo, se observamos que o preço de um produto afeta a receita, nossa crença sobre a receita de preços próximos também deve mudar [^37]. Para lidar com crenças correlacionadas, utilizamos uma matriz de covariância $\\Sigma^n$ e uma matriz de precisão $B^n = (\\Sigma^n)^{-1}$ [^38].

As equações de atualização para a média $\\mu$ e a matriz de precisão $B$ são dadas por [^38]:

$$ \\mu^{n+1} = (B^{n+1})^{-1} (B^n \\mu^n + \\beta_w W^{n+1} e_{x_n}) \\qquad (2.15)$$

$$ B^{n+1} = B^n + \\beta_w e_{x_n} (e_{x_n})^T \\qquad (2.16)$$

Onde $e_{x_n}$ é um vetor coluna com um 1 na posição correspondente à alternativa $x_n$ medida e 0 nas demais posições [^38]. A atualização das crenças correlacionadas permite generalizar os resultados de uma única observação para outras alternativas não diretamente medidas [^38].

**Atualização Bayesiana com um prior não informativo**
Quando não temos informações prévias sobre um parâmetro, podemos usar um *prior não informativo* [^40]. Um *prior não informativo* é uma distribuição que não favorece nenhum valor particular do parâmetro [^40]. No contexto Bayesiano, um *prior não informativo* é frequentemente representado por uma densidade normal com variância infinita (ou precisão zero) [^40]. Embora os *priors não informativos* possam ser úteis em algumas situações, eles podem levar a resultados inesperados, especialmente no início do processo de aprendizado [^40].

### Conclusão

A visão Bayesiana oferece uma abordagem flexível e poderosa para o aprendizado adaptativo [^34]. Ao tratar os parâmetros verdadeiros como variáveis aleatórias e incorporar conhecimento prévio, podemos quantificar a incerteza e tomar decisões mais informadas [^34]. As equações de atualização Bayesiana fornecem um meio de combinar crenças *a priori* com dados observacionais para refinar nossas estimativas e reduzir a incerteza ao longo do tempo [^34]. Este capítulo estabeleceu as bases para explorar modelos Bayesianos mais avançados e suas aplicações em uma variedade de problemas de aprendizado adaptativo.

### Referências
[^34]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 34.
[^35]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 35.
[^37]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 37.
[^38]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 38.
[^40]: Optimal Learning. By Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 40.

<!-- END -->