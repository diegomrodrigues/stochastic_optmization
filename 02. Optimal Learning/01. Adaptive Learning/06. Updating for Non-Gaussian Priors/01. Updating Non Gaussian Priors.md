## Updating for Non-Gaussian Priors: Beyond the Normal-Normal Model

### Introdução
Em muitos problemas de aprendizagem adaptativa, a suposição de normalidade tanto para a distribuição *a priori* quanto para a distribuição de amostragem pode não ser apropriada [^45]. O modelo normal-normal, onde tanto a *prior* quanto a distribuição de amostragem são normais, oferece uma conveniência analítica, mas essa conveniência vem à custa da flexibilidade e aplicabilidade em situações do mundo real [^45]. Este capítulo explora a importância de atualizar distribuições *a priori* não Gaussianas, especialmente quando os dados observados são restritos ou discretos [^46].

### Conceitos Fundamentais
O modelo normal-normal é particularmente intuitivo porque o parâmetro desconhecido na distribuição de amostragem é precisamente a média das observações amostradas [^45]. A média da *prior* pode ser interpretada como a melhor estimativa do valor desconhecido, com o desvio padrão representando a incerteza [^45]. No entanto, essa abordagem tem limitações:

1.  **Restrições de Valor:** A distribuição normal assume que as observações podem assumir qualquer valor real. Em cenários onde os dados são inerentemente positivos (tempos de espera) ou discretos (resultados de testes médicos), uma distribuição normal pode ser inadequada [^46].

2.  **Modelagem da Prior:** Mesmo que a distribuição de amostragem possa ser aproximada por uma normal, a *prior* em si pode exigir uma distribuição diferente. Por exemplo, ao estimar taxas de serviço, que devem ser estritamente positivas, uma *prior* normal pode levar a taxas negativas, o que é impossível [^46].

Para lidar com essas limitações, podemos explorar modelos de aprendizado alternativos onde as distribuições de amostragem e *a priori* não são normais [^46]. O objetivo é manter a propriedade de conjugação, o que significa que a distribuição *a posteriori* pertence à mesma família da distribuição *a priori* [^46]. Isso simplifica o processo de atualização e permite uma interpretação mais direta dos parâmetros.

#### Modelos Conjugados Não Gaussianos
Vários modelos não Gaussianos mantêm a propriedade de conjugação, oferecendo alternativas adequadas para diferentes tipos de dados:

*   **Modelo Gamma-Exponencial:** Adequado para situações onde as observações são contínuas e positivas, como tempos de serviço. Aqui, o tempo de serviço segue uma distribuição exponencial com parâmetro λ, e λ segue uma distribuição gama [^46]. As equações de atualização para os parâmetros da distribuição gama são simples e intuitivas [^47].

*   **Modelo Gamma-Poisson:** Semelhante ao modelo Gamma-Exponencial, mas para observações discretas. Por exemplo, o número de clientes que visitam uma loja em um dia segue uma distribuição de Poisson com taxa λ, e λ segue uma distribuição gama [^48].

*   **Modelo Pareto-Uniforme:** Usado para estimar o máximo de uma distribuição uniforme em um intervalo [0, B], onde B é desconhecido. B segue uma distribuição de Pareto [^48].

*   **Modelos para aprender probabilidades:** Com o objetivo de aprender a probabilidade de ocorrência de um determinado evento, em vez do valor econômico de um evento.
    *   **Modelo Beta-Bernoulli:** A observação é modelada como uma variável aleatória de Bernoulli, indicando sucesso ou fracasso. A probabilidade de sucesso p segue uma distribuição beta [^49]. As atualizações para os parâmetros da distribuição beta são simples e intuitivas [^50].
    *   **Generalização Multivariada:** O modelo Beta-Bernoulli pode ser generalizado para um cenário multivariado, onde cada observação pode ser classificada em uma de $K$ categorias diferentes. A probabilidade de uma observação pertencer à categoria $k$ segue uma distribuição de Dirichlet [^51].

#### Aprendendo uma Variância Desconhecida
Em algumas situações, a precisão da observação ($BW$) pode ser desconhecida e deve ser aprendida junto com a média verdadeira μ [^52]. Isso leva a um modelo normal-gama, onde uma distribuição *a priori* conjunta é criada para (μ, $BW$). A distribuição marginal de $BW$ é Gama(a, b), e a distribuição condicional de μ, dado $BW$ = r, é N(θ, rτ) [^52]. As estimativas das quantidades desconhecidas obtidas a partir da distribuição normal-gama são dadas por:
$$\
E\mu = \theta, \quad E B_W = \frac{a}{b}
$$

### Conclusão
A escolha de uma *prior* não Gaussiana é crucial quando as suposições da distribuição normal não se alinham com a natureza dos dados ou crenças *a priori* [^46]. Modelos conjugados como o gamma-exponencial, gamma-Poisson e beta-Bernoulli oferecem alternativas viáveis, mantendo a conveniência computacional e permitindo uma representação mais precisa da incerteza [^46]. A seleção do modelo apropriado depende das características específicas do problema, incluindo o tipo de dados e o conhecimento *a priori* disponível [^45].

### Referências
[^45]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 45
[^46]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 46
[^47]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 47
[^48]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 48
[^49]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 49
[^50]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 50
[^51]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 51
[^52]: Optimal Learning, Warren B. Powell and Ilya O. Ryzhov, Copyright© 2018 John Wiley & Sons, Inc., p. 52
<!-- END -->