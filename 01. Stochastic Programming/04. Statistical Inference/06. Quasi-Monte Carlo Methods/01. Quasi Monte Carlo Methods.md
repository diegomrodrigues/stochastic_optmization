## Capítulo 5.4: Métodos Quasi-Monte Carlo para Aproximação de Expectativas

### Introdução

Nas seções anteriores deste capítulo, notavelmente nas Seções 5.1 a 5.3, exploramos extensivamente a abordagem de Aproximação por Média Amostral (SAA) para resolver problemas de otimização estocástica e equações generalizadas estocásticas [^1, ^2, ^32, ^33]. A pedra angular dessa abordagem é a substituição de um valor esperado, como $f(x) = \mathbb{E}[F(x, \xi)]$, por uma média amostral $\hat{f}_N(x) = \frac{1}{N} \sum_{j=1}^{N} F(x, \xi^j)$ [^2]. Essa aproximação baseia-se fundamentalmente na geração de uma amostra *aleatória* $\xi^1, \ldots, \xi^N$ de realizações do vetor de dados incertos $\xi$, tipicamente gerada usando técnicas de Monte Carlo (MC) [^40]. Discutimos as propriedades estatísticas desses estimadores SAA, incluindo sua consistência [^6, ^7, ^8, ^9, ^10, ^11, ^28, ^29, ^34], comportamento assintótico [^16, ^19, ^20, ^21, ^30, ^31, ^36, ^37] e taxas de convergência exponencial sob certas condições [^41, ^42, ^43, ^44, ^45, ^46, ^47, ^48, ^49, ^52, ^53].

No entanto, a abordagem MC padrão, apesar de sua generalidade e robustez, pode não ser a mais eficiente, especialmente quando a dimensão $d$ do vetor de dados aleatórios $\xi$ (ou do vetor uniforme U do qual ele pode ser derivado) é pequena [^54]. Esta seção introduz uma abordagem alternativa conhecida como métodos **Quasi-Monte Carlo (QMC)**. Assumindo, sem perda de generalidade através de uma transformação apropriada [^54, ^55], que desejamos avaliar a expectativa $\mathbb{E}[\psi(U)] = \int_{I^d} \psi(u)du$, onde $U$ é um vetor aleatório uniformemente distribuído no hipercubo unitário $d$-dimensional $I^d = [0, 1]^d$ [^55], os métodos QMC propõem a aproximação:

> $$ \mathbb{E}[\psi(U)] \approx \frac{1}{N} \sum_{j=1}^{N} \psi(u_j) $$
> para uma sequência *determinística* de pontos $u_1, \ldots, u_N \in I^d$ cuidadosamente escolhida [^56].

A intuição é que, ao selecionar os pontos $u_j$ de forma mais regular e uniforme do que uma amostra aleatória típica, que tende a formar aglomerados (clusters) e deixar regiões descobertas [^55], podemos obter uma taxa de convergência mais rápida para a aproximação da integral. Como veremos, enquanto o erro da média amostral MC é de ordem estocástica $O_p(N^{-1/2})$ independentemente da dimensão $d$ (embora a variância possa depender de $d$) [^55], o erro da aproximação QMC pode, sob certas condições, decair mais rapidamente, potencialmente próximo a $O(N^{-1})$ [^55, ^58, ^59], embora a dependência da dimensão $d$ se manifeste de forma diferente [^55, ^61, ^62]. Esta seção se baseia fortemente nos conceitos apresentados por Niederreiter [^54].

### Conceitos Fundamentais de QMC

#### Uniformidade e Discrepância

A chave para o sucesso dos métodos QMC reside na escolha de sequências $\{u_j\}$ que cubram o espaço $I^d$ da maneira mais uniforme possível. Uma condição necessária para a convergência da aproximação QMC para $\mathbb{E}[\psi(U)]$ para qualquer função $\psi$ integrável à Riemann é que a sequência seja uniformemente distribuída em $I^d$. Isso significa que, para qualquer intervalo (subconjunto retangular) $A \subset I^d$, a proporção de pontos da sequência que caem em $A$ deve convergir para o volume (medida de Lebesgue $d$-dimensional) de $A$ [^57]:

$$ \lim_{N \to \infty} \frac{1}{N} \sum_{j=1}^{N} \mathbf{1}_A(u_j) = V_d(A) $$ [^57]

onde $\mathbf{1}_A(\cdot)$ é a função indicadora do conjunto $A$.

Para quantificar a uniformidade de um conjunto finito de pontos $\{u_1, \ldots, u_N\}$, introduz-se a noção de **discrepância**. A **discrepância estrela (star discrepancy)** $D^*(u_1, \ldots, u_N)$ é definida como a máxima diferença entre a proporção empírica de pontos e o volume teórico, sobre todos os subintervalos ancorados na origem [^57]:

$$ D^*(u_1, \ldots, u_N) := \sup_{A \in \mathcal{J}^*} \left| \frac{1}{N} \sum_{j=1}^{N} \mathbf{1}_A(u_j) - V_d(A) \right| $$ [^57]

onde $\mathcal{J}^*$ é a família de todos os subintervalos de $I^d$ da forma $\prod_{i=1}^d [0, b_i)$. É possível demonstrar que a condição de convergência (5.144) para todos os intervalos $A \subset I^d$ (e, portanto, a convergência da aproximação QMC para funções integráveis à Riemann) é equivalente à condição de que $\lim_{N \to \infty} D^*(u_1, \ldots, u_N) = 0$ [^58].

#### Limites de Erro: A Desigualdade de Koksma-Hlawka

Uma propriedade mais importante da discrepância estrela é que ela permite fornecer limites superiores determinísticos para o erro de aproximação QMC. Isso é formalizado pela **desigualdade de Koksma-Hlawka**.

No caso unidimensional ($d=1$), o Teorema 5.26 (Koksma) estabelece que se $\psi: [0, 1] \to \mathbb{R}$ tem **variação total limitada** $V(\psi)$, então para qualquer conjunto de pontos $u_1, \ldots, u_N \in [0, 1]$ [^58]:

$$ \left| \frac{1}{N} \sum_{j=1}^{N} \psi(u_j) - \int_0^1 \psi(u) du \right| \le V(\psi) D^*(u_1, \ldots, u_N) $$ [^58]

A variação total $V(\psi)$ é definida como $\sup \sum_{i=1}^m |\psi(t_i) - \psi(t_{i-1})|$, onde o supremo é tomado sobre todas as partições $0 = t_0 < t_1 < \ldots < t_m = 1$ do intervalo $[0, 1]$ [^58]. O contexto fornece um esboço da prova deste teorema, utilizando integração e soma por partes [^58].

Este resultado foi estendido para o caso multidimensional ($d > 1$) por Hlawka (Teorema 5.27) [^59]. Se $\psi: I^d \to \mathbb{R}$ tem **variação limitada no sentido de Hardy e Krause**, denotada por $V(\psi)$, então para qualquer $u_1, \ldots, u_N \in I^d$ [^59]:

$$ \left| \frac{1}{N} \sum_{j=1}^{N} \psi(u_j) - \int_{I^d} \psi(u) du \right| \le V(\psi) D^*(u_1, \ldots, u_N) $$ [^59]

A variação no sentido de Hardy e Krause $V(\psi)$ é uma soma de variações no sentido de Vitali sobre as faces de $I^d$ de todas as dimensões $k=1, \ldots, d$ (ver equação (5.148)) [^59]. A variação no sentido de Vitali $V^{(d)}(\psi)$ mede a "oscilação" da função e é definida através de somas alternadas dos valores de $\psi$ nos vértices de subintervalos (ver equação (5.147)) [^59].

> **Caixa de Destaque: Desigualdade de Koksma-Hlawka**
> O erro da aproximação Quasi-Monte Carlo é limitado pelo produto da variação da função integranda (no sentido de Hardy e Krause) e a discrepância estrela do conjunto de pontos utilizado:
> $$ \text{Erro} \le V(\psi) D^*(u_1, \ldots, u_N) $$ [^59]

Embora fundamental, a desigualdade de Koksma-Hlawka tem limitações práticas. A constante $V(\psi)$ pode ser muito grande, tornando o limite superior pouco informativo [^59]. Pior ainda, se a função $\psi(u) = F(H^{-1}(u))$ tem variação ilimitada (o que ocorre tipicamente se o suporte da variável aleatória original $\xi$ é ilimitado, fazendo $H^{-1}(u)$ tender ao infinito nos extremos 0 ou 1), a desigualdade não se aplica diretamente [^59].

#### Sequências de Baixa Discrepância

A desigualdade de Koksma-Hlawka motiva a construção de sequências $\{u_j\}$ para as quais a discrepância $D^*(u_1, \ldots, u_N)$ seja a menor possível para todo $N$. Tais sequências são chamadas de **sequências de baixa discrepância (low-discrepancy sequences)** [^60].

Sabe-se que para qualquer conjunto de $N$ pontos em $I^d$, $D^*(u_1, \ldots, u_N)$ é sempre maior ou igual a uma quantidade de ordem $O(N^{-1})$ [^60]. Em $d=1$, o limite inferior $1/(2N)$ é atingido pela sequência $u_j = (2j-1)/(2N)$ [^60]. No entanto, não existe uma sequência infinita $u_1, u_2, \ldots$ tal que $D^*(u_1, \ldots, u_N) \le c/N$ para alguma constante $c$ e todo $N$ [^60]. A melhor taxa de decaimento assintótica possível para $D^*$ em $d=1$ é $O(N^{-1} \ln N)$ [^60].

*   **Sequência de Van der Corput:** Uma construção clássica que atinge a taxa $O(N^{-1} \ln N)$ em $d=1$ é a **sequência de van der Corput** [^60, ^61]. Para uma base inteira $b \ge 2$, qualquer inteiro $n \ge 0$ tem uma expansão única na base $b$: $n = \sum_{i \ge 0} a_i(n) b^i$ [^60]. A função **radical-inversa** $\phi_b(n)$ é definida "refletindo" os dígitos em torno da vírgula decimal: $\phi_b(n) := \sum_{i \ge 0} a_i(n) b^{-i-1}$ [^61]. A sequência de van der Corput na base $b$ é então $u_j := \phi_b(j)$ para $j = 0, 1, 2, \ldots$ [^61]. Para esta sequência, $D^*(u_1, \ldots, u_N) \le C_b N^{-1} \ln N$ para alguma constante $C_b$ [^61].

*   **Sequência de Halton:** Uma extensão multidimensional é a **sequência de Halton** [^61]. Sejam $p_1, \ldots, p_d$ os primeiros $d$ números primos. A sequência de Halton é definida como $u_j := (\phi_{p_1}(j), \ldots, \phi_{p_d}(j)) \in I^d$ para $j = 0, 1, 2, \ldots$ [^61]. Para esta sequência, a discrepância estrela satisfaz [^61]:
    $$ D^*(u_1, \ldots, u_N) \le A_d N^{-1} (\ln N)^d + O(N^{-1} (\ln N)^{d-1}) $$ [^61]
    onde a constante $A_d = \prod_{i=1}^d \frac{p_i - 1}{2 \ln p_i}$ [^61].

#### Desempenho e Dimensionalidade

A taxa de convergência $O(N^{-1} (\ln N)^d)$ para a sequência de Halton, via Koksma-Hlawka, compara-se favoravelmente com a taxa $O_p(N^{-1/2})$ do MC para dimensões $d$ baixas [^61, ^62]. No entanto, a constante $A_d$ no termo principal cresce superexponencialmente com $d$ (pois, pelo teorema dos números primos, $p_d \sim d \ln d$) [^62]. Isso torna os limites superiores de erro baseados na discrepância inúteis para dimensões $d$ maiores [^62]. Na prática, acredita-se que os métodos QMC sejam vantajosos sobre o MC para problemas de dimensão baixa a moderada, talvez $d \le 20$, mas essa vantagem diminui com o aumento de $d$ [^62]. A análise exata depende da classe específica de problemas e do método QMC aplicado, sendo uma área de investigação ativa [^62]. Deve-se notar que o erro da aproximação por média amostral MC, $O_p(N^{-1/2})$, não depende explicitamente de $d$ em sua taxa, embora a variância subjacente possa depender [^55].

### QMC Randomizado

Uma desvantagem das sequências QMC determinísticas é a dificuldade em estimar o erro de aproximação [^62]. Os limites de erro como (5.149) são tipicamente muito frouxos (pessimistas) e difíceis ou impossíveis de calcular na prática, pois $V(\psi)$ raramente é conhecida [^62].

Para superar isso, pode-se usar uma **randomização** da sequência QMC, preservando sua estrutura regular. Um método simples foi proposto por Cranley e Patterson [^62]. Gera-se um único vetor aleatório $u$ uniformemente distribuído em $I^d$. Então, a sequência QMC original $u_1, \ldots, u_N$ é deslocada por $u$, módulo 1:

$$ \tilde{u}_j := (u_j + u) \pmod 1, \quad j = 1, \ldots, N $$ [^62]

onde a operação $\pmod 1$ é entendida coordenada a coordenada [^62]. Pode-se mostrar que cada ponto $\tilde{u}_j$ individualmente ainda tem distribuição uniforme em $I^d$. Consequentemente, o estimador QMC randomizado $\frac{1}{N} \sum_{j=1}^N \psi(\tilde{u}_j)$ é um estimador **não viesado** da expectativa $\mathbb{E}[\psi(U)]$ [^62].

A vantagem chave é que a variância deste estimador randomizado pode ser significativamente menor que a variância do estimador MC padrão com o mesmo tamanho de amostra $N$ [^62]. Além disso, ao repetir o processo de randomização $M$ vezes (gerando $M$ vetores de deslocamento $u^{(1)}, \ldots, u^{(M)}$ independentes), obtêm-se $M$ estimativas independentes e identicamente distribuídas da integral. A média dessas $M$ estimativas ainda é não viesada, e sua variância pode ser estimada pela variância amostral das $M$ replicações [^62]. Isso fornece uma maneira prática de construir intervalos de confiança para a integral usando QMC.

### Conclusão

Os métodos Quasi-Monte Carlo oferecem uma alternativa determinística (ou randomizada) às técnicas padrão de Monte Carlo para aproximar expectativas da forma $\mathbb{E}[\psi(U)]$. Utilizando sequências de pontos determinísticas e de baixa discrepância, como as de van der Corput e Halton [^60, ^61], QMC pode alcançar taxas de convergência assintoticamente mais rápidas do que MC, especialmente em dimensões baixas a moderadas [^61, ^62]. A desigualdade de Koksma-Hlawka fornece um fundamento teórico para esses métodos, ligando o erro de aproximação à variação da função integranda e à discrepância da sequência [^58, ^59]. No entanto, as constantes envolvidas nos limites de erro podem crescer rapidamente com a dimensão, e a estimativa de erro para QMC determinístico é problemática [^59, ^62]. A randomização, como a proposta por Cranley e Patterson [^62], aborda a questão da estimativa de erro, fornecendo estimadores não viesados e permitindo a avaliação da variância por replicação, mantendo potencialmente a vantagem de variância reduzida sobre o MC [^62]. No contexto da Aproximação por Média Amostral (SAA) discutida anteriormente [^2, ^41], QMC pode ser visto como um método alternativo para construir a função aproximada $\hat{f}_N(x)$ ou estimar termos de expectativa dentro de problemas de otimização estocástica ou equações generalizadas, particularmente quando a aleatoriedade subjacente pode ser mapeada para o cubo unitário $I^d$.

### Referências

[^1]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on stochastic programming: modeling and theory*. MPS/SIAM Series on Optimization. (Page 155)
[^2]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equation 5.2, Page 155)
[^3]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Page 156)
[^4]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Page 156)
[^5]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Page 156)
[^6]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Proposition 5.1, Page 156; Proof on Page 157)
[^7]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.1.1, Page 157)
[^8]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Proposition 5.2, Page 157; Proof on Page 158)
[^9]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.3, Page 158)
[^10]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Page 158)
[^11]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.4, Page 159)
[^12]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equation 5.10, Page 160)
[^13]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.5, Page 160)
[^14]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Remark 5, Page 161)
[^15]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Remark 6, Page 162)
[^16]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.1.2, Equations 5.19-5.21, Page 163)
[^17]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equation 5.22 discussion, Page 163)
[^18]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Proposition 5.6, Page 163; Proof on Page 164)
[^19]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Assumptions A1, A2, Equation 5.23, Page 164)
[^20]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Functional CLT discussion, Page 164)
[^21]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.7, Equations 5.24-5.29, Page 165)
[^22]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.1.3, Page 166)
[^23]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Assumptions S1-S4, Page 166)
[^24]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equation 5.31, Page 166)
[^25]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.33-5.34, Page 167)
[^26]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.8, Equations 5.35-5.37, Page 167)
[^27]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.38-5.41, Remark 7, Page 168; Remark 8, Page 169)
[^28]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.1.4, Equations 5.46-5.47, Assumptions A\'1-A\'3, Page 170)
[^29]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.9, Page 170; Proof on Page 171)
[^30]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Assumptions A\'4-A\'5, Equation 5.49, Page 171)
[^31]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.10, Equation 5.50, Page 171; Proof on Pages 171-172)
[^32]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.2, Equation 5.60, Page 174)
[^33]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.61, 5.63, 5.64, 5.66, Page 174; Equation 5.67, Page 175)
[^34]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.2.1, Theorem 5.12, Page 175)
[^35]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Assumption E2, Page 176)
[^36]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Definition 5.13, Page 176)
[^37]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.14, Assumption E3, Theorem 5.15, Page 177)
[^38]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.2.2, Equations 5.70-5.71, Page 177)
[^39]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.72-5.75, Page 178; Equations 5.76-5.82, Pages 179-180)
[^40]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.3, Page 180)
[^41]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Page 181)
[^42]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.85-5.87, Page 181)
[^43]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.88-5.92, Page 182)
[^44]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Assumption M2, Theorem 5.16, Equations 5.93-5.94, Page 182)
[^45]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.95-5.97, Assumption M3, Equations 5.98-5.103, Page 183)
[^46]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.17, Equations 5.104-5.105, Remark 10, Remark 11, Page 184)
[^47]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Remark 12, Equations 5.106-5.109, Page 185)
[^48]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.3.2, Assumptions M4-M5, Equations 5.110-5.115, Page 185-186)
[^49]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.18, Equations 5.116-5.118, Page 186; Proof and Equation 5.119-5.120, Page 187)
[^50]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Remark 13, Page 187; Complexity discussion, Page 188)
[^51]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Assumption M6, Equation 5.121, Corollary 5.19, Equations 5.122-5.123, Page 188)
[^52]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Remark 14, Remark 15, Equations 5.124-5.127, Page 189; Corollary 5.20, Equations 5.128-5.131, Page 190; Example 5.21, Equations 5.132-5.133, Page 191)
[^53]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.3.3, Definition 5.22, Equations 5.134-5.135, Theorem 5.23, Equation 5.136-5.138, Page 192; Theorem 5.24, Equation 5.139, Page 193)
[^54]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.4, Equations 5.140-5.141, Page 193)
[^55]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Page 194)
[^56]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equation 5.143, Page 194)
[^57]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equation 5.144, Definition 5.25, Equation 5.145, Page 195)
[^58]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.26 (Koksma), Equation 5.146, Proof sketch, Page 195)
[^59]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Equations 5.147-5.148, Theorem 5.27 (Hlawka), Equation 5.149, Discussion on V(ψ), Page 196)
[^60]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Low-discrepancy definition, bounds, van der Corput intro, Page 197)
[^61]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Radical-inverse function, van der Corput definition, Halton sequence definition, Discrepancy bounds, Equations 5.150-5.153, Page 197)
[^62]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Discussion on A_d, dimensionality, error estimation drawback, Randomized QMC (Cranley-Patterson), Page 198)
[^63]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.5, Page 198)
[^64]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.5.1 Latin Hypercube Sampling, Equations 5.155-5.157, Page 199)
[^65]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.5.2 Linear Control Random Variables, Equations 5.158-5.160, Page 200)
[^66]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.5.3 Importance Sampling and Likelihood Ratio, Equations 5.161-5.165, Page 201)
[^67]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Likelihood ratio for derivatives, Equation 5.166, Page 202)
[^68]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.6 Validation Analysis, Page 202)
[^69]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.6.1 Estimation of Optimality Gap, Equations 5.167-5.178, Pages 202-206)
[^70]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.6.2 Statistical Testing of Optimality Conditions, Equations 5.185-5.195, Pages 207-209)
[^71]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.7 Chance Constrained Problems, Equations 5.196-5.200, Page 210)
[^72]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Proposition 5.29, Equations 5.201-5.202, Page 211)
[^73]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Proposition 5.30, Equations 5.203-5.205, Page 211-212)
[^74]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Convex Approximation Approach, Equations 5.206-5.207, Lemma 5.31, Assumptions F1-F2, Page 213)
[^75]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Theorem 5.32, Equations 5.208-5.220, Pages 214-215)
[^76]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Sample size estimates for feasibility, Equations 5.221-5.224, Page 216)
[^77]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Validation upper/lower bounds, Equations 5.225-5.235, Pages 217-219)
[^78]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.8 SAA Method Applied to Multistage Stochastic Programming, Pages 221-229)
[^79]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Section 5.9 Stochastic Approximation Method, Pages 230-248)
[^80]: Shapiro, A. (2009). Chapter 5: Statistical Inference. (Exercises, Pages 249-252)

<!-- END -->