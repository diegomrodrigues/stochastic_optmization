## Capítulo 5.4.1: Discrepância Estrela como Métrica de Qualidade em Conjuntos de Pontos QMC

### Introdução

Como explorado extensivamente nas seções anteriores deste capítulo, a aproximação de expectativas, como $f(x) := E[F(x, \\xi)]$ [^1], é um problema central em inferência estatística e programação estocástica. O método de **Sample Average Approximation (SAA)** [^1], baseado em amostras aleatórias $\\xi^1, ..., \\xi^N$ geradas por técnicas de **Monte Carlo (MC)** [^1], fornece um estimador $f_N(x)$ [^1] que converge para $f(x)$ sob condições de regularidade, como garantido pela Lei dos Grandes Números (LLN) [^2]. Vimos que $f_N(x)$ é um estimador não viesado, $E[f_N(x)] = f(x)$ [^2], e seu erro é probabilisticamente da ordem de $O_p(N^{-1/2})$ [^9], com uma distribuição assintoticamente normal governada pela variância $\\sigma^2(x) = \\text{Var}[F(x, \\xi)]$ [^9].

No entanto, a Seção 5.3 introduziu a ideia de que a amostragem MC padrão, embora robusta, pode não ser a forma mais eficiente de gerar pontos para aproximação de integrais, especialmente em dimensões mais baixas [^40]. Uma amostra iid $U^1, ..., U^N$ no hipercubo unitário $I^d = [0, 1]^d$ tende a formar aglomerados (*cluster*) e deixar outras áreas descobertas (*uncovered*) [^40]. Isso motivou a introdução dos métodos **Quasi-Monte Carlo (QMC)** na Seção 5.4 [^39], que utilizam sequências determinísticas ou *low-discrepancy* $u_1, ..., u_N \\in I^d$ [^43] cuidadosamente escolhidas para cobrir o espaço de forma mais uniforme, visando uma taxa de convergência potencialmente mais rápida do que MC [^40].

Para quantificar a uniformidade de tais conjuntos de pontos e, consequentemente, a qualidade da aproximação QMC $E[\\psi(U)] \\approx \\frac{1}{N} \\sum_{j=1}^N \\psi(u_j)$ [^40], são necessárias métricas específicas. Este capítulo foca em uma das mais fundamentais dessas métricas: a **discrepância estrela (star discrepancy)**. Analisaremos sua definição formal, interpretação e relevância, baseando-nos exclusivamente nas informações contextuais fornecidas anteriormente e na definição fundamental apresentada.

### Conceitos Fundamentais

A avaliação da qualidade de um conjunto de pontos $\\{u_1, ..., u_N\\}$ utilizado em métodos QMC passa pela sua capacidade de mimetizar a distribuição uniforme no hipercubo unitário $I^d$. A **discrepância estrela** é uma medida quantitativa dessa uniformidade.

> **Definição 5.25 (Star Discrepancy).** A **star discrepancy** de um conjunto de pontos $\\{u_1, ..., u_N\\} \\subset I^d$ é definida por
> $$ D^*(u_1, ..., u_N) := \\sup_{A \\in \\Lambda} \\left| \\frac{1}{N} \\sum_{j=1}^N 1_A(u_j) - V_d(A) \\right| $$
> onde $\\Lambda$ é a família de todos os subintervalos de $I^d$ da forma $\\prod_{i=1}^d [0, b_i)$ [^41].

Nesta definição, $1_A(u_j)$ é a função indicadora, que vale 1 se $u_j \\in A$ e 0 caso contrário. O termo $\\frac{1}{N} \\sum_{j=1}^N 1_A(u_j)$ representa a proporção empírica de pontos que caem no subintervalo (ou caixa ancorada na origem) $A$. $V_d(A)$ denota a medida de Lebesgue d-dimensional (volume) do conjunto $A \\subset \\mathbb{R}^d$ [^41]. A **star discrepancy** mede, portanto, o maior desvio absoluto entre a proporção empírica de pontos em uma caixa ancorada na origem e o volume dessa caixa, considerando todas as caixas possíveis desse tipo. Um valor baixo de $D^*$ indica que a distribuição empírica dos pontos se aproxima bem da distribuição uniforme no que diz respeito a esses subconjuntos específicos.

A importância da **star discrepancy** reside em sua conexão com o erro de integração QMC. Como vimos na Seção 5.4, para que a aproximação QMC $\\frac{1}{N} \\sum_{j=1}^N \\psi(u_j)$ convirja para a integral $\\int_{I^d} \\psi(u)du$, é necessário que a sequência $\\{u_j\\}$ satisfaça $\\lim_{N\\to\\infty} \\frac{1}{N} \\sum_{j=1}^N 1_A(u_j) = V_d(A)$ para qualquer intervalo $A \\subset I^d$ [^41]. Esta condição é equivalente a $\\lim_{N\\to\\infty} D^*(u_1, ..., u_N) = 0$ [^41].

Mais quantitativamente, o contexto apresentou teoremas que relacionam diretamente a **star discrepancy** ao erro da aproximação QMC.

> **Teorema 5.26 (Koksma).** Se $\\psi : [0, 1] \\to \\mathbb{R}$ tem **variação limitada (bounded variation)** $V(\\psi)$, então para quaisquer $u_1, ..., u_N \\in [0, 1]$ temos
> $$ \\left| \\frac{1}{N} \\sum_{j=1}^N \\psi(u_j) - \\int_0^1 \\psi(u)du \\right| \\le V(\\psi) D^*(u_1, ..., u_N) $$ [^41].

> **Teorema 5.27 (Hlawka).** Se $\\psi : I^d \\to \\mathbb{R}$ tem **variação limitada no sentido de Hardy e Krause** $V(\\psi)$ em $I^d$, então para quaisquer $u_1, ..., u_N \\in I^d$ temos
> $$ \\left| \\frac{1}{N} \\sum_{j=1}^N \\psi(u_j) - \\int_{I^d} \\psi(u)du \\right| \\le V(\\psi) D^*(u_1, ..., u_N) $$ [^42].

Esses teoremas (Koksma-Hlawka) estabelecem que o erro da integração QMC é limitado pelo produto da **star discrepancy** do conjunto de pontos e uma medida da variação da função integranda. Isso contrasta com o erro MC, cuja magnitude é controlada pela variância da função e pela raiz quadrada do número de pontos, $\\sigma N^{-1/2}$ [^9, ^40]. A meta dos métodos QMC é construir sequências $\\{u_j\\}$ para as quais $D^*(u_1, ..., u_N)$ decaia mais rapidamente que a taxa MC de $O_p(N^{-1/2})$.

Sequências que atingem essa meta são chamadas de **sequências de baixa discrepância (low-discrepancy sequences)** [^43]. O contexto menciona que, no caso unidimensional (d=1), o limite inferior para a discrepância é da ordem de $O(N^{-1})$, especificamente $D^*(u_1, ..., u_N) \\ge 1/(2N)$, e esta taxa é atingida pela sequência $u_j = (2j-1)/(2N)$ [^43]. No entanto, para sequências infinitas, não é possível atingir $O(N^{-1})$ uniformemente [^43]. O melhor que se pode obter é a taxa $O(N^{-1} \\ln N)$, que é alcançada, por exemplo, pela **sequência de van der Corput** [^43], definida usando a função radical-inversa $\\phi_b(n)$ em uma base $b \\ge 2$ [^43].

Para dimensões $d > 1$, o contexto introduz a **sequência de Halton**, que generaliza a ideia da sequência de van der Corput usando bases primas $p_1, ..., p_d$ para cada dimensão [^43]. Para a sequência de Halton, a **star discrepancy** satisfaz
$$ D^*(u_1, ..., u_N) \\le A_d N^{-1}(\\ln N)^d + O(N^{-1}(\\ln N)^{d-1}) $$
para $N \\ge 2$, onde a constante $A_d$ cresce rapidamente com a dimensão $d$ [^43]. Embora a taxa de decaimento $O(N^{-1}(\\ln N)^d)$ seja assintoticamente superior à taxa MC $O_p(N^{-1/2})$ para $d$ fixo, a constante $A_d$ (e similarmente a constante $V(\\psi)$ na cota de erro de Hlawka) pode se tornar excessivamente grande, tornando as cotas de erro QMC inúteis para dimensões maiores [^43, ^44]. Isso sugere que, na prática, os métodos QMC podem ser mais vantajosos que MC apenas para dimensões relativamente baixas (e.g., $d \\le 20$ [^44]), embora a aplicabilidade dependa da classe específica de problemas [^44].

O contexto também menciona brevemente a possibilidade de randomizar sequências QMC, como a sugestão de Cranley e Patterson [^44], para permitir a estimativa do erro de aproximação, uma vantagem inerente aos métodos MC [^40].

### Conclusão

A **discrepância estrela**, $D^*(u_1, ..., u_N)$, emerge como uma ferramenta teórica fundamental no campo dos métodos **Quasi-Monte Carlo**. Ela fornece uma medida rigorosa da uniformidade de um conjunto de pontos finito dentro do hipercubo unitário, especificamente quantificando o pior desvio entre a medida empírica e a medida de volume uniforme para caixas ancoradas na origem [^41]. Sua principal relevância deriva da sua conexão direta com o erro de integração QMC, estabelecida por desigualdades do tipo Koksma-Hlawka [^41, ^42], onde um $D^*$ menor implica um erro de aproximação menor para funções de variação limitada.

As **sequências de baixa discrepância**, como as de van der Corput e Halton [^43], são construídas explicitamente para minimizar $D^*$ e atingir taxas de convergência assintoticamente superiores a $O_p(N^{-1/2})$ dos métodos MC [^9, ^40]. No entanto, como discutido no contexto [^43, ^44], as constantes envolvidas nas cotas de erro baseadas em discrepância podem crescer rapidamente com a dimensão, limitando a vantagem prática do QMC sobre o MC a problemas de dimensionalidade moderada. A análise da **star discrepancy** fornece, portanto, insights cruciais sobre os pontos fortes e as limitações dos métodos QMC em comparação com as abordagens de amostragem MC exploradas no restante deste capítulo.

### Referências

[^1]: p. 155: Consider the following stochastic programming problem: Min {f(x) := E[F(x,§)]}. ... sample average approximation (SAA) Min fN(x) := (1/N) Σ F(x, ξj)
[^2]: p. 156: By the Law of Large Numbers we have that, under some regularity conditions, fN(x) converges pointwise w.p. 1 to f(x) as N → ∞. ... We also have that E[ fN(x)] = f(x), i.e., fN(x) is an unbiased estimator of f(x).
[^9]: p. 163: Consistency of the SAA estimators gives a certain assurance that the error of the estimation approaches zero... variance σ²(x)/N, where σ²(x) := Var [F(x, ξ)]... by the CLT we have that N1/2 [fN(x) - f(x)] D→ Yx, ... Yx ~ N (0, σ²(x))... error of estimation of f (x) is (stochastically) of order Op(N−1/2).
[^11]: p. 165: Under mild additional conditions ... it follows from (5.25) that N1/2E[θN – θ*] tends to E[ inf x∈S Y(x)] as N → ∞, that is, E[θN] – θ* = N−1/2E [inf x∈S Y(x)] + o(N−1/2). ... if S is not a singleton, then the bias E[θN] – θ* typically is strictly less than zero and is of order O(N−1/2).
[^39]: p. 193: In this section we give a brief discussion of the so-called quasi-Monte Carlo methods. This section is based on Niederreiter [138]...
[^40]: p. 193, 194: Let ξ be a real valued random variable having cdf H(z) = Pr(ξ ≤ z). ... E[F(ξ)] = ∫ F(z)dH(z). ... E[ψ(U)] = ∫ ψ(u)du. ... evaluation by the Monte Carlo method is based on generating an iid sample U¹, ..., UN ... approximating E[ψ(U)] by the average ψN := N⁻¹ Σ ψ(Uj). Alternatively, one can employ the Riemann sum approximation ∫ ψ(u)du ≈ (1/N) Σ ψ(uj). ... If the function ψ(u) is Lipschitz continuous on [0,1], then the error of the Riemann sum approximation is of order O(N⁻¹), while the Monte Carlo sample average error is of (stochastic) order Op(N⁻¹/²). ... an iid sample U1,..., UN will tend to cluster in some areas while leaving other areas of the interval [0,1] uncovered. ... For d > 1 ... partition Id into the corresponding N = Md subintervals and use a corresponding Riemann sum approximation N⁻¹ Σ ψ(uj). The resulting error is of order O(M⁻¹) = O(N⁻¹/d). ... for larger values of d the Riemann sums approach quickly becomes unacceptable. On the other hand, the rate of convergence (error bounds) of the Monte Carlo sample average approximation of E[ψ(U)] does not depend directly on dimensionality d but only on the corresponding variance Var[ψ(U)]. Yet the problem of uneven covering of Id by an iid sample Uj, j = 1, . . ., N, remains persistent. Quasi-Monte Carlo methods employ the approximation E[ψ(U)] ≈ (1/N) Σ ψ(uj) for a carefully chosen (deterministic) sequence of points u1,..., uN ∈ Id.
[^41]: p. 195: ... if lim N→∞ (1/N) Σ 1A(uj) = Vd(A) for any interval A C Id. Here Vd(A) denotes the d-dimensional Lebesgue measure (volume) of set A C Rd. Definition 5.25. The star discrepancy of a point set {u1, ..., uN} C Id is defined by D*(u1,..., UN) := sup A∈Λ |(1/N) Σ 1A(uj) - Vd(A)|, where Λ is the family of all subintervals of Id of the form Π[0, bi). It is possible to show that for a sequence uj ∈ Id, j = 1, . . ., condition (5.144) holds iff limN→∞ D*(u1,..., uN) = 0. Theorem 5.26 (Koksma). If ψ : [0, 1] → R has bounded variation V (ψ), then for any u1,..., uN ∈ [0, 1] we have |(1/N) Σ ψ(uj) - ∫ ψ(u)du| ≤ V(ψ)D*(u1, ..., uN). Proof...
[^42]: p. 196: Theorem 5.27 (Hlawka). If ψ : Id → R has bounded variation V (ψ) on Id in the sense of Hardy and Krause, then for any u₁, . . ., uN ∈ Id we have |(1/N) Σ ψ(uj) - ∫ ψ(u)du| ≤ V(ψ)D*(u1, ..., uN). ... the involved constant V(ψ)/2 typically is far too large for practical calculations.
[^43]: p. 197: A sequence {uj} j∈N C Id is called a low-discrepancy sequence if D*(u1, ..., uN) is "small" for all N > 1. ... It is not difficult to show that D*(u1, ..., uN) always greater than or equal to 1/(2N) and this lower bound is attained for uj := (2j − 1)/2N, j = 1, . . ., N. ... there does not exist a sequence u₁, . . ., in [0,1] such that D*(u1,..., uN) ≤ c/N for some c > 0 and all N ∈ N. It is possible to show that a best possible for D*(u1, ..., uN), for a sequence of points uj ∈ [0, 1], j = 1, . . ., is of order O (N⁻¹ ln N). ... construct a sequence for which this rate is attained. ... radical-inverse function фb(n) ... Definition 5.28. For an integer b ≥ 2, the van der Corput sequence in base b is the sequence uj := фb(j), j = 0, 1, . . . . It is possible to show that to every van der Corput sequence u1,..., uN in base b, corresponds constant Cb such that D*(u1, ..., uN) ≤ Cb N⁻¹ ln N ∀N ∈N. A classical extension of van der Corput sequences to multidimensional settings is the following. Let p1 = 2, p2 = 3, ..., pd be the first d prime numbers. Then the Halton sequence, in the bases p1,..., pd, is defined as uj := (фp₁(j), . . ., фpd(j)) ∈ Id, j = 0, 1, . . . . It is possible to show that for that sequence, D*(u1, ..., uN) < AdN⁻¹(ln N)d + O(N⁻¹(ln N)d⁻¹) ∀N ≥ 2, where Ad = Π(pi-1)/ln(pi). By bound (5.149) of Theorem 5.27, this implies that the error of the corresponding quasi–Monte Carlo approximation is of order O (N⁻¹(ln N)d)...\n[^44]: p. 198: ... the coefficient Ad, of the leading term in the right-hand side of (5.153), grows superexponentially with increase of the dimension d. This makes the corresponding error bounds useless for larger values of d. ... It seems that for low dimensional problems, say, d ≤ 20, quasi-Monte Carlo methods are advantageous over Monte Carlo methods. With increase of the dimension d this advantage becomes less apparent. ... A drawback of (deterministic) quasi-Monte Carlo sequences {uj}j∈N is that there is no easy way to estimate the error... A way of dealing with this problem is to use a randomization of the set {u1, . . ., uN }... Such a simple randomization procedure was suggested by Cranley and Patterson [39]. That is, generate a random point u uniformly distributed over Id, and use the randomization ũj := (uj + u) mod 1, j = 1,..., N.

<!-- END -->