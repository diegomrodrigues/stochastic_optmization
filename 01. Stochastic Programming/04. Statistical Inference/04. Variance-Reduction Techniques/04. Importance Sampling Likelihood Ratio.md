## Capítulo 5.5.3: Técnicas de Amostragem por Importância e Razão de Verossimilhança

### Introdução

Nos capítulos anteriores, exploramos extensivamente as propriedades estatísticas dos estimadores SAA (Sample Average Approximation) [^1, ^2] e métodos para avaliar sua convergência e qualidade [^27, ^28, ^32]. Uma questão central na aplicação prática do método SAA, especialmente quando baseado em amostragem de Monte Carlo, é a variância dos estimadores. Como vimos, a variância do estimador SAA `f_N(x)` é dada por `σ²(x)/N` [^9], onde `σ²(x) = Var[F(x, ξ)]`. Reduzir essa variância pode acelerar a convergência e melhorar a precisão das soluções obtidas. As seções anteriores introduziram técnicas como Latin Hypercube Sampling [^45] e Variáveis Aleatórias de Controle Linear [^46]. Este capítulo aprofunda-se em outra classe poderosa, embora sensível, de técnicas de redução de variância: **Amostragem por Importância (Importance Sampling - IS)** e o **Método da Razão de Verossimilhança (Likelihood Ratio - LR)**. A ideia fundamental da Amostragem por Importância é modificar a distribuição de probabilidade a partir da qual as amostras são geradas, concentrando o esforço amostral nas regiões do espaço de resultados que mais contribuem para o valor esperado que se deseja estimar [^46, ^47]. Contudo, como será detalhado, a eficácia dessas técnicas depende crucialmente da escolha da nova distribuição de amostragem, e uma escolha inadequada pode levar à instabilidade e até mesmo ao aumento da variância [^47].

### Amostragem por Importância (Importance Sampling)

Considere o problema de estimar `f(x) = E[F(x, ξ)]`, onde `ξ` é um vetor aleatório com função de densidade de probabilidade (pdf) `h(·)` [^46]. A Amostragem por Importância baseia-se na introdução de uma pdf alternativa, `ψ(·)`, a partir da qual as amostras serão geradas. Assume-se que `ψ(ζ)` é bem definida e que `ψ(ζ) = 0` implica `h(ζ) = 0` [^47]. Isso garante que a razão `h(ζ)/ψ(ζ)` esteja bem definida (considerando `0/0 = 0`, como na definição fornecida [^47]).

Definimos a **razão de verossimilhança (likelihood ratio)** como:
$$\nL(ζ) := \frac{h(ζ)}{ψ(ζ)}\n$$ [^46]

Com esta definição, podemos reescrever o valor esperado `f(x)` da seguinte forma:
$$\nf(x) = \int F(x, ζ) h(ζ) dζ = \int F(x, ζ) \frac{h(ζ)}{ψ(ζ)} ψ(ζ) dζ = \int F(x, ζ) L(ζ) ψ(ζ) dζ\n$$
Esta última integral representa o valor esperado da variável aleatória `F(x, Z)L(Z)` quando `Z` é um vetor aleatório com pdf `ψ(·)`. Denotando esta expectativa como `E_ψ`, temos:
$$\nf(x) = E_ψ[F(x, Z)L(Z)]\n$$ [^47]

Isso sugere o **estimador de Amostragem por Importância**:
$$\n\tilde{f}_N(x) := \frac{1}{N} \sum_{j=1}^{N} F(x, ζ^j) L(ζ^j)\n$$ [^47]
onde `ζ^1, ..., ζ^N` são `N` amostras independentes e identicamente distribuídas (iid) geradas a partir da pdf `ψ(·)`. Este estimador é não viesado para `f(x)`, pois `E_ψ[\tilde{f}_N(x)] = E_ψ[F(x, Z)L(Z)] = f(x)`.

O objetivo da IS é escolher `ψ(·)` de forma a minimizar a variância do estimador `\tilde{f}_N(x)`, que é dada por:
$$\nVar_ψ[\tilde{f}_N(x)] = \frac{1}{N} Var_ψ[F(x, Z)L(Z)] = \frac{1}{N} \left( E_ψ[(F(x, Z)L(Z))^2] - (f(x))^2 \right)\n$$
Minimizar a variância é equivalente a minimizar o segundo momento `E_ψ[(F(x, Z)L(Z))^2]` [^47], uma vez que `f(x)` não depende de `ψ`. Temos:
$$\nE_ψ[F(x, Z)^2 L(Z)^2] = \int F(x, ζ)^2 L(ζ)^2 ψ(ζ) dζ = \int \frac{F(x, ζ)^2 h(ζ)^2}{ψ(ζ)} dζ\n$$ [^47]

Para encontrar a pdf `ψ(·)` que minimiza esta expressão, sujeita à restrição `∫ ψ(ζ)dζ = 1`, utilizamos a desigualdade de Cauchy-Schwarz, como sugerido no texto [^47]:
$$\n\left( \int |F(x, ζ)h(ζ)| dζ \right)^2 = \left( \int \frac{|F(x, ζ)h(ζ)|}{\sqrt{ψ(ζ)}} \sqrt{ψ(ζ)} dζ \right)^2 \le \left( \int \frac{F(x, ζ)^2 h(ζ)^2}{ψ(ζ)} dζ \right) \left( \int ψ(ζ) dζ \right)\n$$ [^47]
Como `∫ ψ(ζ)dζ = 1`, temos:
$$\nE_ψ[F(x, Z)^2 L(Z)^2] = \int \frac{F(x, ζ)^2 h(ζ)^2}{ψ(ζ)} dζ \ge \left( \int |F(x, ζ)h(ζ)| dζ \right)^2\n$$
A igualdade (e, portanto, o mínimo) é atingida quando `\sqrt{ψ(ζ)}` é proporcional a `|F(x, ζ)h(ζ)| / \sqrt{ψ(ζ)}`, o que implica que `ψ(ζ)` deve ser proporcional a `|F(x, ζ)h(ζ)|`. A **densidade de amostragem ótima**, `ψ*(·)`, é então dada por:
$$\nψ^*(ζ) := \frac{|F(x, ζ)h(ζ)|}{\int |F(x, y)h(y)| dy}\n$$ [^47]

> **Implicação da Densidade Ótima:** Se a função `F(x, ·)` for não negativa, a escolha ótima `ψ^*(ζ) = F(x, ζ)h(ζ) / f(x)` (assumindo `f(x) > 0`) leva a `F(x, Z)L(Z) = F(x, Z) [h(Z) / ψ^*(Z)] = F(x, Z) [h(Z) / (F(x, Z)h(Z)/f(x))] = f(x)`. Neste caso, o integrando se torna uma constante, e a variância do estimador IS é zero [^47].

No entanto, a implementação direta da `ψ*` ótima é geralmente impossível, pois ela depende da integral `∫ |F(x, y)h(y)| dy` ou, no caso não negativo, do próprio valor `f(x)` que desejamos estimar [^47]. A abordagem prática consiste em encontrar uma pdf `ψ(·)` que seja *aproximadamente* proporcional a `|F(x, ·)h(·)|` [^47]. Isso geralmente envolve usar conhecimento prévio sobre o problema para identificar as regiões "importantes" do espaço de `ξ`.

### Sensibilidade e Instabilidade da Amostragem por Importância

Conforme mencionado na introdução e explicitado no contexto [^47], a Amostragem por Importância é uma técnica extremamente **sensível** à escolha da pdf `ψ(·)` e é **notória por sua instabilidade**.

A sensibilidade decorre diretamente da dependência do desempenho na qualidade da aproximação de `ψ` a `ψ*`. Se `ψ` diferir significativamente de `ψ*`, a variância do estimador IS pode ser muito maior do que a do estimador de Monte Carlo padrão.

A instabilidade está frequentemente associada ao comportamento da razão de verossimilhança `L(ζ) = h(ζ)/ψ(ζ)` nas caudas da distribuição [^47]. Se a densidade de amostragem `ψ(ζ)` atribuir probabilidades muito baixas a regiões onde o produto `|F(x, ζ)h(ζ)|` não é desprezível (ou seja, se `ψ` subestimar a importância de certas regiões sob `h`), o valor de `L(ζ)` pode se tornar excessivamente grande nessas regiões. Isso é particularmente problemático se `ψ` tiver caudas mais leves que `h` (ponderada por `|F|`). O resultado pode ser um estimador cuja variância é dominada por algumas poucas amostras `ζ^j` que caem nessas regiões de baixa probabilidade sob `ψ` mas alta razão de verossimilhança `L(ζ^j)`, levando a uma variância muito alta e a um comportamento instável do estimador [^47]. O texto descreve isso como a razão de verossimilhança sendo a razão entre dois números muito pequenos na cauda [^47].

> **Desafios na Otimização SAA:** A sensibilidade da IS à escolha de `ψ` e sua dependência em `x` (através de `F(x, ·)` na `ψ*` ideal) tornam seu uso direto dentro de um procedimento de otimização SAA problemático. Uma única escolha de `ψ(·)` pode ser eficaz para alguns valores de `x` no espaço de decisão `X`, mas desastrosa para outros. Consequentemente, a aplicação da IS em otimização estocástica geralmente requer técnicas adaptativas ou escolhas de `ψ` que sejam robustas para uma gama de valores de `x`, o que nem sempre é viável [^47].

### Método da Razão de Verossimilhança (Likelihood Ratio Method)

O Método da Razão de Verossimilhança (LR) está intimamente relacionado à IS e é frequentemente empregado para estimar **derivadas** de valores esperados, ou seja, sensibilidades de `f(x)` em relação a `x`. O contexto ilustra isso com o exemplo do valor ótimo `Q(x, ξ)` do problema de segundo estágio em programação linear estocástica [^48].

Assumindo a configuração do exemplo [^48], onde `Q(x, ξ) = Q̃(h - Tx)`, `h` tem pdf `η(·)` e `T` são independentes, podemos escrever a expectativa condicional como:
$$\nE_{h|T}[Q(x, ξ)] = \int Q̃(h - Tx) η(h) dh = \int Q̃(z) η(z + Tx) dz\n$$
onde `z = h - Tx`. Introduzindo uma pdf de amostragem `ψ(ζ)`, podemos reescrever a expectativa usando a razão de verossimilhança `L(x, ζ) := η(ζ + Tx) / ψ(ζ)`:
$$\nE_{h|T}[Q(x, ξ)] = \int Q̃(ζ) L(x, ζ) ψ(ζ) dζ = E_ψ[Q̃(Z)L(x, Z)]\n$$ [^48]
onde `Z` tem pdf `ψ(·)`.

A vantagem chave surge ao calcular derivadas em relação a `x`. Se a função `η(·)` for suave, a razão de verossimilhança `L(x, ζ)` também será suave em `x` (assumindo `T` fixo ou sua distribuição não dependa de `x` de forma complexa). Sob condições de regularidade que permitem a troca de derivada e integral (ou expectativa), temos:
$$\n\nabla_x f(x) = \nabla_x E_ψ[Q̃(Z)L(x, Z)] = E_ψ[\nabla_x (Q̃(Z)L(x, Z))] = E_ψ[Q̃(Z) \nabla_x L(x, Z)]\n$$ [^48]
Isso permite estimar o gradiente `∇_x f(x)` amostrando o termo `Q̃(Z) ∇_x L(x, Z)` sob a distribuição `ψ`. Notavelmente, isso evita a necessidade de diferenciar a função `Q̃(z)`, que pode ser não suave (por exemplo, ter derivadas por partes constantes em programação linear, como observado em [^48]).

O texto também aponta que `L(Z) - 1` pode ser usado como uma variável de controle linear [^46], uma vez que `E_ψ[L(Z)] = ∫ L(ζ)ψ(ζ) dζ = ∫ h(ζ) dζ = 1` (assumindo `L` não dependa de `x` aqui para simplificar) [^48].

### Conclusão

As técnicas de Amostragem por Importância e Razão de Verossimilhança oferecem abordagens alternativas para a estimação de Monte Carlo, com o potencial de reduzir significativamente a variância em comparação com a amostragem padrão [^47]. A IS alcança isso alterando a distribuição de probabilidade para focar nas regiões mais importantes, enquanto o método LR utiliza a razão de verossimilhança, muitas vezes para facilitar o cálculo de derivadas [^48]. No entanto, o sucesso da IS depende criticamente da escolha cuidadosa da nova densidade de probabilidade `ψ(·)`. Uma escolha inadequada pode levar à instabilidade, especialmente devido ao comportamento da razão de verossimilhança nas caudas da distribuição [^47]. Além disso, a dependência da `ψ` ótima em `x` complica sua aplicação direta em problemas de otimização SAA. Embora poderosas em teoria, essas técnicas exigem análise cuidadosa e validação para aplicação prática bem-sucedida.

### Referências

[^1]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 155). Elsevier. Contexto: Definição do problema estocástico (5.1) e SAA (5.2).
[^2]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 156-159). Elsevier. Contexto: Propriedades de convergência e consistência dos estimadores SAA (valor ótimo e soluções ótimas).
[^3]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 156). Elsevier. Contexto: Convergência pontual e uniforme, estimador não viesado `f_N(x)`.
[^4]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 157-158). Elsevier. Contexto: Consistência do valor ótimo e soluções ótimas (Teorema 5.3).
[^5]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 159). Elsevier. Contexto: Consistência no caso convexo (Teorema 5.4).
[^6]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 160-161). Elsevier. Contexto: SAA com conjunto viável estimado `X_N` (Teorema 5.5).
[^7]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 161). Elsevier. Contexto: Restrições de valor esperado `g_i(x) = E[G_i(x, ξ)]`.
[^8]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 162). Elsevier. Contexto: Restrições de chance (probabilísticas) e sua formulação SAA.
[^9]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 163). Elsevier. Contexto: Asintótica do valor ótimo SAA `v_N`, CLT para `f_N(x)`.
[^10]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 163-164). Elsevier. Contexto: Viés descendente do estimador `v_N`, `E[v_N] <= v*`.
[^11]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 164-165). Elsevier. Contexto: Asintótica de primeira ordem de `v_N` via CLT funcional e Teorema Delta (Teorema 5.7).
[^12]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 166). Elsevier. Contexto: Introdução à assintótica de segunda ordem.
[^13]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 166-167). Elsevier. Contexto: Teorema Delta de segunda ordem e derivação da assintótica (Teorema 5.8).
[^14]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 168). Elsevier. Contexto: Matriz de covariância do gradiente e distribuição assintótica do viés.
[^15]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 169). Elsevier. Contexto: Caso com restrições explícitas, condições LICQ e complementaridade estrita.
[^16]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 170). Elsevier. Contexto: Programas Estocásticos Minimax.
[^17]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 171). Elsevier. Contexto: Consistência e assintótica para o caso minimax convexo-côncavo (Teorema 5.10).
[^18]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 172). Elsevier. Contexto: Problema com restrições de valor esperado, Lagrangiano.
[^19]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 173). Elsevier. Contexto: Assintótica para problemas convexos com restrições (Teorema 5.11), uso de amostras independentes vs. comuns para restrições (Remark 9).
[^20]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 174). Elsevier. Contexto: Introdução a Equações Generalizadas Estocásticas (SGE).
[^21]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 175). Elsevier. Contexto: SGE SAA, consistência das soluções (Teorema 5.12).
[^22]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 176). Elsevier. Contexto: Regularidade forte para SGE (Definição 5.13).
[^23]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 177). Elsevier. Contexto: Existência e unicidade de solução SAA sob regularidade forte (Teorema 5.14, 5.15), assintótica dos estimadores SGE.
[^24]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 178). Elsevier. Contexto: Assintótica para desigualdades variacionais poliédricas.
[^25]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 179). Elsevier. Contexto: Condições KKT como SGE, regularidade forte e condições de segunda ordem.
[^26]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 180). Elsevier. Contexto: Métodos de Amostragem Monte Carlo, números aleatórios comuns (CRN) vs. amostras independentes.
[^27]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 181). Elsevier. Contexto: Introdução às taxas exponenciais de convergência e estimativas de tamanho de amostra.
[^28]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 181-182). Elsevier. Contexto: Taxas exponenciais para conjuntos viáveis finitos, uso de grandes desvios (LD).
[^29]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 183). Elsevier. Contexto: Ligação com função taxa de LD, Teorema 5.16, condição (M3).
[^30]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 184). Elsevier. Contexto: Estimativa de tamanho de amostra (Teorema 5.17), dependência logarítmica em |X| e α, dependência em (σ/ε)².
[^31]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 185). Elsevier. Contexto: Extensão para conjuntos viáveis gerais (não finitos), condições (M4, M5).
[^32]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 186). Elsevier. Contexto: Teorema 5.18 - estimativa de tamanho de amostra no caso geral.
[^33]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 187). Elsevier. Contexto: Discussão sobre a complexidade estimada `O(σ²/ε²)`.
[^34]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 188). Elsevier. Contexto: Discussão sobre conservadorismo da estimativa, modificação (M6), Corolário 5.19.
[^38]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 192). Elsevier. Contexto: Soluções ótimas sharp, convergência exponencial finita.
[^39]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 193). Elsevier. Contexto: Caso poliédrico/linear por partes.
[^40]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 193-194). Elsevier. Contexto: Métodos Quasi-Monte Carlo (QMC), erro determinístico vs. estocástico.
[^41]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 195). Elsevier. Contexto: Discrepância estrela (star discrepancy), Teorema de Koksma.
[^42]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 196). Elsevier. Contexto: Variação no sentido de Hardy e Krause, Teorema de Hlawka.
[^43]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 197). Elsevier. Contexto: Sequências de baixa discrepância (van der Corput, Halton), taxas de erro QMC.
[^44]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 198). Elsevier. Contexto: QMC randomizado para estimativa de erro.
[^45]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 199). Elsevier. Contexto: Latin Hypercube Sampling (LHS).
[^46]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 200). Elsevier. Contexto: Método de Variáveis Aleatórias de Controle Linear; Introdução a Importance Sampling e Likelihood Ratio.
[^47]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 201). Elsevier. Contexto: Detalhes da Amostragem por Importância, derivação da pdf ótima, sensibilidade e instabilidade.
[^48]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 202). Elsevier. Contexto: Aplicação do método LR para estimar derivadas, exemplo de programação linear de dois estágios.
[^49]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (pp. 202-203). Elsevier. Contexto: Análise de Validação - estimativa do gap de otimalidade.
[^50]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 204). Elsevier. Contexto: Discussão sobre viés e variância na estimativa do gap.
[^51]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 205). Elsevier. Contexto: Uso de bounds determinísticos (baseados em subgradientes) para `v_N`.
[^52]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 206). Elsevier. Contexto: Validação para problemas minimax e com restrições de expectativa.
[^53]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 207). Elsevier. Contexto: Teste Estatístico de Condições de Otimalidade (KKT).
[^54]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 208). Elsevier. Contexto: Estimador do gradiente, matriz de covariância, região de confiança.
[^55]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 209). Elsevier. Contexto: Estatística de teste para `∇f(x) ∈ K(x)`, distribuição qui-quadrado, p-valor.
[^56]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 210). Elsevier. Contexto: Problemas com Restrições de Chance.
[^57]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 211). Elsevier. Contexto: Abordagem SAA para restrições de chance, semicontinuidade, consistência.
[^58]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 212). Elsevier. Contexto: Dificuldades com descontinuidade de `p_N(x)`, abordagem de aproximação convexa.
[^59]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 213). Elsevier. Contexto: Lema de Helly, unicidade da solução ótima SAA (F1, F2).
[^60]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 214). Elsevier. Contexto: Teorema de Campi e Garatti (Teorema 5.32).
[^61]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 215). Elsevier. Contexto: Prova do Teorema 5.32.
[^62]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 216). Elsevier. Contexto: Estimativas de tamanho de amostra para garantir viabilidade.
[^63]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 217). Elsevier. Contexto: Validação de solução ótima - verificação de viabilidade (bounds superiores para `p(x)`).
[^64]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 218). Elsevier. Contexto: Validação - bounds inferiores para `v*`.
[^65]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 219). Elsevier. Contexto: Esquema de bound inferior baseado em quantil (Proposição 5.33).
[^67]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 221). Elsevier. Contexto: Método SAA aplicado a Programação Estocástica Multiestágio, esquemas de amostragem condicional.
[^68]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 222). Elsevier. Contexto: Análise de consistência para 3 estágios.
[^69]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 223). Elsevier. Contexto: Viés descendente, comparação de variância entre amostragem condicional independente vs. idêntica.
[^70]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 224). Elsevier. Contexto: Exemplo de seleção de portfólio multiestágio.
[^71]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 225). Elsevier. Contexto: Análise de viés para função utilidade potência.
[^72]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 226). Elsevier. Contexto: Validação estatística para multiestágio, bounds superiores via políticas viáveis.
[^73]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 227). Elsevier. Contexto: Estimativas de complexidade para multiestágio, crescimento exponencial.
[^74]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 228). Elsevier. Contexto: Condições para análise de complexidade em 3 estágios (M\'1-M\'8).
[^75]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 229). Elsevier. Contexto: Bound exponencial uniforme (Teorema 7.67), estimativa de tamanho de amostra para 3 estágios.
[^76]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 230). Elsevier. Contexto: Método de Aproximação Estocástica (SA), oráculo estocástico.
[^77]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 231). Elsevier. Contexto: Algoritmo SA clássico (5.281), análise de convergência inicial.
[^78]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 232). Elsevier. Contexto: Análise de convergência sob convexidade forte, taxa `O(1/j)`.
[^79]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 233). Elsevier. Contexto: Exemplo, lentidão da convergência com `γ_j = θ/j`, abordagem SA robusta.
[^80]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 234). Elsevier. Contexto: Análise da abordagem robusta (Nemirovski/Yudin), bounds de erro esperado.
[^81]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 235). Elsevier. Contexto: Política de passo constante, taxa `O(N^{-1/2})`, comparação com SAA via Chebyshev.
[^82]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 236). Elsevier. Contexto: Método Mirror Descent SA, normas gerais, função geradora de distância.
[^83]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 237). Elsevier. Contexto: Função de entropia, prox-função, prox-mapping.
[^84]: Shapiro, A. (2009). Statistical Inference. *In Handbooks in Operations Research and Management Science* (p. 238). Elsevier. Contexto: Lema chave para análise do Mirror Descent (Lema 5.38).
[^85]: Shapiro, A. (2