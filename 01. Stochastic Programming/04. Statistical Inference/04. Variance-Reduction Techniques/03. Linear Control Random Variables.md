## Método das Variáveis Aleatórias de Controle Linear

### Introdução

Este capítulo aprofunda uma técnica específica de redução de variância conhecida como o **método das variáveis aleatórias de controle linear** (Linear Control Random Variables Method). No contexto da inferência estatística para problemas de programação estocástica, como o problema geral $Min_{x \in X} \{f(x) := E[F(x, \xi)]\}$ [^1], a estimação precisa do valor esperado $f(x)$ é fundamental. A abordagem de Aproximação por Média Amostral (SAA) [^2] depende da qualidade dessas estimativas. Como discutido na introdução da Seção 5.5 [^3], técnicas de redução de variância visam melhorar a eficiência dos estimadores de Monte Carlo, como $f_N(x)$. O método de controle linear explora a correlação entre a função de interesse $F(x, \xi)$ e uma outra variável aleatória, a **variável de controle** $A(x, \xi)$, que possui média zero [^4]. O objetivo é construir um novo estimador com variância menor que o estimador original por média amostral.

### Conceitos Fundamentais

#### Definição e Propriedades Básicas

Suponha que dispomos de uma função mensurável $A(x, \xi)$ tal que seu valor esperado seja zero para todo $x \in X$, ou seja, $E[A(x, \xi)] = 0$ [^4]. Para qualquer escalar $t \in \mathbb{R}$, podemos definir uma nova variável aleatória controlada $Z_t(x, \xi)$ como:
$$ Z_t(x, \xi) = F(x, \xi) + tA(x, \xi) $$
O valor esperado desta variável controlada é:
$$ E[Z_t(x, \xi)] = E[F(x, \xi)] + tE[A(x, \xi)] = f(x) + t \cdot 0 = f(x) $$ [^5].
Isso demonstra que $Z_t(x, \xi)$ é um estimador não enviesado (unbiased) para $f(x)$, independentemente do valor de $t$. O ponto central é analisar a variância deste novo estimador. Utilizando a fórmula padrão para a variância da soma de variáveis aleatórias, temos:
$$ Var[Z_t(x, \xi)] = Var[F(x, \xi) + tA(x, \xi)] = Var[F(x, \xi)] + t^2Var[A(x, \xi)] + 2t Cov(F(x, \xi), A(x, \xi)) $$ [^6].

#### Otimização do Parâmetro de Controle

O objetivo é selecionar o parâmetro $t$ de forma a minimizar a variância $Var[Z_t(x, \xi)]$ [^7]. Para encontrar o valor ótimo $t^*$, derivamos a expressão da variância em relação a $t$ e igualamos a zero:
$$ \frac{d}{dt} Var[Z_t(x, \xi)] = 2t Var[A(x, \xi)] + 2 Cov(F(x, \xi), A(x, \xi)) = 0 $$
Resolvendo para $t$, obtemos o **parâmetro de controle ótimo** $t^*$:
$$ t^* = -\frac{Cov(F(x, \xi), A(x, \xi))}{Var[A(x, \xi)]} $$
Podemos reescrever $t^*$ em termos do coeficiente de correlação $\rho_{F,A}(x) := Corr(F(x, \xi), A(x, \xi))$ [^9]. Sabendo que $Cov(Y, Z) = Corr(Y, Z) \sqrt{Var(Y)Var(Z)}$, temos:
> $$ t^* := -\rho_{F,A}(x) \left[ \frac{Var(F(x, \xi))}{Var(A(x, \xi))} \right]^{1/2} $$ [^8].

#### Redução da Variância

Substituindo $t^*$ na expressão da variância $Var[Z_t(x, \xi)]$, obtemos a variância mínima alcançável com este método:
> $$ Var[F(x, \xi) + t^*A(x, \xi)] = Var[F(x, \xi)] [1 - \rho_{F,A}(x)^2] $$ [^10].

Esta fórmula é crucial pois demonstra que a **redução de variância** depende diretamente do quadrado do coeficiente de correlação entre $F(x, \xi)$ e a variável de controle $A(x, \xi)$. Quanto mais forte a correlação (positiva ou negativa), ou seja, quanto mais próximo $|\rho_{F,A}(x)|$ estiver de 1, maior será a redução na variância. *Portanto, a eficácia do método das variáveis de controle linear está intrinsecamente ligada à capacidade de encontrar uma variável de controle $A(x, \xi)$ que seja altamente correlacionada com a função de interesse $F(x, \xi)$* [^13, ^18]. Se a correlação for nula, $t^*=0$ e nenhuma redução de variância é obtida.

#### Implementação Prática e Estimação

Na prática, o valor ótimo $t^*$ geralmente não é conhecido, pois depende das variâncias e covariâncias populacionais [^11]. Contudo, para um dado $x \in X$ e uma amostra $\xi^1, \dots, \xi^N$, podemos estimar $t^*$ utilizando as estimativas amostrais da covariância e das variâncias [^11]. Seja $\hat{t}$ a estimativa resultante de $t^*$. O estimador de $f(x)$ baseado na variável de controle linear torna-se então:
$$ \hat{f}^*(x) := \frac{1}{N} \sum_{j=1}^{N} [F(x, \xi^j) + \hat{t} A(x, \xi^j)] $$ [^12].
Este estimador $\hat{f}^*(x)$ terá uma variância menor que o estimador SAA padrão $f_N(x)$ se $F(x, \xi)$ e $A(x, \xi)$ forem, de fato, altamente correlacionados [^13].

#### Limitações e Considerações

A aplicação do método de controle linear enfrenta algumas limitações importantes.
Primeiramente, o estimador $\hat{t}$ (e, por conseguinte, $t^*$) depende do ponto $x$ e da amostra gerada $\xi^1, \dots, \xi^N$ [^14]. Isso torna a incorporação direta deste método em procedimentos de otimização SAA, onde $x$ varia, bastante complexa [^15]. Por essa razão, *o método das variáveis aleatórias de controle linear é principalmente adequado para estimar o valor esperado $f(x)$ em um ponto fixo $x$* [^16].
Em segundo lugar, se a mesma amostra $\xi^1, \dots, \xi^N$ for utilizada tanto para estimar $\hat{t}$ quanto para calcular $\hat{f}^*(x)$, o estimador $\hat{f}^*(x)$ pode apresentar um leve viés (bias) em relação a $f(x)$ [^17].
Finalmente, o sucesso do método depende crucialmente da identificação de uma função $A(x, \xi)$ apropriada, que satisfaça $E[A(x, \xi)]=0$ e possua alta correlação com $F(x, \xi)$ [^18]. A escolha de tal função é altamente dependente do problema específico em questão [^19].

#### Exemplo e Extensões

Um exemplo de aplicação surge em problemas de programação estocástica de dois estágios com recurso da forma (2.1)-(2.2). Se o vetor aleatório $h = h(\omega)$ e a matriz $T = T(\omega)$ no problema de segundo estágio (2.2) forem independentemente distribuídos, e $\mu := E[h]$, então a função $A(x, \xi) := (h - \mu)^T Tx$, onde $\xi = (h, T)$, pode ser usada como variável de controle [^20]. Sua média é zero devido à independência: $E[(h - \mu)^T Tx] = E[(h - \mu)^T] E[T] x = 0^T E[T] x = 0$ [^20]. A correlação desta $A(x, \xi)$ com a função objetivo $F(x, \xi)$ (o valor ótimo do segundo estágio) precisaria ser avaliada para determinar a eficácia.

O procedimento descrito pode ser estendido de forma direta para o caso em que múltiplas variáveis de controle $A_1(x, \xi), \dots, A_m(x, \xi)$, cada uma com média zero e correlacionada com $F(x, \xi)$, estão disponíveis [^21]. Neste caso, busca-se uma combinação linear $\sum_{i=1}^m t_i A_i(x, \xi)$ que maximize a redução de variância, o que geralmente envolve conceitos de regressão múltipla para determinar os coeficientes ótimos $t_1^*, \dots, t_m^*$.

### Conclusão

O método das variáveis aleatórias de controle linear é uma técnica de redução de variância que pode ser eficaz quando é possível identificar uma variável auxiliar $A(x, \xi)$ com média zero e fortemente correlacionada com a função de interesse $F(x, \xi)$. A redução de variância potencial é quantificada por $Var[F(x, \xi)] [1 - \rho_{F,A}(x)^2]$ [^10], destacando a importância da correlação $\rho_{F,A}(x)$ [^18]. No entanto, suas limitações, especialmente a dependência do estimador do parâmetro de controle $\hat{t}$ em relação a $x$ e à amostra, e o potencial viés introduzido, restringem sua aplicabilidade direta em otimização SAA, tornando-o mais adequado para a estimação de $f(x)$ em pontos fixos [^15, ^16, ^17]. Apesar disso, representa uma ferramenta valiosa dentro do conjunto de técnicas disponíveis para melhorar a eficiência das simulações de Monte Carlo em programação estocástica.

### Referências

[^1]: Page 155, Capítulo 5, Seção 5.1, Equação (5.1). Consider the following stochastic programming problem: Min {f(x) := E[F(x, ξ)]}.
[^2]: Page 155, Capítulo 5, Seção 5.1, Equação (5.2). This leads to the so-called sample average approximation (SAA) Min f_N(x) := (1/N) Σ F(x, ξ^j).
[^3]: Page 198, Capítulo 5, Seção 5.5. In this section we briefly discuss some other variance-reduction techniques which seem to be useful in the SAA method.
[^4]: Page 200, Capítulo 5, Seção 5.5.2. Suppose that we have a measurable function A(x, ξ) such that E[A(x, ξ)] = 0 for all x ∈ X.
[^5]: Page 200, Capítulo 5, Seção 5.5.2. Then, for any t ∈ R, the expected value of F(x, ξ) + tA(x, ξ) is f (x)...
[^6]: Page 200, Capítulo 5, Seção 5.5.2. ...while Var[F(x, ξ) + tA(x, ξ)] = Var [F(x, ξ)] + t²Var [A(x, ξ)] + 2t Cov(F(x, ξ), A(x, ξ)).
[^7]: Page 200, Capítulo 5, Seção 5.5.2. It follows that the above variance attains its minimum, with respect to t, for...
[^8]: Page 200, Capítulo 5, Seção 5.5.2, Equação (5.158). t* := -ρ_F,A(x) [Var(F(x, ξ)) / Var(A(x, ξ))]^(1/2).
[^9]: Page 200, Capítulo 5, Seção 5.5.2. ...where ρ_F,A(x) := Corr(F(x, ξ), A(x, ξ))...
[^10]: Page 200, Capítulo 5, Seção 5.5.2, Equação (5.159). Var[F(x, ξ) + t*A(x, ξ)] = Var [F(x, ξ)] [1 – ρ_F,A(x)²].
[^11]: Page 200, Capítulo 5, Seção 5.5.2. For a given x ∈ X and generated sample ξ¹, ..., ξ^N, one can estimate, in the standard way, the covariance and variances appearing in the right-hand side of (5.158), and hence construct an estimate t̂ of t*.
[^12]: Page 200, Capítulo 5, Seção 5.5.2, Equação (5.160). Then f(x) can be estimated by f̂*(x) := (1/N) Σ [F(x, ξ^j) + t̂ A(x, ξ^j)].
[^13]: Page 200, Capítulo 5, Seção 5.5.2. By (5.159), the linear control estimator f̂*(x) has a smaller variance than f_N(x) if F(x, ξ) and A(x, ξ) are highly correlated with each other.
[^14]: Page 200, Capítulo 5, Seção 5.5.2. Let us make the following observations. The estimator t̂, of the optimal value t*, depends on x and the generated sample.
[^15]: Page 200, Capítulo 5, Seção 5.5.2. Therefore, it is difficult to apply linear control estimators in an SAA optimization procedure.
[^16]: Page 200, Capítulo 5, Seção 5.5.2. That is, linear control estimators are mainly suitable for estimating expectations at a fixed point.
[^17]: Page 200, Capítulo 5, Seção 5.5.2. Also, if the same sample is used in estimating t̂ and f̂*(x), then f̂*(x) can be a slightly biased estimator of f (x).
[^18]: Page 200, Capítulo 5, Seção 5.5.2. Of course, the above linear control procedure can be successful only if a function A(x, ξ), with mean zero and highly correlated with F(x, ξ), is available.
[^19]: Page 200, Capítulo 5, Seção 5.5.2. Choice of such a function is problem dependent.
[^20]: Page 200, Capítulo 5, Seção 5.5.2. For instance, one can use a linear function A(x, ξ) := λ(ξ)ᵀx. Consider, for example, two-stage stochastic programming problems with recourse of the form (2.1)-(2.2). Suppose that the random vector h = h(ω) and matrix T = T(ω), in the second-stage problem (2.2), are independently distributed, and let μ := E[h]. Then E[(h − μ)ᵀ] = E [(h − μ)ᵀ] E [T] = 0, and hence one can use A(x, ξ) := (h − μ)ᵀTx as the control variable.
[^21]: Page 200, Capítulo 5, Seção 5.5.2. Let us finally remark that the above procedure can be extended in a straightforward way to a case where several functions A₁(x,ξ),..., Am(x, ξ), each with zero mean and highly correlated with F(x, ξ), are available.

<!-- END -->