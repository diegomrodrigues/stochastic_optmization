## Propriedades Assintóticas do Valor Ótimo SAA e Estimadores Relacionados

### Introdução

Como estabelecido anteriormente, o problema de programação estocástica geral pode ser formulado como a minimização de uma função esperada, $f(x) := \mathbb{E}[F(x, \xi)]$ [^1], sobre um conjunto factível $X$. A abordagem de Aproximação por Média Amostral (Sample Average Approximation - SAA) substitui a função objetivo esperada $f(x)$ por sua aproximação de média amostral $f_N(x) := \frac{1}{N} \sum_{j=1}^{N} F(x, \xi^j)$ [^3], onde $\xi^1, \dots, \xi^N$ é uma amostra de $N$ realizações do vetor aleatório $\xi$ [^2]. O problema SAA resultante é então $\min_{x \in X} f_N(x)$ [^3]. Denotamos o valor ótimo do problema verdadeiro (5.1) por $v^*$ e o conjunto de soluções ótimas por $S$, enquanto $v_N$ e $\hat{S}_N$ denotam o valor ótimo e o conjunto de soluções ótimas do problema SAA (5.2), respetivamente [^5].

Este capítulo aprofunda as propriedades estatísticas dos estimadores SAA, com foco particular no comportamento assintótico do valor ótimo $v_N$ à medida que o tamanho da amostra $N$ cresce. Analisaremos a relação entre o erro de estimação da função objetivo SAA, $f_N(x)$, e as propriedades assintóticas de $v_N$, incluindo sua tendenciosidade (bias) e convergência [^11]. A compreensão dessas propriedades é crucial para avaliar a qualidade das soluções obtidas via SAA e para o desenvolvimento de métodos de inferência estatística.

### Propriedades do Estimador por Média Amostral $f_N(x)$

O estimador por média amostral $f_N(x)$ da função objetivo verdadeira $f(x)$ possui propriedades estatísticas fundamentais. Primeiramente, para qualquer $x \in X$ fixo, $f_N(x)$ é um estimador **não viesado** (unbiased) de $f(x)$, ou seja, $\mathbb{E}[f_N(x)] = f(x)$ [^4]. Esta propriedade decorre diretamente da linearidade da esperança e do pressuposto de que cada $\xi^j$ na amostra tem a mesma distribuição que $\xi$.

Assumindo que a amostra é iid (independentemente e identicamente distribuída) e que a variância de $F(x, \xi)$ é finita, a variância do estimador $f_N(x)$ é dada por $\sigma^2(x)/N$, onde $\sigma^2(x) := \text{Var}[F(x, \xi)]$ [^12]. A variância diminui à medida que o tamanho da amostra $N$ aumenta, o que é consistente com a Lei dos Grandes Números (Law of Large Numbers - LLN), que, sob condições de regularidade, garante que $f_N(x)$ converge pontualmente para $f(x)$ com probabilidade 1 (w.p. 1) quando $N \to \infty$ [^6].

Além disso, o Teorema do Limite Central (Central Limit Theorem - CLT) descreve o comportamento distribucional de $f_N(x)$ para amostras grandes. Especificamente, temos que $N^{1/2} [f_N(x) - f(x)]$ converge em distribuição para uma variável aleatória $Y_x$ com distribuição normal de média 0 e variância $\sigma^2(x)$, denotada por $Y_x \sim \mathcal{N}(0, \sigma^2(x))$ [^13]. Isto implica que, para $N$ suficientemente grande, $f_N(x)$ possui uma distribuição aproximadamente normal com média $f(x)$ e variância $\sigma^2(x)/N$ [^14]. Esta normalidade assintótica permite a construção de intervalos de confiança aproximados para $f(x)$, utilizando uma estimativa da variância $\sigma^2(x)$, como a variância amostral $\hat{\sigma}^2(x)$:
$$ \hat{\sigma}^2(x) := \frac{1}{N-1} \sum_{j=1}^N [F(x, \xi^j) - f_N(x)]^2 $$ [^15].
O intervalo de confiança aproximado de $100(1-\alpha)\%$ para $f(x)$ é então dado por:
$$ \left[ f_N(x) - z_{\alpha/2} \frac{\hat{\sigma}(x)}{\sqrt{N}}, f_N(x) + z_{\alpha/2} \frac{\hat{\sigma}(x)}{\sqrt{N}} \right] $$ [^15], onde $z_{\alpha/2}$ é o quantil $1-\alpha/2$ da distribuição normal padrão [^15]. O erro na estimação de $f(x)$ é, portanto, estocasticamente da ordem $O_p(N^{-1/2})$ [^15].

### Propriedades do Valor Ótimo SAA $v_N$

Analisemos agora o comportamento do valor ótimo $v_N = \inf_{x \in X} f_N(x)$ do problema SAA (5.2) [^16]. Uma propriedade crucial de $v_N$ é que ele constitui um estimador **viesado para baixo** (downward biased) do valor ótimo verdadeiro $v^*$ [^19]. Isso pode ser demonstrado considerando a desigualdade (5.22):\
$$ \inf_{x' \in X} \mathbb{E}[f_N(x')] \ge \mathbb{E}[\inf_{x \in X} f_N(x)] $$ [^17].
Como $\mathbb{E}[f_N(x')] = f(x')$ [^4] para qualquer $x'$, o lado esquerdo é igual a $\inf_{x' \in X} f(x') = v^*$. O lado direito é $\mathbb{E}[v_N]$. Portanto, temos a relação fundamental:
> **Tendenciosidade (Bias) do Valor Ótimo SAA:**
> $$ \mathbb{E}[v_N] \le v^* $$
[^18], [^19].

Esta desigualdade significa que, em média, o valor ótimo obtido pela resolução do problema SAA subestima o verdadeiro valor ótimo. Este viés para baixo é uma consequência da otimização sobre uma função objetivo que é ela própria uma estimativa ruidosa da função verdadeira, explorando favoravelmente o erro amostral. Esta propriedade faz de $v_N$ (ou sua média $\bar{v}_{N,M}$ sobre múltiplas replicações [^32]) um limite inferior estatístico válido para $v^*$ [^31].

O viés de $v_N$ não é constante; ele diminui à medida que o tamanho da amostra $N$ aumenta. Especificamente, para amostras iid, o viés diminui monotonicamente com $N$ [^20], [^34].

> **Proposição 5.6.** Seja $v_N$ o valor ótimo do problema SAA (5.2), e suponha que a amostra é iid. Então, $\mathbb{E}[v_N] \le \mathbb{E}[v_{N+1}] \le v^*$ para qualquer $N \in \mathbb{N}$ [^20].
>
> *Prova (Esboço):* A prova baseia-se em escrever $f_{N+1}(x)$ em termos de $f_N(x)$ e da $(N+1)$-ésima observação $\xi^{N+1}$, e usar a propriedade iid e a desigualdade de Jensen (ou a derivação explícita na página 164 [^21]) para mostrar que $\mathbb{E}[v_{N+1}] \ge \mathbb{E}[v_N]$ [^21]. A desigualdade $\mathbb{E}[v_{N+1}] \le v^*$ segue da discussão anterior [^18]. $\blacksquare$

A **consistência** do estimador $v_N$, ou seja, $v_N \to v^*$ w.p. 1 quando $N \to \infty$, é garantida sob condições de regularidade, como a convergência uniforme de $f_N(x)$ para $f(x)$ sobre $X$ (ou sobre conjuntos compactos relevantes) [^7], [^8], [^9].

A **análise assintótica de primeira ordem** fornece mais detalhes sobre a taxa de convergência e o viés assintótico. Sob as hipóteses (A1) e (A2) (integrabilidade quadrática de $F(\bar{x}, \xi)$ para algum $\bar{x}$ e condição de Lipschitz estocástica para $F(x, \xi)$ [^22]) e compacidade de $X$, o Teorema 5.7 estabelece que [^24]:
$$ N^{1/2} (v_N - v^*) \xrightarrow{\mathcal{D}} \inf_{x \in S} Y(x) $$ [^25], onde $Y$ é o processo Gaussiano limite do processo $N^{1/2}(f_N - f)$ visto como um elemento aleatório em $C(X)$ [^23].

A partir disso, pode-se derivar o comportamento assintótico do viés (Equação 5.29) [^26]:
$$ \mathbb{E}[v_N] - v^* = N^{-1/2} \mathbb{E}\left[\inf_{x \in S} Y(x)\right] + o(N^{-1/2}) $$ [^26].

Esta fórmula revela insights importantes sobre o viés:
1.  Se o conjunto de soluções ótimas $S$ **não for um singleton** (contiver mais de um ponto), o termo $\mathbb{E}[\inf_{x \in S} Y(x)]$ é tipicamente negativo, pois o mínimo de várias variáveis aleatórias normais (mesmo que de média zero) geralmente tem uma esperança negativa [^27]. Neste caso, o viés $\mathbb{E}[v_N] - v^*$ é estritamente negativo para $N$ grande e é da ordem $O(N^{-1/2})$ [^27], [^35]. O viés tende a ser maior (mais negativo) quanto maior for o conjunto $S$ [^28].
2.  Se $S = \{\bar{x}\}$ for um **singleton** (solução ótima única), então $\inf_{x \in S} Y(x) = Y(\bar{x})$. Como $\mathbb{E}[Y(\bar{x})] = 0$, o termo dominante $N^{-1/2}$ desaparece, e o viés $\mathbb{E}[v_N] - v^*$ é da ordem $o(N^{-1/2})$ [^27], [^35].
3.  Sob condições mais fortes, como as necessárias para a análise de segunda ordem (incluindo a unicidade da solução ótima $\bar{x}$ e diferenciabilidade de $f$ em $\bar{x}$), pode-se mostrar que o viés é da ordem $O(N^{-1})$ [^29], [^30].

Portanto, a taxa com que o viés $\mathbb{E}[v_N] - v^*$ decai para zero depende crucialmente da estrutura do conjunto de soluções ótimas $S$ do problema verdadeiro.

### Conclusão

A análise assintótica dos estimadores SAA revela propriedades fundamentais tanto da função objetivo amostral $f_N(x)$ quanto do valor ótimo amostral $v_N$. O estimador $f_N(x)$ é não viesado, sua variância decai como $1/N$, e ele é assintoticamente normal devido ao CLT. Por outro lado, o valor ótimo $v_N$ é um estimador viesado para baixo de $v^*$, com um viés que diminui monotonicamente à medida que $N$ aumenta. A taxa assintótica deste decaimento de viés é tipicamente $O(N^{-1/2})$ quando existem múltiplas soluções ótimas e mais rápida (pelo menos $o(N^{-1/2})$, e frequentemente $O(N^{-1})$ sob condições mais fortes) quando a solução ótima é única. Estas propriedades são essenciais para a interpretação dos resultados da SAA e para a construção de procedimentos de validação e inferência estatística em otimização estocástica.

### Referências

[^1]: Page 155, Eq (5.1)
[^2]: Page 155, Introdução da amostra $\xi^1, \dots, \xi^N$
[^3]: Page 155, Eq (5.2)
[^4]: Page 156, "We also have that E[ fn(x)] = f(x), i.e., fn(x) is an unbiased estimator of f(x)."
[^5]: Page 156, Definição de $v^*, S, v_N, \hat{S}_N$
[^6]: Page 156, "By the Law of Large Numbers..."
[^7]: Page 157, Section 5.1.1 Consistency of SAA Estimators, Eq (5.6)
[^8]: Page 157, Proposition 5.2
[^9]: Page 158, Theorem 5.3
[^10]: Page 163, Section 5.1.2 Title
[^11]: Page 163, "Consistency ... gives a certain assurance that the error of the estimation approaches zero ... does not give any indication of the magnitude of the error..."
[^12]: Page 163, "...the sample average estimator fn(x), of f(x), is unbiased and has variance σ²(x)/N, where σ²(x) := Var [F(x, ξ)]..."
[^13]: Page 163, Eq (5.19) and surrounding text.
[^14]: Page 163, "...fn(x) has asymptotically normal distribution, i.e., for large N, fn(x) has approximately normal distribution with mean f(x) and variance σ²(x)/N."
[^15]: Page 163, Eq (5.20), (5.21) and surrounding text.
[^16]: Page 163, "Consider now the optimal value vn of the SAA problem (5.2)."
[^17]: Page 163, Eq (5.22)
[^18]: Page 163, "Since E[fn(x)] = f(x), it follows that v* ≥ E[vn]. ... This means that vn provides a valid statistical lower bound for the optimal value v*..."
[^19]: Page 163, "...vn is a downward biased estimator of v*."
[^20]: Page 163, Proposition 5.6.
[^21]: Page 164, Proof of Proposition 5.6.
[^22]: Page 164, Assumptions (A1), (A2).
[^23]: Page 164, Functional CLT reference.
[^24]: Page 165, Theorem 5.7 statement.
[^25]: Page 165, Eq (5.25).
[^26]: Page 165, Eq (5.29).
[^27]: Page 165, Discussion following Eq (5.29).
[^28]: Page 165, "Moreover, the bias tends to be bigger the larger the set S is."
[^29]: Page 168, Remark 7.
[^30]: Page 165, Comparison between O(N^-1) and O(N^(-1/2)) bias.
[^31]: Page 203, "Consider the optimal value vn ... We have that v* ≥ E[vn]. ... This means that vn provides a valid statistical lower bound for the optimal value v*..."
[^32]: Page 203, Definition of $\\bar{v}_{N,M}$.
[^33]: Page 204, "There are two types of error in using $\\bar{v}_{N,M}$ as an estimator of v*, namely, the bias v* - E[vn] and variability of $\\bar{v}_{N,M}$..."
[^34]: Page 204, "It was shown in Proposition 5.6 ... it follows that the bias v* - E[vn] decreases monotonically with an increase of the sample size N."
[^35]: Page 204, Discussion on bias rates O(N^(-1/2)) vs O(N^-1).
<!-- END -->