## Capítulo 5.1.2: Análise Assintótica de Primeira e Segunda Ordem para o Valor Ótimo SAA

### Introdução

Nos capítulos anteriores, estabelecemos as propriedades fundamentais dos estimadores SAA (Sample Average Approximation). Vimos que o problema SAA [^2], definido como $Min_{x \in X} f_N(x) := \frac{1}{N} \sum_{j=1}^{N} F(x, \xi^j)$, serve como uma aproximação para o problema estocástico original ("verdadeiro") $Min_{x \in X} \{f(x) := E[F(x, \xi)]\}$ [^1]. Discutimos a **consistência** [^5] dos estimadores SAA, ou seja, as condições sob as quais o valor ótimo SAA $v_N$ [^4] e o conjunto de soluções ótimas $S_N$ [^4] convergem, com probabilidade 1 (w.p. 1), para seus correspondentes verdadeiros, $v^*$ [^3] e $S$ [^3], respectivamente, à medida que o tamanho da amostra $N$ tende ao infinito [^6, ^7, ^8, ^9, ^10]. A convexidade do problema SAA, que ocorre se as funções $f_N(\cdot)$ e os conjuntos $X_N$ forem convexos w.p. 1 [^11], facilita a demonstração da consistência sob certas condições [^9, ^11].

Embora a consistência garanta a convergência no limite, ela não fornece informações sobre a magnitude do erro de estimação para um tamanho de amostra $N$ finito [^13]. Este capítulo aprofunda a análise estatística de $v_N$, focando na sua distribuição assintótica e nas propriedades de seu viés. Investigaremos a **assintótica de primeira ordem**, que caracteriza a distribuição limite de $N^{1/2}(v_N - v^*)$, e a **assintótica de segunda ordem**, que oferece um refinamento dessa análise, fornecendo termos de ordem superior na expansão de $v_N$. Exploraremos as condições necessárias para derivar essas propriedades assintóticas, destacando o papel da estrutura do problema, como a convexidade, e das propriedades do integrando $F(x, \xi)$.

### Viés do Estimador SAA

Antes de mergulhar na análise assintótica, é importante notar que o valor ótimo SAA, $v_N$, é geralmente um estimador viesado para baixo do verdadeiro valor ótimo $v^*$ [^18]. Considerando a desigualdade $f_N(x') \ge \inf_{x \in X} f_N(x) = v_N$ para qualquer $x' \in X$ [^16], podemos tomar o valor esperado em ambos os lados e minimizar sobre $x' \in X$. Como $E[f_N(x')] = f(x')$ [^18], obtemos $\inf_{x' \in X} E[f_N(x')] \ge E[\inf_{x \in X} f_N(x)]$ [^17], o que implica $v^* \ge E[v_N]$ [^18].

> **Proposição 5.6:** Seja $v_N$ o valor ótimo do problema SAA (5.2), e suponha que a amostra $\xi^1, ..., \xi^N$ seja iid. Então, $E[v_N] \le E[v_{N+1}] \le v^*$ para qualquer $N \in \mathbb{N}$ [^19].

Esta proposição [^19] mostra que o viés $v^* - E[v_N]$ é não negativo e diminui monotonicamente com o aumento do tamanho da amostra $N$. A magnitude e a taxa de convergência desse viés estão intimamente ligadas à distribuição assintótica de $v_N$.

### Assintótica de Primeira Ordem

A análise assintótica de primeira ordem [^20] busca caracterizar a distribuição limite da diferença normalizada entre o estimador SAA e o verdadeiro valor ótimo, $N^{1/2}(v_N - v^*)$. A derivação desses resultados requer certas condições de regularidade sobre o integrando $F(x, \xi)$.

> **Suposições para Assintótica de Primeira Ordem:**
> (A1) Para algum ponto $\bar{x} \in X$, a esperança $E[F(\bar{x}, \xi)^2]$ é finita [^21].
> (A2) Existe uma função mensurável $C: \Xi \rightarrow \mathbb{R}_+$ tal que $E[C(\xi)^2]$ é finita e $|F(x, \xi) - F(x', \xi)| \le C(\xi)\|x - x'\|$ para todo $x, x' \in X$ e q.s. $\xi \in \Xi$ [^21].

A suposição (A1) garante que a variância de $F(\bar{x}, \xi)$ seja finita. A suposição (A2) é uma condição de **Lipschitz estocástica** sobre $F(x, \xi)$ em relação a $x$. Essas suposições implicam que a função de valor esperado $f(x)$ e a variância $\sigma^2(x) := \text{Var}[F(x, \xi)]$ são finitas para todo $x \in X$, e que $f(x)$ é Lipschitz contínua em $X$ com constante $\kappa := E[C(\xi)]$ [^22]. Se $X$ for compacto, o conjunto $S$ de minimizadores de $f(x)$ sobre $X$ é não vazio [^22].

Sob estas condições e assumindo que a amostra é iid e $X$ é compacto, o **Teorema Central do Limite funcional (functional CLT)** estabelece que o processo estocástico $N^{1/2}(f_N - f)$ converge em distribuição para um processo Gaussiano $Y$, visto como um elemento aleatório no espaço $C(X)$ das funções contínuas em $X$ munido da norma do supremo [^23]. A estrutura de covariância do processo limite $Y$ é a mesma da função aleatória $F(x, \xi)$ [^23].

Com base no CLT funcional e no método Delta aplicado à funcional $V(\psi) := \inf_{x \in X} \psi(x)$ (que é Lipschitz contínua e direcionalmente diferenciável pelo teorema de Danskin [^28]), podemos derivar a distribuição assintótica de $v_N$.

> **Teorema 5.7:** Seja $v_N$ o valor ótimo do problema SAA (5.2). Suponha que a amostra seja iid, o conjunto $X$ seja compacto, e as suposições (A1) e (A2) sejam satisfeitas [^24]. Então:
> $$ v_N = \inf_{x \in S} f_N(x) + o_p(N^{-1/2}) $$ [^25]
> $$ N^{1/2} (v_N - v^*) \xrightarrow{D} \inf_{x \in S} Y(x) $$ [^26]
> Se, adicionalmente, $S = \{\bar{x}\}$ for um singleton, então:
> $$ N^{1/2} (v_N - v^*) \xrightarrow{D} \mathcal{N}(0, \sigma^2(\bar{x})) $$ [^27]

O teorema [^24] revela que a distribuição assintótica de $N^{1/2}(v_N - v^*)$ depende crucialmente da natureza do conjunto de soluções ótimas $S$. Se $S$ contém mais de um ponto, a distribuição limite é dada pelo ínfimo do processo Gaussiano $Y(x)$ sobre $S$ [^26]. Como $Y(x)$ tem média zero para todo $x$, mas $\inf_{x \in S} Y(x)$ geralmente tem uma média negativa se $|S|>1$, isso leva a um viés assintótico [^29]:
$$ E[v_N] - v^* = N^{-1/2} E[\inf_{x \in S} Y(x)] + o(N^{-1/2}) $$ [^29]
Este viés é tipicamente da ordem $O(N^{-1/2})$ quando $S$ não é um singleton [^29, ^46]. Por outro lado, se $S = \{\bar{x}\}$ é um singleton, então $\inf_{x \in S} Y(x) = Y(\bar{x})$, que tem média zero. Neste caso, $v_N$ é assintoticamente normal [^27] e o viés $E[v_N] - v^*$ é de ordem $o(N^{-1/2})$ [^29].

### Assintótica de Segunda Ordem

A análise assintótica de segunda ordem [^30] visa fornecer uma expansão mais refinada para $v_N$, incluindo termos de ordem $N^{-1}$. Esta análise está intimamente relacionada à derivação da assintótica de primeira ordem para as *soluções* ótimas $\hat{x}_N$ do problema SAA [^31]. Para prosseguir, são necessárias suposições consideravelmente mais fortes [^31], incluindo a existência de uma solução ótima única $\bar{x}$ para o problema verdadeiro.

A análise baseia-se no **método Delta de segunda ordem** e na análise de perturbação de segunda ordem [^32]. Trabalhamos no espaço $W^{1,\infty}(U)$ de funções Lipschitz contínuas em um conjunto convexo compacto $U$ contendo $X$ em seu interior, munido da norma apropriada [^32].

> **Suposições para Assintótica de Segunda Ordem:** [^33]
> (S1) A função $f(x)$ é Lipschitz contínua em $U$, possui um minimizador único $\bar{x}$ sobre $X$, e é duas vezes continuamente diferenciável em $\bar{x}$.
> (S2) O conjunto $X$ é regular de segunda ordem em $\bar{x}$.
> (S3) A condição de crescimento quadrático vale em $\bar{x}$.
> (S4) A função $F(\cdot, \xi)$ é Lipschitz contínua em $U$ e diferenciável em $\bar{x}$ para q.s. $\xi \in \Xi$.

Sob estas condições, a funcional $V(\psi) = \inf_{x \in X} \psi(x)$ é Hadamard direcionalmente diferenciável de segunda ordem em $f$ tangencialmente a um subespaço apropriado $K \subset W^{1,\infty}(U)$ [^35]. A derivada direcional de segunda ordem, $V_f''(\delta)$, para uma direção $\delta \in K$, é dada por [^36]:
$$ V_f''(\delta) = \inf_{h \in C(\bar{x})} \{2h^T \nabla \delta(\bar{x}) + h^T \nabla^2 f(\bar{x}) h - s(-\nabla f(\bar{x}), T_X''(\bar{x}, h))\} $$
onde $C(\bar{x})$ é o cone crítico do problema verdadeiro em $\bar{x}$, $T_X''(\bar{x}, h)$ é o conjunto tangente de segunda ordem a $X$ em $\bar{x}$ na direção $h$, e $s(\cdot, A)$ é a função suporte do conjunto $A$ [^36]. Se $X$ for definido por restrições da forma $G(x) \in K$, existe uma forma dual para $V_f''(\delta)$ envolvendo o Hessiano da Lagrangiana e os multiplicadores de Lagrange $\Lambda(\bar{x})$ [^37].

Assumindo adicionalmente que $N^{1/2}(f_N - f)$ converge em distribuição para um elemento aleatório $Y$ de $W^{1,\infty}(U)$ [^34], o método Delta de segunda ordem leva aos seguintes resultados:

> **Teorema 5.8:** Suponha que as suposições (S1)-(S4) valem e $N^{1/2}(f_N - f) \xrightarrow{D} Y$ em $W^{1,\infty}(U)$ [^38]. Então:
> $$ v_N = f_N(\bar{x}) + \frac{1}{2} V_f''(f_N - f) + o_p(N^{-1}) $$ [^39]
> $$ N [v_N - f_N(\bar{x})] \xrightarrow{D} \frac{1}{2} V_f''(Y) $$ [^40]
> Se, adicionalmente, para cada $\delta \in K$, o problema de otimização tem solução ótima única $h = h(\delta)$, então:
> $$ N^{1/2} (\hat{x}_N - \bar{x}) \xrightarrow{D} h(Y) $$ [^41]

A equação [^39] fornece a expansão de segunda ordem para $v_N$. O termo $f_N(\bar{x})$ representa a contribuição de primeira ordem (pois $f_N(\bar{x}) - v^* = f_N(\bar{x}) - f(\bar{x}) = V_f'(f_N - f)$), enquanto $\frac{1}{2} V_f''(f_N - f)$ é o termo de segunda ordem. A equação [^40] caracteriza a distribuição assintótica deste termo de segunda ordem. Notavelmente, a equação [^41] fornece a distribuição assintótica da *solução* ótima SAA $\hat{x}_N$.

Pode-se mostrar que, sob condições de regularidade apropriadas, $N[v_N - f_N(\bar{x})]$ converge em distribuição para $v(Z)$ [^43], onde $Z \sim \mathcal{N}(0, \Sigma)$ com $\Sigma = \text{Cov}[\nabla F(\bar{x}, \xi)]$ [^42], e $v(Z)$ é o valor ótimo de um problema quadrático relacionado envolvendo $Z$, o Hessiano da Lagrangiana e o conjunto tangente de segunda ordem [^44]. O valor esperado $E[v(Z)]$ pode ser interpretado como o viés assintótico de $v_N$ (normalizado por $N$), que é da ordem $O(N^{-1})$ [^46]. Como $v(Z)$ é não positivo (pode-se verificar tomando $h=0$ em (5.40) [^46]), isso confirma que o viés $E[v_N] - v^*$ tende a ser negativo, mesmo no caso de solução única.

### Conclusão

A análise assintótica do valor ótimo SAA $v_N$ revela detalhes importantes sobre o erro de estimação $v_N - v^*$. A assintótica de primeira ordem, sob condições de Lipschitz [^21], mostra que a taxa de convergência é $O_p(N^{-1/2})$. A distribuição limite depende da unicidade da solução ótima verdadeira $S$: normalidade assintótica se $S$ for singleton [^27], ou o ínfimo de um processo Gaussiano caso contrário [^26]. O viés $E[v_N] - v^*$ é tipicamente $O(N^{-1/2})$ no caso não singleton e $o(N^{-1/2})$ no caso singleton [^29].

A assintótica de segunda ordem requer suposições mais fortes, incluindo diferenciabilidade de segunda ordem e regularidade [^33]. Ela fornece um refinamento da expansão de $v_N$, com termos de ordem $N^{-1}$ [^39, ^40], e está ligada à convergência de $N^{1/2}(\hat{x}_N - \bar{x})$ para a solução de um problema auxiliar [^41, ^45]. No caso de solução única, o viés $E[v_N] - v^*$ torna-se de ordem $O(N^{-1})$ [^46]. Essas análises são cruciais para entender o comportamento dos estimadores SAA e podem informar o desenvolvimento de métodos de redução de viés ou construção de intervalos de confiança mais precisos.

### Referências

[^1]: Min {f(x) := E[F(x,ξ)]}. x∈X (Eq 5.1, p. 155)
[^2]: Min fN(x) := (1/N) Σ F(x, ξ^j). x∈X (Eq 5.2, p. 155)
[^3]: We denote by v* and S the optimal value and the set of optimal solutions, respectively, of the true problem (5.1). (p. 156)
[^4]: ...and by vN and SN the optimal value and the set of optimal solutions, respectively, of the SAA problem (5.2). (p. 156)
[^5]: It is said that an estimator θ̂N of a parameter θ is consistent if θ̂N converges w.p. 1 to θ as N → ∞. (p. 157)
[^6]: ...if the pointwise LLN holds, then lim sup N→∞ vN ≤ v* w.p. 1. (Eq 5.6, p. 157)
[^7]: Proposition 5.2. Suppose that fN(x) converges to f(x) w.p. 1, as N → ∞, uniformly on X. Then vN converges to v* w.p. 1 as N → ∞. (p. 157)
[^8]: Theorem 5.3. Suppose that there exists a compact set C ... such that: (i) the set S ... is nonempty and is contained in C, (ii) the function f(x) is finite valued and continuous on C, (iii) fN(x) converges to f(x) w.p. 1, as N → ∞, uniformly in x ∈ C, and (iv) w.p. 1 for N large enough the set SN is nonempty and SN ⊂ C. Then vN → v* and D(SN, S) → 0 w.p. 1 as N → ∞. (p. 158)
[^9]: Theorem 5.4. Suppose that: (i) the integrand function F is random lower semicontinuous, (ii) for almost every ξ ∈ Ξ the function F(·, ξ) is convex, (iii) the set X is closed and convex, (iv) the expected value function f is lower semicontinuous..., (v) the set S ... is nonempty and bounded, and (vi) the LLN holds pointwise. Then vN → v* and D(SN, S) → 0 w.p. 1 as N → ∞. (p. 159)
[^10]: Theorem 5.5. Suppose that in addition to the assumptions of Theorem 5.3 the following conditions hold: (a) If xN ∈ XN and xN converges w.p. 1 to a point x, then x ∈ X. (b) For some point x̄ ∈ S there exists a sequence x̄N ∈ XN such that x̄N → x̄ w.p. 1. Then vN → v* and D(SN, S) → 0 w.p. 1 as N → ∞. (p. 160)
[^11]: The SAA problem (5.10) is convex if the functions fN(·) and the sets XN are convex w.p. 1. It is also possible to show consistency of the SAA estimators of problem (5.10) under the assumptions of Theorem 5.4 together with conditions (a) and (b) of the above Theorem 5.5, and convexity of the set XN. (p. 161)
[^12]: Section 5.1.2 Asymptotics of the SAA Optimal Value (p. 163)
[^13]: Consistency... does not give any indication of the magnitude of the error for a given sample. Suppose... let us fix a point x ∈ X. Then we have that the sample average estimator fN(x), of f(x), is unbiased and has variance σ²(x)/N, where σ²(x) := Var [F(x, ξ)] is supposed to be finite. Moreover, by the CLT we have that... (p. 163)
[^14]: N^(1/2) [fN(x) - f(x)] →D Yx, Yx ~ N(0, σ²(x)). (Eq 5.19, p. 163)
[^15]: This leads to the following (approximate) 100(1 – α)% confidence interval for f(x): [fN(x) - zα/2ô(x)/√N, fN(x) + zα/2ô(x)/√N], where ô²(x) := (N-1)^(-1) Σ[F(x, ξ^j) – fN(x)]². That is, the error of estimation of f(x) is (stochastically) of order Op(N^(-1/2)). (Eq 5.20-5.21 and text, p. 163)
[^16]: Consider now the optimal value vN of the SAA problem (5.2). Clearly we have that for any x' ∈ X the inequality fN(x') ≥ inf_{x∈X} fN(x) holds. (p. 163)
[^17]: By taking the expected value of both sides... and minimizing the left-hand side..., we obtain inf_{x'∈X} E[fN(x')] ≥ E[inf_{x∈X} fN(x)]. (Eq 5.22, p. 163)
[^18]: Since E[fN(x)] = f(x), it follows that v* ≥ E[vN]. In fact, typically, E[vN] is strictly less than v*, i.e., vN is a downward biased estimator of v*. (p. 163)
[^19]: Proposition 5.6. Let vN be the optimal value... suppose that the sample is iid. Then E[vN] ≤ E[vN+1] ≤ v* for any N ∈ N. (p. 163)
[^20]: First Order Asymptotics of the SAA Optimal Value (p. 164)
[^21]: We use the following assumptions about the integrand F: (A1) For some point x̄ ∈ X the expectation E[F(x̄, ξ)²] is finite. (A2) There exists a measurable function C : Ξ → R+ such that E[C(ξ)²] is finite and |F(x, ξ) - F(x', ξ)| ≤ C(ξ)||x - x'|| for all x, x' ∈ X and a.e. ξ ∈ Ξ. (p. 164)
[^22]: The above assumptions imply that the expected value f(x) and variance σ²(x) are finite valued for all x ∈ X. Moreover, it follows from (5.23) that |f(x) - f(x')| ≤ κ||x - x'||, ∀x, x' ∈ X, where κ := E[C(ξ)], and hence f(x) is Lipschitz continuous on X. If X is compact, we have then that the set S, of minimizers of f(x) over X, is nonempty. (p. 164)
[^23]: Moreover, by assumptions (A1) and (A2), compactness of X, and since the sample is iid, we have that N^(1/2)(fN – f) converges in distribution to Y, viewed as a random element of C(X). This is a so-called functional CLT. (p. 164)
[^24]: Theorem 5.7. Let vN be the optimal value... Suppose that the sample is iid, the set X is compact, and assumptions (A1) and (A2) are satisfied. Then the following holds: (p. 165)
[^25]: vN = inf_{x∈S} fN(x) + op(N^(-1/2)). (Eq 5.24, p. 165)
[^26]: N^(1/2) (vN - v*) →D inf_{x∈S} Y(x). (Eq 5.25, p. 165)
[^27]: If, moreover, S = {x̄} is a singleton, then N^(1/2) (vN - v*) →D N(0, σ²(x̄)). (Eq 5.26, p. 165)
[^28]: Proof. Proof is based on the functional CLT and the Delta theorem... Define the min-value function V(ψ) := inf_{x∈X} ψ(x)... V(·) is Lipschitz continuous... By the Danskin theorem..., V(·) is directionally differentiable... applying the Delta theorem to the min-function V(·) at μ := f... we obtain (5.25)... (p. 165)
[^29]: Under mild additional conditions... it follows from (5.25) that N^(1/2)E[vN – v*] tends to E[inf_{x∈S} Y(x)] as N → ∞, that is, E[vN] – v* = N^(-1/2)E[inf_{x∈S} Y(x)] + o(N^(-1/2)). (Eq 5.29, p. 165)
[^30]: Section 5.1.3 Second Order Asymptotics (p. 166)
[^31]: Formula (5.24) gives a first order expansion of... vN. In this section we discuss a second order term... It turns out that the second order analysis of vN is closely related to deriving (first order) asymptotics of optimal solutions... We assume in this section that the true... problem (5.1) has unique optimal solution x̄... In order to proceed with the second order analysis we need to impose considerably stronger assumptions. (p. 166)
[^32]: Our analysis is based on the second order Delta theorem... and second order perturbation analysis... we work with the space W^(1,∞)(U)... (p. 166)
[^33]: We make the following assumptions about the true problem: (S1) The function f(x) is Lipschitz continuous on U, has unique minimizer x̄ over x ∈ X, and is twice continuously differentiable at x̄. (S2) The set X is second order regular at x̄. (S3) The quadratic growth condition (7.70) holds at x̄. ... (S4) Function F(·, ξ) is Lipschitz continuous on U and differentiable at x̄ for a.e. ξ ∈ Ξ. (p. 166)
[^34]: We view fN as a random element of W^(1,∞)(U), and assume, further, that N^(1/2)(fN – f) converges in distribution to a random element Y of W^(1,∞)(U). (p. 166)
[^35]: Consider the min-function V : W^(1,∞)(U) → R defined as V(ψ) := inf_{x∈X} ψ(x)... (p. 166)
[^36]: By Theorem 7.23, under assumptions (S1)–(S3), the min-function V(·) is second order Hadamard directionally differentiable at f tangentially to the set K and we have the following formula for the second order directional derivative in a direction δ ∈ K: V_f''(δ) = inf_{h∈C(x̄)} {2h^T∇δ(x̄) + h^T∇²f(x̄)h – s( – ∇f(x̄), T_X''(x̄, h))}. (Eq 5.31, p. 166)
[^37]: Moreover, suppose that the set X is given in the form X := {x ∈ R^n : G(x) ∈ K}... Then... the optimal value of the right-hand side of (5.31) can be written in a dual form... V_f''(δ) = inf_{h∈C(x̄)} sup_{λ∈Λ(x̄)} {2h^T∇δ(x̄) + h^T∇²_x L(x̄, λ)h – s(λ, T_K''(h))}. (Eq 5.33-5.34, p. 167)
[^38]: Theorem 5.8. Suppose that the assumptions (S1)–(S4) hold and N^(1/2)(fN – f) converges in distribution to a random element Y of W^(1,∞)(U). Then... (p. 167)
[^39]: vN = fN(x̄) + (1/2)V_f''(fN – f) + op(N^(-1)). (Eq 5.35, p. 167)
[^40]: N[vN – fN(x̄)] →D (1/2)V_f''(Y). (Eq 5.36, p. 167)
[^41]: Moreover, suppose that for every δ ∈ K the problem in the right-hand side of (5.31) has unique optimal solution h = h(δ). Then N^(1/2) (x̂N - x̄) →D h(Y). (Eq 5.37, p. 167)
[^42]: Note also that by the (finite dimensional) CLT we have that N^(1/2)[∇fN(x̄) – ∇f(x̄)] converges in distribution to normal N(0, Σ) with the covariance matrix Σ = E[(∇F(x̄, ξ) – ∇f(x̄))(∇F(x̄, ξ) – ∇f(x̄))^T]. (Eq 5.38, p. 168)
[^43]: Let Z be a random vector having normal distribution, Z ~ N(0, Σ)... Then... we have that under appropriate regularity conditions, N[vN – fN(x̄)] →D v(Z). (Eq 5.39, p. 168)
[^44]: ...where v(Z) is the optimal value of the problem Min_{h∈C(x̄)} sup_{λ∈Λ(x̄)} {2h^T Z + h^T∇²_x L(x̄, λ)h – s(λ, T_K''(h))}. (Eq 5.40, p. 168)
[^45]: Moreover, if for all Z, problem (5.40) possesses unique optimal solution h = h(Z), then N^(1/2) (x̂N - x̄) →D h(Z). (Eq 5.41, p. 168)
[^46]: Remark 7. Note that E[fN(x̄)] = f(x̄) = v*. Therefore, under... the assumption that the true problem has unique optimal solution x̄, we have by (5.39) that the expected value of the term N^(-1)v(Z) can be viewed as the asymptotic bias of vN. This asymptotic bias is of order O(N^(-1)). This can be compared with formula (5.29) for the asymptotic bias of order O(N^(-1/2)) when the set of optimal solutions... is not a singleton. Note also that v(·) is nonpositive; to see this, just take h = 0 in (5.40). (p. 168)

<!-- END -->