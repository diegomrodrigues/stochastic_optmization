## Estimativas de Tamanho Amostral e Complexidade para SAA sob Condições de Moment Generating Function

### Introdução

Nos capítulos anteriores e nas seções iniciais deste capítulo, introduzimos o problema de programação estocástica `Min {f(x) := E[F(x,ξ)]}` [^1] e sua aproximação via Sample Average Approximation (SAA), `Min fN(x) := (1/N) Σ F(x, ξj)` [^1]. Discutimos as propriedades estatísticas dos estimadores SAA, notavelmente a consistência do valor ótimo `v_N` e do conjunto de soluções ótimas `ŜN` sob condições de regularidade apropriadas, garantindo que `v_N → v*` e `D(ŜN, S) → 0` w.p. 1 quando `N → ∞` [^3, ^4, ^5].

Entretanto, a convergência assintótica, embora fundamental, não fornece uma quantificação do erro para um tamanho amostral `N` finito. Para aplicações práticas e para compreender a **complexidade** inerente à resolução de problemas estocásticos via SAA, é crucial derivar estimativas do tamanho amostral `N` necessário para atingir uma determinada precisão `ε` com uma probabilidade desejada `1 - α`. A Seção 5.3 introduziu taxas exponenciais de convergência [^27]. A análise inicial focou no caso de um conjunto factível `X` finito (Seção 5.3.1) [^27, ^28, ^29, ^30], que, embora instrutiva, é restritiva.

Este capítulo aprofunda a análise, concentrando-se nas estimativas de tamanho amostral para o caso geral onde `X` é um subconjunto limitado, não necessariamente finito, de Rⁿ (Seção 5.3.2) [^31]. Investigaremos como certas condições sobre a **moment-generating function (MGF)** de diferenças da função objetivo estocástica `F(x, ξ)` influenciam o tamanho amostral requerido e a complexidade computacional associada ao método SAA. Especificamente, exploraremos a condição central definida pela desigualdade `Mx\',x(t) ≤ exp(σ²x\',xt²/2)` [^31] e suas implicações. Analisaremos a estimativa de tamanho amostral resultante, destacando sua dependência nos parâmetros do problema como a variabilidade (`σ²`), a precisão desejada (`ε`), a dimensão (`n`) e o nível de confiança (`α`), e contrastando a complexidade resultante com a de métodos de otimização determinística [^34].

### Conceitos Fundamentais e Estimativas de Tamanho Amostral

Para derivar estimativas de tamanho amostral no caso geral, necessitamos de pressupostos mais fortes sobre o comportamento da função `F(x, ξ)`. Expandindo a discussão da Seção 5.3.2 [^31], detalharemos as condições chave e os resultados subsequentes.

**Condições Fundamentais (M4, M5)**

Duas hipóteses são centrais para a análise que se segue. A primeira (M5) impõe uma condição de regularidade do tipo Lipschitz sobre `F(x, ξ)` em relação a `x`.

> **(M5)** Existe uma função (mensurável) `κ: Ξ → R+` tal que sua moment-generating function `Mk(t)` é finita numa vizinhança de zero e `|F(x\', ξ) – F(x, ξ)| ≤ κ(ξ)||x\' - x||` para q.t.p. `ξ ∈ Ξ` e todos `x\', x ∈ X` [^31].

Esta hipótese implica que a função esperança `f(x)` é Lipschitz contínua em `X` com constante de Lipschitz `L = E[κ(ξ)]` [^32]. Adicionalmente, pelo teorema de Cramér sobre grandes desvios, para qualquer `L\' > L`, existe uma constante positiva `β = β(L\')` tal que `Pr(κ̂N > L\') ≤ exp(-Nβ)`, onde `κ̂N := N⁻¹ Σ κ(ξʲ)` [^32]. Isso garante que a constante de Lipschitz da função SAA `fN(·)` pode ser limitada por `L\'` com alta probabilidade para `N` suficientemente grande, pois `fN(x\') - fN(x) ≤ κ̂N ||x\' - x||` w.p. 1 [^32].

A segunda e mais crucial hipótese (M4) controla o comportamento das caudas da variável aleatória que representa a diferença entre o desvio de `F(x, ξ)` de sua média em dois pontos `x\'` e `x`.

> **(M4)** Para quaisquer `x\', x ∈ X`, existe uma constante `σx\',x > 0` tal que a **moment-generating function** `Mx\',x(t) = E[exp(tYx\',x)]` da variável aleatória `Yx\',x := [F(x\', ξ) − f(x\')] − [F(x, ξ) − f(x)]` satisfaz `Mx\',x(t) ≤ exp(σ²x\',xt²/2)`, `∀t ∈ R` [^31].

Esta condição é uma forma de requisito de cauda subgaussiana sobre a variável `Yx\',x`. Ela implica que a variância de `Yx\',x` é limitada por `σ²x\',x`. Uma consequência importante é a existência de um limite uniforme para estas constantes: `σ² := supx\',x∈X σ²x\',x < ∞`, assumindo que `X` é compacto [^32]. Note que `E[Yx\',x] = 0` [^32]. Se `Yx\',x` tem distribuição normal, a igualdade em (5.110) vale com `σ²x\',x = Var[Yx\',x]` [^32].

**Estimativa de Tamanho Amostral (Teorema 5.18)**

Sob as hipóteses (M1) (definida na Seção 5.3 [^27], garantindo que `f(x)` é bem definida e finita), (M4) e (M5), e assumindo que `X` é limitado com diâmetro `D`, podemos estabelecer uma estimativa para o tamanho amostral `N`.

> **Teorema 5.18.** Suponha que as hipóteses (M1), (M4)–(M5) valem, com a constante correspondente `σ²` definida em (5.113) [^32] sendo finita, o conjunto `X` tem diâmetro finito `D`, e sejam `ε > 0`, `δ ∈ [0, ε)`, `α ∈ (0, 1)`, `L\' > L := E[κ(ξ)]`, e `β = β(L\')` as constantes correspondentes. Então, para o tamanho amostral `N` satisfazendo
> $$\
> N \\ge \\frac{8\\sigma^2}{(\\varepsilon - \\delta)^2} \\left[ n \\ln\\left(\\frac{8eL\'D}{\\varepsilon - \\delta}\\right) + \\ln\\left(\\frac{2}{\\alpha}\\right) \\right] + \\beta^{-1} \\ln\\left(\\frac{2}{\\alpha}\\right)\
> $$ (Baseado em (5.116) [^32] e (5.119) [^33])
> segue que `Pr(ŜδN ⊂ Sε) ≥ 1 − α` [^33].

Este teorema garante que, com probabilidade pelo menos `1 - α`, qualquer solução `δ`-ótima do problema SAA é também uma solução `ε`-ótima do problema verdadeiro, desde que `N` seja suficientemente grande conforme a estimativa. A prova, esboçada em [^32, ^33], envolve a construção de uma `ν`-rede no conjunto `X` (com `ν = (ε - δ)/(4L\')`), a aplicação do resultado do caso finito (Teorema 5.17 [^30]) a um problema reduzido definido nesta rede, e o controle da constante de Lipschitz `κ̂N` via (5.114) [^32]. A estimativa final (5.116) [^32] relaciona `N` com `σ²`, `ε`, `δ`, `n` (dimensão de `x`), `D`, `L\'`, `β` e `α`.

**Análise de Complexidade**

A estimativa de tamanho amostral (5.116) [^32] fornece insights sobre a **complexidade** de resolver o problema estocástico via SAA. Analisando a dependência nos parâmetros, observamos que para uma escolha comum como `δ = ε/2`, o termo dominante na estimativa é proporcional a `σ² / (ε - δ)² = 4σ² / ε²` [^34].

> Isto sugere uma complexidade de ordem `O(σ²/ε²)` em relação à precisão desejada `ε` [^34].

Este resultado contrasta fortemente com a otimização determinística (convexa), onde a complexidade é tipicamente limitada em termos de `ln(ε⁻¹)` [^34]. A dependência quadrática inversa em `ε` parece ser inevitável para métodos baseados em amostragem Monte Carlo [^30, ^34].

Por outro lado, a estimativa (5.116) [^32] mostra uma dependência linear na dimensão `n` do espaço de decisão (dentro do logaritmo) e uma dependência linear em `ln(α⁻¹)` [^34]. Isto implica que aumentar o nível de confiança, por exemplo, de 99% para 99.99%, requer apenas um aumento modesto no tamanho amostral (por um fator de `ln(100) ≈ 4.6`) [^34]. A constante `σ²` reflete a variabilidade do problema; maior variabilidade inerente (maior `σ²`) naturalmente exige um `N` maior para a mesma garantia de precisão [^34].

**Casos Especiais e Refinamentos**

Uma forma particular da hipótese (M4) é a (M6), que surge naturalmente quando `F(x, ξ)` satisfaz uma condição Lipschitz mais forte.

> **(M6)** Existe constante `λ > 0` tal que para quaisquer `x\', x ∈ X` a moment-generating function `Mx\',x(t)` de `Yx\',x` satisfaz `Mx\',x(t) ≤ exp (λ² ||x\' – x ||²t²/2)`, `∀t ∈ R` [^34].

Neste caso, `σ²x\',x = λ²||x\' - x||²`, e podemos tomar a constante uniforme `σ² = λ²D²` [^34]. Sob (M1), (M5) e (M6), o Corolário 5.19 [^34] fornece a estimativa:
$$\
N \\ge \\frac{O(1)\\lambda^2 D^2}{(\\varepsilon - \\delta)^2} \\left[ n \\ln\\left(\\frac{O(1)LD}{\\varepsilon - \\delta}\\right) + \\ln\\left(\\frac{1}{\\alpha}\\right) \\right]\
$$ (5.122) [^34]

Um caso ainda mais específico ocorre se a constante de Lipschitz `κ(ξ)` na hipótese (M5) for independente de `ξ`, ou seja, `κ(ξ) = L` constante [^35]. Neste caso, `|F(x\', ξ) – F(x, ξ)| ≤ L ||x\' − x ||` [^35]. Segue-se da desigualdade de Hoeffding (estimativa (7.186) [^35]) que (M6) vale com `λ = 2L` [^35]. A estimativa de tamanho amostral (5.122) torna-se então:
$$\
N \\ge \\frac{O(1)L^2 D^2}{(\\varepsilon - \\delta)^2} \\left[ n \\ln\\left(\\frac{O(1)LD}{\\varepsilon - \\delta}\\right) + \\ln\\left(\\frac{1}{\\alpha}\\right) \\right]\
$$ (Baseado em (5.126) [^35])

É importante notar que, embora as estimativas (5.116), (5.122), (5.126) assumam que `X` é limitado, para problemas convexos esta hipótese pode ser relaxada substituindo `X` por um level set `Sª = {x ∈ X : f(x) ≤ v* + a}` para algum `a > ε`, desde que `Sª` tenha diâmetro finito `D*a` (Remark 14 [^35]). As hipóteses (M4)-(M6) precisariam então ser verificadas apenas neste conjunto. Além disso, a verificação de (M4) pode ser simplificada se o conjunto ótimo `S` for conhecido e fechado, bastando verificar a condição para `x ∈ X \\ Sε\'` e `x\' = u(x)` onde `u` é uma projeção em `S` (Remark 15 [^35]).

Finalmente, a Remark 13 [^33] aponta que as estimativas ainda se mantêm, com constantes possivelmente diferentes, se a condição da MGF em (M4) ou (M6) valer apenas num intervalo finito `t ∈ [-a, a]`, desde que `0 < ε - δ < aσ²`.

### Conclusão

Este capítulo detalhou as estimativas de tamanho amostral para o método SAA no contexto geral de problemas de programação estocástica com conjuntos factíveis compactos, sob a hipótese chave de um controle subgaussiano (M4) sobre as diferenças da função objetivo estocástica. Demonstramos que o tamanho amostral `N` necessário para garantir que uma solução `δ`-ótima da SAA seja `ε`-ótima para o problema verdadeiro com probabilidade `1 - α` escala como `O(σ²/ε²)`. Esta dependência quadrática inversa na precisão `ε` é uma característica fundamental dos métodos Monte Carlo e contrasta com a dependência logarítmica encontrada em otimização determinística. A análise também revelou dependências lineares na dimensão `n` (logaritmicamente) e em `ln(α⁻¹)`, e uma dependência direta na variância `σ²` do problema. Embora estas estimativas sejam frequentemente conservadoras na prática [^27], elas fornecem um entendimento crucial da complexidade teórica da abordagem SAA e dos fatores que influenciam o esforço computacional necessário. Para avaliação prática da qualidade de uma solução para um `N` específico, métodos de validação, como os discutidos na Seção 5.6 [^48], são frequentemente empregados.

### Referências
[^1]: (5.1) e (5.2), página 155.
[^2]: Página 156.
[^3]: Seção 5.1.1, página 157.
[^4]: Teorema 5.3 e Proposição 5.1, páginas 158, 156.
[^5]: Teorema 5.4, página 159.
[^6]: (5.10), página 160.
[^7]: Remark 5, página 161.
[^8]: Páginas 161-162.
[^9]: Seção 5.1.2, página 163.
[^10]: Suposições (A1), (A2) e texto seguinte, página 164.
[^11]: Teorema 5.7 e texto seguinte, página 165.
[^12]: Seção 5.1.3, página 166.
[^13]: Teorema 5.8, página 167.
[^14]: Remark 7, página 168.
[^15]: Página 169.
[^16]: Seção 5.1.4, página 170.
[^17]: Teorema 5.10 e texto anterior, página 171.
[^18]: Página 172.
[^19]: Teorema 5.11 e Remark 9, página 173.
[^20]: Seção 5.2, página 174.
[^21]: Seção 5.2.1, página 175.
[^22]: Definição 5.13 e texto seguinte, página 176.
[^23]: Teorema 5.14, Teorema 5.15 e Seção 5.2.2, página 177.
[^24]: Página 178.
[^25]: Página 179.
[^26]: Seção 5.3, página 180.
[^27]: Texto introdutório da Seção 5.3 e Seção 5.3.1, página 181.
[^28]: Página 182.
[^29]: Hipótese (M3) e texto seguinte, página 183.
[^30]: Teorema 5.17, Remark 10 e Remark 11, página 184.
[^31]: Seção 5.3.2, Hipóteses (M4) e (M5), página 185.
[^32]: Texto após (M5), (5.112)-(5.115) e Teorema 5.18, página 186.
[^33]: Prova do Teorema 5.18 e Remark 13, página 187.
[^34]: Discussão após Teorema 5.18, Hipótese (M6) e Corolário 5.19, página 188.
[^35]: Exemplo Lipschitz constante, Remark 14 e Remark 15, página 189.
[^36]: Corolário 5.20 e texto seguinte, página 190.
[^37]: Exemplo 5.21 e Seção 5.3.3, página 191.
[^38]: Teorema 5.23, Teorema 5.24 e texto, página 192.
[^39]: Seção 5.4, página 193.
[^40]: Página 194.
[^41]: Definição 5.25 e Teorema 5.26, página 195.
[^42]: Teorema 5.27 e texto, página 196.
[^43]: Definição 5.28 e texto, página 197.
[^44]: Seção 5.5, página 198.
[^45]: Seção 5.5.1, página 199.
[^46]: Seção 5.5.2 e Seção 5.5.3, página 200.
[^47]: Página 201.
[^48]: Seção 5.6 e Seção 5.6.1, página 202.
[^49]: Página 203.
[^50]: Página 204.
[^51]: Remark 16, página 205.
[^52]: Página 206.
[^53]: Seção 5.6.2, página 207.
[^54]: Página 208.
[^55]: Página 209.
[^56]: Seção 5.7, página 210.
[^57]: Proposição 5.29 e Proposição 5.30, página 211.
[^58]: Página 212.
[^59]: Lemma 5.31 e Hipóteses (F1), (F2), página 213.
[^60]: Teorema 5.32, página 214.
[^61]: Página 215.
[^62]: Página 216.
[^63]: Página 217.
[^64]: Página 218.
[^65]: Proposição 5.33, página 219.
[^66]: Página 220.
[^67]: Seção 5.8 e Seção 5.8.1, página 221.
[^68]: Página 222.
[^69]: Página 223.
[^70]: Exemplo 5.34, página 224.
[^71]: Página 225.
[^72]: Seção 5.8.2, página 226.
[^73]: Página 227.
[^74]: Hipóteses (M\'6)-(M\'8) e Teorema 5.35, página 228.
[^75]: Remark 17 e texto seguinte, página 229.
[^76]: Seção 5.9, Remark 18, página 230.
[^77]: (5.281) e texto seguinte, página 231.
[^78]: (5.296) e texto seguinte, página 232.
[^79]: Seção 5.9.2, página 233.
[^80]: Página 234.
[^81]: Seção "Constant Stepsizes and Error Estimates", página 235.
[^82]: (5.314) e Seção 5.9.3, página 236.
[^83]: (5.317)-(5.321), página 237.
[^84]: (5.322)-(5.327), página 238.
[^85]: (5.328)-(5.336), página 239.
[^86]: Proposição 5.39 e Seção "Constant Stepsize Policy", página 240.
[^87]: Exemplo 5.40, página 241.
[^88]: Seção "Comparison with the SAA Approach" e Teorema 5.41, página 242.
[^89]: Página 243.
[^90]: Seção 5.9.4, página 244.
[^91]: Lemma 5.42, página 245.
[^92]: Teorema 5.43, página 246.
[^93]: Página 247.
[^94]: Remark 19, página 248.
[^95]: Exercícios, página 249.
[^96]: Exercícios, página 250.
[^97]: Exercícios, página 251.
[^98]: Exercícios, página 252.

<!-- END -->