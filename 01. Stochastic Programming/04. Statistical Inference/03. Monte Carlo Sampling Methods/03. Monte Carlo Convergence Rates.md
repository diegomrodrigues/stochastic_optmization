```portuguese
## Taxas Exponenciais de Convergência e Estimativas de Tamanho Amostral em Amostragem Monte Carlo

### Introdução

Como discutido anteriormente no Capítulo 5, a Abordagem por Média Amostral (SAA - Sample Average Approximation) é uma técnica fundamental para resolver problemas de programação estocástica da forma $Min_{x \in X} \{f(x) := E[F(x, \xi)]\}$ [^1]. A SAA substitui a função objetivo esperada $f(x)$ por sua aproximação por média amostral $f_N(x) := \frac{1}{N} \sum_{j=1}^N F(x, \xi^j)$, baseada em uma amostra $\xi^1, ..., \xi^N$ do vetor aleatório $\xi$ [^1]. Vimos em seções anteriores (e.g., Seção 5.1.1) que, sob condições de regularidade suaves, o valor ótimo $\hat{v}_N$ e as soluções ótimas $\hat{S}_N$ do problema SAA (5.2) convergem quase certamente (w.p. 1) para seus correspondentes $v^*$ e $S$ do problema verdadeiro (5.1) à medida que o tamanho da amostra $N$ aumenta [^3, ^4, ^5, ^6]. No entanto, esses resultados de consistência não fornecem uma indicação da qualidade das soluções para um tamanho de amostra $N$ finito [^27].

Este capítulo aprofunda a análise das propriedades estatísticas dos estimadores SAA, focando especificamente nas **taxas exponenciais de convergência** e nas **estimativas de tamanho amostral** necessárias para garantir um certo nível de precisão e confiabilidade. Investigaremos como obter uma estimativa do tamanho amostral $N$ necessário para resolver o problema verdadeiro com uma dada acurácia, resolvendo o problema SAA [^27]. Distinguiremos entre o caso em que o conjunto viável $X$ é finito e o caso geral, onde $X$ pode ser um subconjunto contínuo de $\mathbb{R}^n$. Veremos que, no caso finito, o tamanho amostral $N$ requerido depende logaritmicamente da cardinalidade de $X$ e da probabilidade de tolerância $\alpha$. No caso geral, as estimativas podem ser derivadas sob hipóteses sobre a **função geradora de momentos (moment-generating function - MGF)** da variável aleatória relevante e a **continuidade Lipschitz** da função. Assumiremos ao longo deste capítulo, salvo indicação contrária, que a amostra $\xi^1, ..., \xi^N$ é independentemente e identicamente distribuída (iid) e que a função esperada $f(x)$ é bem definida e finita para todo $x \in X$ (Suposição M1) [^27].

### Taxas Exponenciais e Estimativas de Tamanho Amostral no Caso de um Conjunto Viável Finito

Nesta seção, consideramos o cenário em que o conjunto viável $X$ é **finito**, embora sua cardinalidade $|X|$ possa ser muito grande [^27]. Para $\epsilon \ge 0$, denotamos por $S^\epsilon := \{x \in X : f(x) \le v^* + \epsilon\}$ o conjunto de soluções $\epsilon$-ótimas do problema verdadeiro (5.1), e por $\hat{S}_N^\delta := \{x \in X : f_N(x) \le \hat{v}_N + \delta\}$ o conjunto de soluções $\delta$-ótimas do problema SAA (5.2) para parâmetros $\epsilon > 0$ e $\delta \in [0, \epsilon]$ [^27]. Como $X$ é finito, os conjuntos $S^\epsilon$ e $\hat{S}_N^\delta$ são não vazios e finitos [^27].

Nosso objetivo é estimar a probabilidade do evento $\{\hat{S}_N^\delta \subset S^\epsilon\}$, que significa que qualquer solução $\delta$-ótima do problema SAA é também uma solução $\epsilon$-ótima do problema verdadeiro [^27]. Podemos escrever a negação deste evento como:
$$
\{\hat{S}_N^\delta \not\subset S^\epsilon\} = \bigcup_{x \in X \setminus S^\epsilon} \bigcap_{y \in X} \{f_N(x) \le f_N(y) + \delta\}
$$ [^27].
Aplicando a união de limites (union bound), obtemos:
$$
\Pr(\hat{S}_N^\delta \not\subset S^\epsilon) \le \sum_{x \in X \setminus S^\epsilon} \Pr\left( \bigcap_{y \in X} \{f_N(x) \le f_N(y) + \delta\} \right)
$$ [^27].

Para prosseguir, consideramos um mapeamento $u : X \setminus S^\epsilon \to X$. Assumimos que este mapeamento $u(\cdot)$ é escolhido de tal forma que
$$
f(u(x)) \le f(x) - \epsilon^*, \quad \forall x \in X \setminus S^\epsilon
$$ (5.88) [^28]
para algum $\epsilon^* > \delta$. Tal mapeamento sempre existe, pois $X$ é finito. Por exemplo, se $u: X \setminus S^\epsilon \to S$ mapeia para o conjunto de soluções ótimas $S$, então (5.88) vale com $\epsilon^* := \min_{x \in X \setminus S^\epsilon} f(x) - v^*$ [^28], e temos $\epsilon^* > \epsilon$ pois $X$ é finito [^28]. A escolha de $u(\cdot)$ oferece flexibilidade na derivação [^28].

Para cada $x \in X \setminus S^\epsilon$, definimos a variável aleatória
$$
Y(x, \xi) := F(u(x), \xi) - F(x, \xi)
$$ (5.90) [^28].
Observe que $E[Y(x, \xi)] = f(u(x)) - f(x) \le -\epsilon^*$ para todo $x \in X \setminus S^\epsilon$ [^28]. A média amostral correspondente é $\hat{Y}_N(x) := \frac{1}{N} \sum_{j=1}^N Y(x, \xi^j) = f_N(u(x)) - f_N(x)$ [^28].
A partir de (5.87), podemos majorar a probabilidade de falha:
$$
\Pr(\hat{S}_N^\delta \not\subset S^\epsilon) \le \sum_{x \in X \setminus S^\epsilon} \Pr\{f_N(x) - f_N(u(x)) \le \delta\} = \sum_{x \in X \setminus S^\epsilon} \Pr\{\hat{Y}_N(x) \ge -\delta\}
$$ (5.91) [^28].

Seja $I_x(\cdot)$ a função de taxa de grandes desvios (large deviations rate function) da variável aleatória $Y(x, \xi)$ [^28]. A desigualdade (5.91), juntamente com o limite superior de grandes desvios (referenciado como (7.173) no contexto), implica:
$$
1 - \Pr(\hat{S}_N^\delta \subset S^\epsilon) = \Pr(\hat{S}_N^\delta \not\subset S^\epsilon) \le \sum_{x \in X \setminus S^\epsilon} e^{-NI_x(-\delta)}
$$ (5.92) [^28].
Esta desigualdade é válida para qualquer tamanho de amostra $N$ [^28]. Para prosseguir, introduzimos a seguinte suposição sobre a função geradora de momentos (MGF):

**(M2)** Para cada $x \in X \setminus S^\epsilon$, a função geradora de momentos $E[e^{tY(x,\xi)}]$ da variável aleatória $Y(x, \xi)$ é finita em uma vizinhança de $t=0$ [^28].

Esta suposição (M2) é satisfeita, por exemplo, se o suporte $\Xi$ de $\xi$ é um subconjunto limitado de $\mathbb{R}^d$, ou se $Y(x, \cdot)$ cresce no máximo linearmente e $\xi$ tem uma distribuição de uma família exponencial [^28]. Sob a suposição (M2), podemos enunciar o seguinte teorema:

**Teorema 5.16.** Sejam $\epsilon > 0$ e $\delta \in [0, \epsilon)$ números não negativos. Então,
$$
1 - \Pr(\hat{S}_N^\delta \subset S^\epsilon) \le |X \setminus S^\epsilon| e^{-N\eta(\delta, \epsilon)} \le |X| e^{-N\eta(\delta, \epsilon)}
$$ (5.93) [^28]
onde
$$
\eta(\delta, \epsilon) := \min_{x \in X \setminus S^\epsilon} I_x(-\delta)
$$ (5.94) [^28].
Além disso, se $\delta < \epsilon^*$ e a suposição (M2) vale, então $\eta(\delta, \epsilon) > 0$ [^28].

**Prova.** A desigualdade (5.93) é uma consequência imediata de (5.92) e da definição de $\eta(\delta, \epsilon)$ [^29]. Se $\delta < \epsilon^*$, então $-\delta > -\epsilon^* \ge E[Y(x, \xi)]$ para todo $x \in X \setminus S^\epsilon$ [^29]. Pela teoria de grandes desvios (veja discussão acima da equação (7.178) no contexto), sob a suposição (M2), $I_x(z) > 0$ para $z > E[Y(x, \xi)]$, o que implica $I_x(-\delta) > 0$ para todo $x \in X \setminus S^\epsilon$ [^29]. Como $X \setminus S^\epsilon$ é finito, o mínimo $\eta(\delta, \epsilon)$ é positivo [^29]. $\blacksquare$

Um corolário assintótico imediato da desigualdade (5.93) é:
$$
\limsup_{N \to \infty} \frac{1}{N} \ln [1 - \Pr(\hat{S}_N^\delta \subset S^\epsilon)] \le -\eta(\delta, \epsilon)
$$ (5.95) [^29].
Isso significa que a probabilidade de uma solução $\delta$-ótima da SAA não ser $\epsilon$-ótima para o problema verdadeiro decai **exponencialmente rápido** para zero à medida que $N \to \infty$ [^29]. É importante notar que, como é possível usar um mapeamento $u: X \setminus S^\epsilon \to S$ com $\epsilon^* > \epsilon$ (veja (5.89)), essa taxa exponencial de convergência se mantém mesmo para $\delta = \epsilon$, e em particular para $\delta = \epsilon = 0$ [^29]. Contudo, se $\delta = \epsilon$ e a diferença $\epsilon^* - \epsilon$ for pequena, a constante $\eta(\delta, \epsilon)$ pode ser próxima de zero [^29]. De fato, para $\delta$ próximo de $-E[Y(x, \xi)]$, podemos aproximar (usando a referência (7.178) do contexto):
$$
I_x(-\delta) \approx \frac{(-\delta - E[Y(x, \xi)])^2}{2\sigma_x^2} \ge \frac{(\epsilon^* - \delta)^2}{2\sigma_x^2}
$$ (5.96) [^29]
onde $\sigma_x^2 := \text{Var}[Y(x, \xi)] = \text{Var}[F(u(x), \xi) - F(x, \xi)]$ [^29].

Para obter estimativas explícitas de tamanho amostral, faremos a seguinte suposição:

**(M3)** Existe uma constante $\sigma > 0$ tal que para qualquer $x \in X \setminus S^\epsilon$, a função geradora de momentos $M_x(t)$ da variável aleatória $Y(x, \xi) - E[Y(x, \xi)]$ satisfaz
$$
M_x(t) \le \exp(\sigma^2 t^2 / 2), \quad \forall t \in \mathbb{R}
$$ (5.98) [^29].

Esta suposição implica que $\ln E[e^{tY(x,\xi)}] - tE[Y(x, \xi)] = \ln M_x(t) \le \sigma^2 t^2 / 2$ [^29]. Consequentemente, a função de taxa $I_x(\cdot)$ de $Y(x, \xi)$ satisfaz:
$$
I_x(z) \ge \sup_{t \in \mathbb{R}} \{t(z - E[Y(x, \xi)]) - \sigma^2 t^2 / 2\} = \frac{(z - E[Y(x, \xi)])^2}{2\sigma^2}, \quad \forall z \in \mathbb{R}
$$ (5.100) [^29].
Em particular, para $z = -\delta$, temos:
$$
I_x(-\delta) \ge \frac{(-\delta - E[Y(x, \xi)])^2}{2\sigma^2} \ge \frac{(\epsilon^* - \delta)^2}{2\sigma^2} \ge \frac{(\epsilon - \delta)^2}{2\sigma^2}
$$ (5.101) [^29]
onde a última desigualdade assume $\epsilon^* \ge \epsilon$. Consequentemente, a constante $\eta(\delta, \epsilon)$ satisfaz:
$$
\eta(\delta, \epsilon) \ge \frac{(\epsilon - \delta)^2}{2\sigma^2}
$$ (5.102) [^29].
Substituindo isso no limite (5.93) do Teorema 5.16, obtemos:
$$
1 - \Pr(\hat{S}_N^\delta \subset S^\epsilon) \le |X| e^{-N(\epsilon - \delta)^2 / (2\sigma^2)}
$$ (5.103) [^29].

Isso leva diretamente à seguinte estimativa de tamanho amostral:

**Teorema 5.17.** Suponha que as suposições (M1) e (M3) sejam válidas. Então, para $\epsilon > 0$, $0 \le \delta < \epsilon$, e $\alpha \in (0, 1)$, se o tamanho da amostra $N$ satisfizer
$$
N \ge \frac{2\sigma^2}{(\epsilon - \delta)^2} \ln \left( \frac{|X|}{\alpha} \right)
$$ (5.104) [^30]
então segue-se que
$$
\Pr(\hat{S}_N^\delta \subset S^\epsilon) \ge 1 - \alpha
$$ (5.105) [^30].

**Prova.** Definindo o lado direito de (5.103) como $\le \alpha$ e resolvendo a inequação resultante para $N$, obtemos (5.104). $\blacksquare$

> **Observação Crucial (Remark 10 no contexto):** Uma característica chave da estimativa (5.104) é que o tamanho amostral $N$ requerido depende **logaritmicamente** tanto do tamanho (cardinalidade) $|X|$ do conjunto viável quanto da probabilidade de tolerância (nível de significância) $\alpha$ [^30]. A constante $\sigma$, postulada na suposição (M3), mede, em certo sentido, a variabilidade do problema considerado [^30]. Se, para algum $x \in X$, a variável aleatória $Y(x, \xi)$ tem distribuição normal com média $\mu_x$ e variância $\sigma_x^2$, sua MGF é $\exp(\mu_x t + \sigma_x^2 t^2 / 2)$, e a MGF $M_x(t)$ em (M3) é $\exp(\sigma_x^2 t^2 / 2)$ [^30]. Nesse caso, $\sigma^2 := \max_{x \in X \setminus S^\epsilon} \sigma_x^2$ fornece o menor valor possível para a constante correspondente em (M3) [^30]. Se $Y(x, \xi)$ é limitada w.p. 1, digamos $|Y(x, \xi) - E[Y(x, \xi)]| \le b$, então pela desigualdade de Hoeffding (Proposição 7.63 e estimativa (7.186) no contexto), temos $M_x(t) \le \exp(b^2 t^2 / 2)$, e podemos tomar $\sigma^2 := b^2$ [^30]. Em qualquer caso, para $\epsilon > 0$ pequeno, temos por (5.96) que $I_x(-\delta)$ pode ser aproximado por baixo por $(\epsilon - \delta)^2 / (2\sigma_x^2)$ [^30].

Outra observação (Remark 11 no contexto) é que para, digamos, $\delta = \epsilon/2$, o lado direito de (5.104) é proporcional a $(\sigma/\epsilon)^2$ [^30]. Para métodos baseados em amostragem Monte Carlo, tal dependência de $\sigma$ e $\epsilon$ parece inevitável [^30]. A condição (5.98) pode ser substituída por uma condição mais geral $M_x(t) \le \exp(\psi(t))$ (Remark 12 no contexto), levando a estimativas da forma $N > \frac{1}{\psi^*(\epsilon-\delta)} \ln(|X|/\alpha)$ (5.109) [^31].

### Estimativas de Tamanho Amostral no Caso Geral

Suponha agora que $X$ seja um subconjunto limitado (não necessariamente finito) de $\mathbb{R}^n$, e que $f(x)$ seja finito para todo $x \in X$ [^31]. Procederemos de forma semelhante às derivações da seção 7.2.9 do contexto [^31]. Faremos as seguintes suposições:

**(M4)** Para quaisquer $x', x \in X$, existe uma constante $\sigma_{x',x} > 0$ tal que a função geradora de momentos $M_{x',x}(t) = E[e^{tY_{x',x}}]$ da variável aleatória $Y_{x',x} := [F(x', \xi) - f(x')] - [F(x, \xi) - f(x)]$ satisfaz
$$
M_{x',x}(t) \le \exp(\sigma_{x',x}^2 t^2 / 2), \quad \forall t \in \mathbb{R}
$$ (5.110) [^31].
Segue-se que $M_{x',x}(t) \le \exp(\sigma^2 t^2 / 2)$ (5.112) [^32], onde $\sigma^2 := \sup_{x', x \in X} \sigma_{x',x}^2$ (5.113) [^32]. Note que $E[Y_{x',x}] = 0$ [^32]. A suposição (M4) é ligeiramente mais forte que (M3) [^32].

**(M5)** Existe uma função (mensurável) $\kappa: \Xi \to \mathbb{R}_+$ tal que sua função geradora de momentos $M_\kappa(t)$ é finita para todo $t$ em uma vizinhança de zero e
$$
|F(x', \xi) - F(x, \xi)| \le \kappa(\xi) \|x' - x\|
$$ (5.111) [^31]
para quase todo $\xi \in \Xi$ e todos $x', x \in X$.
Esta suposição implica que $E[\kappa(\xi)]$ é finito e a função $f(x)$ é Lipschitz contínua em $X$ com constante de Lipschitz $L = E[\kappa(\xi)]$ [^32]. O valor ótimo $v^*$ é finito, desde que $X$ seja limitado [^32]. Pelo teorema de Cramér de grandes desvios, para qualquer $L' > E[\kappa(\xi)]$, existe uma constante positiva $\beta = \beta(L')$ tal que $\Pr(\hat{\kappa}_N > L') \le \exp(-N\beta)$ (5.114) [^32], onde $\hat{\kappa}_N := N^{-1} \sum_{j=1}^N \kappa(\xi^j)$ [^32]. Segue-se de (5.111) que w.p. 1, $|f_N(x') - f_N(x)| \le \hat{\kappa}_N \|x' - x\|$ (5.115) [^32], i.e., $f_N(\cdot)$ é Lipschitz contínua em $X$ com constante $\hat{\kappa}_N$ [^32].

Seja $D := \sup_{x, x' \in X} \|x' - x\|$ o diâmetro do conjunto $X$, que é finito pois $X$ é limitado [^32].

**Teorema 5.18.** Suponha que as suposições (M1) e (M4)-(M5) valham, com a constante $\sigma^2$ correspondente definida em (5.113) sendo finita, que $X$ tenha diâmetro finito $D$, e sejam $\epsilon > 0$, $\delta \in [0, \epsilon)$, $\alpha \in (0, 1)$, $L' > L := E[\kappa(\xi)]$, e $\beta = \beta(L')$ as constantes correspondentes. Seja $\rho > 0$ uma constante especificada em (5.118) (relacionada à cardinalidade de uma rede). Então, para o tamanho da amostra $N$ satisfazendo
$$
N \ge \frac{8\sigma^2}{(\epsilon - \delta)^2} \left[ n \ln\left(\frac{8\rho L'D}{\epsilon - \delta}\right) + \ln\left(\frac{2}{\alpha}\right) \right] \vee \left[ \beta^{-1} \ln\left(\frac{2}{\alpha}\right) \right]
$$ (5.116) [^32]
(onde $a \vee b := \max\{a, b\}$ [^32]), segue-se que $\Pr(\hat{S}_N^\delta \subset S^\epsilon) \ge 1 - \alpha$ [^32].

**Esboço da Prova.** A prova baseia-se em construir uma rede finita $X'$ em $X$ (v-net) com $M \le (\rho D / v)^n$ pontos [^32], onde $v = (\epsilon - \delta) / (4L')$. Aplica-se o Teorema 5.17 ao problema reduzido sobre $X'$ com parâmetros $\epsilon' = \epsilon - L'v$ e $\delta' = \delta + L'v$ [^33]. Usa-se a condição de Lipschitz de $f_N(\cdot)$ (controlada por (5.114) e (5.120)) para estender o resultado da rede $X'$ para todo o conjunto $X$ [^33]. $\blacksquare$

A estimativa (5.116) sugere uma complexidade de ordem $\sigma^2 / \epsilon^2$ em relação à acurácia desejada $\epsilon$ (para $\delta$ fixo, e.g., $\delta = \epsilon/2$) [^34]. Isso contrasta fortemente com a otimização determinística (convexa), onde a complexidade geralmente é limitada em termos de $\ln(\epsilon^{-1})$ [^34]. Parece que essa dependência de $\sigma$ e $\epsilon$ é inevitável para métodos baseados em amostragem Monte Carlo [^34]. Por outro lado, a estimativa (5.116) é **linear na dimensão $n$** do problema de primeiro estágio (via termo $n \ln(\cdot)$) e depende **linearmente de $\ln(\alpha^{-1})$** [^34]. Isso sugere que, usando técnicas de amostragem Monte Carlo, pode-se resolver programas estocásticos de dois estágios com acurácia razoável (e.g., 1% ou 2%) em tempo razoável, desde que: (a) sua variabilidade não seja muito grande, (b) tenha recurso relativamente completo, e (c) o problema SAA correspondente possa ser resolvido eficientemente [^34]. A estimativa (5.116) é, no entanto, frequentemente muito conservadora para cálculos práticos [^34].

Se, em vez de (M4), assumirmos a condição mais específica (M6): existe $\lambda > 0$ tal que $M_{x',x}(t) \le \exp(\lambda^2 \|x' - x\|^2 t^2 / 2)$ (5.121) [^34], então $\sigma^2 \le \lambda^2 D^2$ [^34], e obtemos o:

**Corolário 5.19.** Sob as suposições (M1) e (M5)-(M6), com $X$ de diâmetro finito $D$, para $\epsilon > 0$, $\delta \in [0, \epsilon)$, $\alpha \in (0, 1)$, e $L=E[\kappa(\xi)]$, o tamanho amostral $N$ satisfazendo
$$
N \ge \frac{O(1)\lambda^2 D^2}{(\epsilon - \delta)^2} \left[ n \ln\left(\frac{O(1)LD}{\epsilon - \delta}\right) + \ln\left(\frac{1}{\alpha}\right) \right]
$$ (5.122) [^34]
garante $\Pr(\hat{S}_N^\delta \subset S^\epsilon) \ge 1 - \alpha$ [^34].

Se a constante de Lipschitz em (M5) for determinística, $\kappa(\xi) = L$ (5.124) [^35], podemos tomar $\lambda = 2L$ (via desigualdade de Hoeffding (7.186)) [^35], resultando na estimativa
$$
N \ge \frac{O(1)L^2 D^2}{(\epsilon - \delta)^2} \left[ n \ln\left(\frac{O(1)LD}{\epsilon - \delta}\right) + \ln\left(\frac{1}{\alpha}\right) \right]
$$ (5.126) [^35].

Para problemas convexos, a suposição de $X$ ser limitado pode ser relaxada (Remark 14) [^35]. Se o valor ótimo $v^*$ for finito e o conjunto de níveis $S^a = \{x \in X : f(x) \le v^* + a\}$ tiver diâmetro finito $D_a^*$ para algum $a > \epsilon$, podemos aplicar os resultados ao problema reduzido sobre $S^a$ [^35]. Isso leva à estimativa (5.127) [^36], que depende de $D_a^*$ e $D_{a,\epsilon} := \sup_{x \in S^a \setminus S^{\epsilon'}} \text{dist}(x, S)$ [^36]. Se, adicionalmente, uma condição de crescimento como $f(x) \ge v^* + c [\text{dist}(x, S)]^\gamma$ (5.128) [^36] for válida, podemos obter estimativas mais explícitas como (5.129) [^36]. Para $\gamma=1$ (solução ótima *sharp*), a estimativa (5.131) não depende de $\epsilon$ [^36]. Para $\gamma=2$ (crescimento quadrático), o primeiro termo em (5.129) é da ordem $c^{-1} \epsilon^{-1} \lambda^2$ [^36].

### Conclusão

Este capítulo explorou as taxas de convergência e as estimativas de tamanho amostral associadas à abordagem SAA para programação estocástica. Demonstramos que, sob certas condições envolvendo funções geradoras de momento, a probabilidade de erro (ou seja, a probabilidade de uma solução $\delta$-ótima da SAA não ser $\epsilon$-ótima para o problema verdadeiro) decai exponencialmente com o tamanho da amostra $N$.

Derivamos estimativas explícitas para o tamanho amostral $N$ necessário para garantir uma probabilidade $1-\alpha$ de que $\hat{S}_N^\delta \subset S^\epsilon$. No caso especial de um conjunto viável finito $X$, $N$ depende logaritmicamente de $|X|$ e $\alpha$, e quadraticamente de $\sigma/(\epsilon-\delta)$. No caso geral de um conjunto viável $X$ limitado em $\mathbb{R}^n$, sob condições de Lipschitz e sobre a MGF, a estimativa para $N$ também exibe uma dependência quadrática em $1/(\epsilon-\delta)$, mas agora depende linearmente da dimensão $n$ e logaritmicamente de $1/\alpha$. Essas estimativas fornecem insights sobre a complexidade da resolução de problemas estocásticos via SAA, destacando a influência da variabilidade ($\sigma$), acurácia ($\epsilon, \delta$), confiabilidade ($\alpha$), tamanho do problema ( $|X|$ ou $n, D, L$) no tamanho amostral requerido. Embora muitas vezes conservadoras na prática, essas análises são cruciais para entender as propriedades teóricas e limitações da amostragem Monte Carlo.

### Referências
[^1]: OCR Page 1 (page 155)
[^2]: OCR Page 2 (page 156)
[^3]: OCR Page 3 (page 157)
[^4]: OCR Page 4 (page 158)
[^5]: OCR Page 5 (page 159)
[^6]: OCR Page 6 (page 160)
[^7]: OCR Page 7 (page 161)
[^8]: OCR Page 8 (page 162)
[^9]: OCR Page 9 (page 163)
[^10]: OCR Page 10 (page 164)
[^11]: OCR Page 11 (page 165)
[^12]: OCR Page 12 (page 166)
[^13]: OCR Page 13 (page 167)
[^14]: OCR Page 14 (page 168)
[^15]: OCR Page 15 (page 169)
[^16]: OCR Page 16 (page 170)
[^17]: OCR Page 17 (page 171)
[^18]: OCR Page 18 (page 172)
[^19]: OCR Page 19 (page 173)
[^20]: OCR Page 20 (page 174)
[^21]: OCR Page 21 (page 175)
[^22]: OCR Page 22 (page 176)
[^23]: OCR Page 23 (page 177)
[^24]: OCR Page 24 (page 178)
[^25]: OCR Page 25 (page 179)
[^26]: OCR Page 26 (page 180)
[^27]: OCR Page 27 (page 181)
[^28]: OCR Page 28 (page 182)
[^29]: OCR Page 29 (page 183)
[^30]: OCR Page 30 (page 184)
[^31]: OCR Page 31 (page 185)
[^32]: OCR Page 32 (page 186)
[^33]: OCR Page 33 (page 187)
[^34]: OCR Page 34 (page 188)
[^35]: OCR Page 35 (page 189)
[^36]: OCR Page 36 (page 190)
[^37]: OCR Page 37 (page 191)
[^38]: OCR Page 38 (page 192)
[^39]: OCR Page 39 (page 193)
[^40]: OCR Page 40 (page 194)
[^41]: OCR Page 41 (page 195)
[^42]: OCR Page 42 (page 196)
[^43]: OCR Page 43 (page 197)
[^44]: OCR Page 44 (page 198)
[^45]: OCR Page 45 (page 199)
[^46]: OCR Page 46 (page 200)
[^47]: OCR Page 47 (page 201)
[^48]: OCR Page 48 (page 202)
[^49]: OCR Page 49 (page 203)
[^50]: OCR Page 50 (page 204)
[^51]: OCR Page 51 (page 205)
[^52]: OCR Page 52 (page 206)
[^53]: OCR Page 53 (page 207)
[^54]: OCR Page 54 (page 208)
[^55]: OCR Page 55 (page 209)
[^56]: OCR Page 56 (page 210)
[^57]: OCR Page 57 (page 211)
[^58]: OCR Page 58 (page 212)
[^59]: OCR Page 59 (page 213)
[^60]: OCR Page 60 (page 214)
[^61]: OCR Page 61 (page 215)
[^62]: OCR Page 62 (page 216)
[^63]: OCR Page 63 (page 217)
[^64]: OCR Page 64 (page 218)
[^65]: OCR Page 65 (page 219)
[^66]: OCR Page 66 (page 220)
[^67]: OCR Page 67 (page 221)
[^68]: OCR Page 68 (page 222)
[^69]: OCR Page 69 (page 223)
[^70]: OCR Page 70 (page 224)
[^71]: OCR Page 71 (page 225)
[^72]: OCR Page 72 (page 226)
[^73]: OCR Page 73 (page 227)
[^74]: OCR Page 74 (page 228)
[^75]: OCR Page 75 (