## Capítulo 5.3: Cálculo da Função Média Amostral e Geração de Números Aleatórios Comuns

### Introdução

Como estabelecido anteriormente na Seção 5.1, o problema de programação estocástica `Min {f(x) := E[F(x, ξ)]}` [^3] pode ser aproximado pela **Sample Average Approximation (SAA)** [^3]:
$$ \text{Min}_{x \in X} f_N(x) := \frac{1}{N} \sum_{j=1}^{N} F(x, \xi^j) $$
onde $\xi^1, \dots, \xi^N$ é uma amostra de $N$ realizações do vetor aleatório $\xi$ [^3]. Esta amostra pode ser constituída por dados históricos ou, como frequentemente ocorre em aplicações práticas, pode ser gerada computacionalmente através de técnicas de **amostragem Monte Carlo** [^4].

Neste capítulo, focaremos nos métodos computacionais para calcular os valores da função média amostral $f_N(x)$ no contexto da amostragem Monte Carlo. Discutiremos duas abordagens principais e aprofundaremos a técnica conhecida como **Geração de Números Aleatórios Comuns (CRN - Common Random Number)**, destacando sua utilidade e propriedades estatísticas, particularmente na comparação de valores da função objetivo [^1].

### Conceitos Fundamentais

#### Métodos para Calcular $f_N(x)$

Quando a amostra $\xi^1, \dots, \xi^N$ é gerada por métodos Monte Carlo, existem duas maneiras principais de calcular os valores de $f_N(x)$ para diferentes pontos $x$ [^1]:

1.  **Armazenamento da Amostra Gerada:** A amostra $\xi^1, \dots, \xi^N$ pode ser gerada uma única vez e armazenada na memória do computador. Subsequentemente, sempre que um novo valor de $f_N(x)$ (para um $x$ diferente) precisar ser calculado, a mesma amostra armazenada é recuperada e utilizada [^1]. Este método garante que a mesma base amostral seja usada para todas as avaliações, mas pode exigir uma quantidade significativa de memória, especialmente para grandes valores de $N$ e alta dimensão de $\xi$.

2.  **Geração via Gerador de Números Pseudoaleatórios (PRNG):** Alternativamente, a amostra pode ser gerada "on-the-fly" sempre que necessário, utilizando um gerador de números pseudoaleatórios (PRNG) empregado [^1]. Um PRNG é um algoritmo determinístico que, a partir de um valor inicial chamado **semente (seed)**, produz uma sequência de números que aparenta ser aleatória. Utilizando um número de semente comum (**common seed number**), é possível regenerar exatamente a mesma sequência $\xi^1, \dots, \xi^N$ [^1]. Esta abordagem evita a necessidade de armazenamento extensivo da amostra.

#### Geração de Números Aleatórios Comuns (CRN)

A segunda abordagem descrita acima, quando utilizada para garantir que a *mesma* amostra $\xi^1, \dots, \xi^N$ seja usada para avaliar $f_N(x)$ em diferentes pontos, é conhecida como o método de **Geração de Números Aleatórios Comuns (CRN)** [^1]. A ideia de CRN é bem conhecida em simulação [^1].

A principal utilidade do CRN surge quando desejamos comparar os valores da função objetivo em dois pontos distintos, digamos $x_1$ e $x_2$ pertencentes a $X$ [^1]. Nesse cenário, o interesse reside frequentemente na diferença $f(x_1) - f(x_2)$, que é estimada pela diferença das médias amostrais $f_N(x_1) - f_N(x_2)$ [^1].

**Análise de Variância:**

Considere a estimação da diferença $f(x_1) - f(x_2)$ usando $f_N(x_1) - f_N(x_2)$. Este estimador é não-viciado para $f(x_1) - f(x_2)$ independentemente de as amostras usadas para calcular $f_N(x_1)$ e $f_N(x_2)$ serem as mesmas ou independentes [^1]. No entanto, a variância do estimador difere significativamente entre as duas abordagens.

1.  **Amostras Independentes:** Se $f_N(x_1)$ e $f_N(x_2)$ são calculados usando duas amostras independentes de tamanho $N$, então $f_N(x_1)$ e $f_N(x_2)$ são não correlacionados [^1]. A variância da diferença é dada por [^1]:
    $$ \text{Var}[f_N(x_1) - f_N(x_2)] = \text{Var}[f_N(x_1)] + \text{Var}[f_N(x_2)] \quad (5.83) $$

2.  **Números Aleatórios Comuns (CRN):** Se a mesma amostra (ou o mesmo seed no PRNG) é usada para calcular ambos $f_N(x_1)$ e $f_N(x_2)$, a variância da diferença é [^1]:
    $$ \text{Var}[f_N(x_1) - f_N(x_2)] = \text{Var}[f_N(x_1)] + \text{Var}[f_N(x_2)] - 2 \text{Cov}(f_N(x_1), f_N(x_2)) \quad (5.84) $$

O termo crucial na Eq. (5.84) é a covariância, $ \text{Cov}(f_N(x_1), f_N(x_2)) $. No caso de usar a mesma amostra (CRN), os estimadores $f_N(x_1)$ e $f_N(x_2)$ tendem a ser **positivamente correlacionados** [^1]. Isso ocorre porque, para um mesmo $\xi^j$, os valores $F(x_1, \xi^j)$ e $F(x_2, \xi^j)$ frequentemente variam de forma similar, especialmente se $x_1$ e $x_2$ estão próximos no espaço de decisão. Essa **correlação positiva** induzida pelo CRN tem uma consequência importante:

> *"...a variância da diferença é menor ao usar a mesma amostra devido à correlação positiva."* [^1]

Essa **redução de variância** é o principal benefício do CRN ao comparar estimativas da função objetivo. A magnitude dessa redução pode ser substancial [^2]:

> *"...A diferença entre os estimadores gerados por números aleatórios independentes e comuns ... pode ser especialmente dramática quando os pontos $x_1$ e $x_2$ estão próximos um do outro e, portanto, os estimadores de números aleatórios comuns são altamente positivamente correlacionados."* [^2]

Portanto, o método CRN é particularmente vantajoso em contextos onde comparações precisas entre $f(x_1)$ e $f(x_2)$ são necessárias, como em algoritmos de otimização que avaliam diferentes soluções candidatas ou em análises de sensibilidade onde se estuda o impacto de pequenas mudanças em $x$.

### Conclusão

O cálculo da função média amostral $f_N(x)$ em simulações Monte Carlo pode ser realizado armazenando a amostra gerada ou utilizando um gerador de números pseudoaleatórios com uma semente específica. A técnica de Geração de Números Aleatórios Comuns (CRN), que emprega a mesma amostra (ou semente) para avaliar $f_N(x)$ em diferentes pontos $x$, é fundamental para a comparação eficiente desses valores. Ao induzir uma correlação positiva entre os estimadores $f_N(x_1)$ e $f_N(x_2)$, o CRN reduz significativamente a variância da diferença $f_N(x_1) - f_N(x_2)$ em comparação com o uso de amostras independentes. Esta propriedade torna o CRN uma ferramenta valiosa para aumentar a precisão estatística em tarefas que envolvem a comparação de alternativas em problemas de otimização estocástica.

### Referências

[^1]: Let us also remark that values of the sample average function fn(x) can be computed in two somewhat different ways. The generated sample ξ¹, ..., ξN can be stored in the computer memory and called every time a new value (at a different point x) of the sample average function should be computed. Alternatively, the same sample can be generated by using a common seed number in an employed pseudorandom numbers generator. (This is why this approach is called the common random number generation method.) The idea of common random number generation is well known in simulation. That is, suppose that we want to compare values of the objective function at two points X1, X2 ∈ X. In that case we are interested in the difference f (x1) − f (x2) rather than in the individual values f(x₁) and f (x2). If we use sample average estimates fn(x1) and fn(x2) based on independent samples, both of size N, then fn(x1) and f√(x2) are uncorrelated and Var[fn(x1) - fn(x2)] = Var[fn(x1)] + Var[fn(x2)]. (5.83) On the other hand, if we use the same sample for the estimators fn(x1) and fn(x2), then Var[f(x1) - fn(x2)] = Var[fn(x1)] + Var[fn(x2)] - 2Cov (f(x1), f(x2)). (5.84) In both cases, f(x1) − f(x2) is an unbiased estimator of f(x₁) − f (x2). However, in the case of the same sample, the estimators fn(x1) and fn(x2) tend to be positively correlated with each other, in which case the variance in (5.84) is smaller than the one in (5.83). *(Context OCR page 26)*
[^2]: (5.83). The difference between the independent and the common random number generated estimators of f(x1) - f (x2) can be especially dramatic when the points x1 and x2 are close to each other and hence the common random number generated estimators are highly positively correlated. *(Context OCR page 27)*
[^3]: Consider the following stochastic programming problem: Min {f(x) := E[F(x,§)]}. (5.1) ... This leads to the so-called sample average approximation (SAA) Min fN(x) := (1/N) Σ F(x, ξ^j) (5.2) *(Context OCR page 1)*
[^4]: Suppose that we have a sample §¹, ..., §N of N realizations of the random vector ξ. This random sample can be viewed as historical data of N observations of §, or it can be generated in the computer by Monte Carlo sampling techniques. *(Context OCR page 1)*

<!-- END -->