## Capítulo 5.3: Métodos de Amostragem Monte Carlo

### Introdução

Continuando a análise das propriedades estatísticas dos estimadores da Aproximação por Média Amostral (SAA - Sample Average Approximation), conforme introduzido na Seção 5.1 [^1], este capítulo foca especificamente nos **Métodos de Amostragem Monte Carlo** como uma técnica fundamental para gerar as amostras necessárias para construir o problema SAA. O problema estocástico original que buscamos aproximar é dado por:

$$ \text{Min}_{x \in X} \{f(x) := \mathbb{E}[F(x, \xi)]\} $$ [^1]

onde $X$ é um subconjunto não vazio e fechado de $\mathbb{R}^n$, $\xi$ é um vetor aleatório com distribuição de probabilidade $P$ suportada em $\Xi \subset \mathbb{R}^d$, e $F: X \times \Xi \to \mathbb{R}$ [^1]. A abordagem SAA substitui a esperança matemática pela média amostral, resultando no problema:

$$ \text{Min}_{x \in X} \hat{f}_N(x) := \frac{1}{N} \sum_{j=1}^{N} F(x, \xi^j) $$ [^1]

As amostras $\xi^1, \ldots, \xi^N$ podem ser obtidas de dados históricos ou, como exploraremos aqui, geradas computacionalmente através de técnicas de Monte Carlo [^1]. Assumimos nesta seção que uma amostra aleatória $\xi^1, \ldots, \xi^N$ de $N$ realizações do vetor aleatório $\xi$ pode ser gerada no computador [^26]. O método de amostragem Monte Carlo realiza isso gerando uma sequência $U^1, U^2, \ldots$ de números aleatórios (ou pseudoaleatórios) independentes, uniformemente distribuídos no intervalo $[0,1]$, e então construindo a amostra $\xi^j$ por uma transformação apropriada [^26]. O problema SAA correspondente (5.2) é então utilizado como uma forma de aproximar o problema verdadeiro (5.1), reduzindo drasticamente o número de cenários a serem considerados em comparação com a distribuição original, potencialmente contínua ou com um número muito grande de cenários [^26]. A análise estatística dos problemas SAA construídos dessa forma é o foco, independentemente do algoritmo numérico específico aplicado para resolvê-los [^26]. Salvo indicação em contrário, assumimos nesta seção que a amostra aleatória $\xi^1, \ldots, \xi^N$ é **independentemente e identicamente distribuída (iid)** [^2, ^27].

### Fundamentos da Geração de Amostras Monte Carlo

A essência do método de amostragem Monte Carlo reside na geração de sequências de números aleatórios $U^1, U^2, \ldots$ que são independentes e seguem uma distribuição uniforme em $[0,1]$ [^26]. Na prática computacional, geradores de números pseudoaleatórios são utilizados, os quais produzem sequências determinísticas que mimetizam as propriedades de sequências verdadeiramente aleatórias [^26]. Embora qualquer gerador computacional seja uma máquina determinística finita e, portanto, a sequência gerada eventualmente se repetirá, os geradores modernos possuem períodos de ciclo extremamente longos, tornando este método viável e amplamente testado em inúmeras aplicações [^26].

Uma vez gerada a sequência $U^j$, a amostra do vetor aleatório original $\xi$ é construída através de uma **transformação apropriada** [^26]. Um método comum, particularmente para variáveis aleatórias unidimensionais (d=1), é o método da transformada inversa. Se $H(z) = \text{Pr}(\xi \le z)$ é a função de distribuição acumulada (cdf) de $\xi$, então a variável aleatória $\xi = H^{-1}(U)$, onde $H^{-1}(u) := \inf \{z : H(z) \ge u\}$, possui a cdf $H(\cdot)$ [^39]. Para vetores aleatórios multidimensionais, transformações mais complexas podem ser necessárias, frequentemente baseadas na decomposição da distribuição conjunta ou em métodos como aceitação-rejeição.

A amostra resultante $\xi^1, \ldots, \xi^N$, construída via Monte Carlo, é então usada diretamente na formulação do problema SAA (5.2) [^1, ^26]. É crucial entender que o problema SAA formulado com amostras geradas por Monte Carlo é ele próprio um problema aleatório, pois seu valor ótimo $\hat{v}_N$ e conjunto de soluções ótimas $\hat{S}_N$ dependem da amostra específica $\xi^1, \ldots, \xi^N$ gerada [^2].

### Propriedades Estatísticas e Análise com Amostras Monte Carlo

As propriedades estatísticas dos estimadores SAA $\hat{v}_N$ e $\hat{S}_N$, quando a amostra é gerada por métodos Monte Carlo (assumindo iid), foram extensivamente estudadas nas seções anteriores. Recapitulamos os pontos principais neste contexto:

1.  **Convergência e Consistência:** Sob condições de regularidade suaves, como continuidade de $F(x, \cdot)$ e condições de integrabilidade, a Lei dos Grandes Números (LLN) garante que $\hat{f}_N(x)$ converge pontualmente para $f(x)$ quase certamente (w.p. 1) quando $N \to \infty$ [^2]. Sob condições adicionais, como compacidade de $X$ e convergência uniforme de $\hat{f}_N(x)$ para $f(x)$ em $X$, temos a consistência do valor ótimo, $\hat{v}_N \to v^*$ w.p. 1 [^3], e a consistência do conjunto de soluções ótimas, $\mathbb{D}(\hat{S}_N, S) \to 0$ w.p. 1 [^4]. Resultados de consistência também se aplicam a casos convexos [^5] e problemas com conjuntos viáveis aleatórios $X_N$ [^6], desde que certas condições sejam satisfeitas. A geração iid por Monte Carlo satisfaz a premissa básica para a aplicação destes resultados [^27].

2.  **Viés (Bias):** O estimador SAA do valor ótimo $\hat{v}_N$ é geralmente viesado para baixo em problemas de minimização, ou seja, $\mathbb{E}[\hat{v}_N] \le v^*$ [^9]. Este viés $\mathbb{E}[\hat{v}_N] - v^*$ é não positivo e tende a diminuir monotonicamente (em magnitude) com o aumento do tamanho da amostra $N$, sob a suposição iid [^9]. A magnitude do viés e sua taxa de convergência para zero dependem das propriedades do problema, notavelmente se o conjunto de soluções ótimas $S$ do problema verdadeiro é um singleton ou não [^11].

3.  **Análise Assintótica:** O Teorema Central do Limite (CLT) fornece a distribuição assintótica de $\hat{f}_N(x)$ para um $x$ fixo [^9]. Mais poderosamente, sob condições como Lipschitz continuidade de $F(x, \xi)$ em $x$ (uniformemente em $\xi$ com constante integrável - A2 [^10]), o CLT funcional garante que o processo $N^{1/2}(\hat{f}_N - f)$ converge em distribuição para um processo Gaussiano $Y$ no espaço $C(X)$ de funções contínuas em $X$ (se $X$ for compacto) [^10]. Aplicando o Teorema Delta à função $V(\psi) = \inf_{x \in X} \psi(x)$, obtemos a distribuição assintótica de $N^{1/2}(\hat{v}_N - v^*)$, que é a distribuição de $\inf_{x \in S} Y(x)$ [^11]. Se $S = \{\bar{x}\}$ é um singleton, a distribuição limite é Normal $N(0, \sigma^2(\bar{x}))$, onde $\sigma^2(\bar{x}) = \text{Var}[F(\bar{x}, \xi)]$ [^11]. Isso implica que o erro de estimação é estocasticamente da ordem $O_p(N^{-1/2})$ [^9].

4.  **Estimativas de Tamanho de Amostra:** Embora a consistência garanta a convergência assintótica, ela não informa sobre a qualidade da solução para um $N$ finito [^27]. A Seção 5.3.1 introduz taxas exponenciais de convergência e estimativas de tamanho de amostra, particularmente para o caso de conjuntos viáveis $X$ finitos [^27]. O Teorema 5.17 [^30] fornece uma estimativa para o tamanho da amostra $N$ necessário para garantir que qualquer solução $\delta$-ótima do problema SAA ($\hat{x} \in \hat{S}_\delta^N$) seja também uma solução $\epsilon$-ótima do problema verdadeiro ($ \hat{x} \in S^\epsilon$) com probabilidade pelo menos $1-\alpha$:

    $$ N \ge \frac{2\sigma^2}{(\epsilon - \delta)^2} \ln \left( \frac{|X|}{\alpha} \right) $$ [^30]

    onde $\sigma^2$ é uma constante relacionada à variabilidade do problema (ver Assunção M3 [^29]). Esta estimativa destaca a dependência logarítmica na cardinalidade de $X$ (ou medidas de cobertura em casos mais gerais) e no nível de significância $\alpha$, mas uma dependência quadrática em $(\sigma/\epsilon)$ [^30]. Essa dependência em $\epsilon^{-2}$ parece ser inevitável para métodos baseados em amostragem Monte Carlo [^30].

### Considerações sobre Variância e Eficiência

A eficiência da amostragem Monte Carlo está intrinsecamente ligada à variância do estimador SAA. A variância de $\hat{f}_N(x)$ é $\sigma^2(x)/N$ para amostras iid [^9]. Reduzir essa variância permite obter estimativas mais precisas com o mesmo tamanho de amostra $N$, ou a mesma precisão com um $N$ menor.

Uma técnica relacionada, **Common Random Numbers (CRN)**, é particularmente útil ao *comparar* duas soluções, $x_1$ e $x_2$. Usar a mesma amostra $\xi^1, \ldots, \xi^N$ para calcular $\hat{f}_N(x_1)$ e $\hat{f}_N(x_2)$ geralmente resulta em uma variância menor para a diferença $\hat{f}_N(x_1) - \hat{f}_N(x_2)$ do que usar amostras independentes, especialmente se $x_1$ e $x_2$ estão próximos e $F(\cdot, \xi)$ tem alguma regularidade, pois $\hat{f}_N(x_1)$ e $\hat{f}_N(x_2)$ tendem a ser positivamente correlacionados [^26]. A variância da diferença é dada por $\text{Var}[\hat{f}_N(x_1)] + \text{Var}[\hat{f}_N(x_2)] - 2\text{Cov}(\hat{f}_N(x_1), \hat{f}_N(x_2))$ [^26].

No contexto da construção do próprio problema SAA, especialmente quando há restrições que também são aproximações de esperanças (como em (5.13) [^7] ou (5.17) [^8]), surge a questão de usar a mesma amostra Monte Carlo para a função objetivo e todas as restrições, ou usar amostras independentes. Embora a convergência w.p. 1 seja geralmente mantida em ambos os casos sob condições apropriadas, usar **amostras independentes** pode ser vantajoso do ponto de vista da variabilidade dos estimadores SAA resultantes (e.g., $\hat{v}_N$), pois pode eliminar termos de covariância potencialmente positivos que aumentariam a variância total [^7, ^19].

Outras técnicas, como Quasi-Monte Carlo [^39, ^43], Latin Hypercube Sampling [^45], Variáveis de Controle Lineares [^46] e Amostragem por Importância [^47], buscam melhorar a eficiência da amostragem Monte Carlo, seja reduzindo a variância ou melhorando a taxa de convergência, mas estão além do escopo detalhado desta introdução focada no método básico de Monte Carlo para SAA.

### Conclusão

Os métodos de amostragem Monte Carlo fornecem uma abordagem computacionalmente viável e teoricamente bem fundamentada para gerar as amostras necessárias para a formulação de problemas de Aproximação por Média Amostral (SAA). Ao gerar amostras iid $\xi^1, \ldots, \xi^N$ a partir da distribuição do vetor aleatório $\xi$ (muitas vezes via transformação de variáveis uniformes U[0,1]), o método permite aproximar problemas de programação estocástica complexos por problemas determinísticos (embora dependentes da amostra). As propriedades estatísticas dos estimadores SAA resultantes, como consistência e comportamento assintótico, são bem compreendidas e fornecem a base para a análise e validação de soluções obtidas por este método. A eficiência da abordagem está ligada à variância e ao tamanho da amostra, motivando considerações sobre técnicas de redução de variância e estimativas de tamanho de amostra para garantir a qualidade da solução com esforço computacional razoável.

### Referências

[^1]: Página 155.
[^2]: Página 156.
[^3]: Página 157.
[^4]: Página 158.
[^5]: Página 159.
[^6]: Página 160.
[^7]: Página 161.
[^8]: Página 162.
[^9]: Página 163.
[^10]: Página 164.
[^11]: Página 165.
[^19]: Página 173.
[^26]: Página 180.
[^27]: Página 181.
[^28]: Página 182.
[^29]: Página 183.
[^30]: Página 184.
[^39]: Página 193.
[^40]: Página 194.
[^43]: Página 197.
[^44]: Página 198.
[^45]: Página 199.
[^46]: Página 200.
[^47]: Página 201.
... (outras referências conforme necessário, extraídas das páginas restantes)
[^204]: Página 204.

<!-- END -->