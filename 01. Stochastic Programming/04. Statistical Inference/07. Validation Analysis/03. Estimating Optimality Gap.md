## Estimativa do Gap de Otimalidade na Análise de Validação

### Introdução

No âmbito da programação estocástica, após a obtenção de uma solução candidata para o problema de otimização, torna-se crucial avaliar a sua qualidade. A **Análise de Validação** (Validation Analysis) [^48] dedica-se precisamente a esta tarefa. Uma métrica fundamental para essa avaliação é o **gap de otimalidade** (optimality gap), que quantifica a diferença entre o valor da função objetivo na solução candidata e o valor ótimo verdadeiro do problema.

Considere o problema estocástico geral introduzido anteriormente [^1]:
$$ \text{Min}_{x \in X} \{f(x) := \mathbb{E}[F(x, \xi)]\} $$
onde $X$ é um subconjunto fechado não vazio de $\mathbb{R}^n$, $\xi$ é um vetor aleatório com distribuição $P$ suportada em $\Xi \subset \mathbb{R}^d$, e $F: X \times \Xi \to \mathbb{R}$ [^1]. Seja $v^*$ o valor ótimo verdadeiro deste problema [^2]. Dada uma solução candidata viável $\bar{x} \in X$, o gap de otimalidade é definido como [^48]:
$$ gap(\bar{x}) := f(\bar{x}) - v^* $$
Evidentemente, para qualquer $\bar{x}$ viável, $gap(\bar{x})$ é não-negativo, e $gap(\bar{x}) = 0$ se e somente se $\bar{x}$ for uma solução ótima do problema verdadeiro [^49]. A principal dificuldade na avaliação direta do gap reside no fato de que o valor ótimo verdadeiro, $v^*$, é geralmente desconhecido. A metodologia da **Sample Average Approximation (SAA)** [^1], discutida extensivamente nos capítulos anteriores, fornece ferramentas para estimar este gap. A abordagem central consiste em computar um limite inferior estatístico para $v^*$ e um limite superior estatístico para $f(\bar{x})$, permitindo assim estimar um intervalo que contém o verdadeiro gap de otimalidade [^48]. Este capítulo detalha este procedimento de estimação.

### Estimando os Limites Inferior e Superior

A estratégia para estimar o $gap(\bar{x})$ envolve a estimação separada de seus dois componentes: $f(\bar{x})$ e $v^*$.

**Estimando um Limite Inferior para $v^*$**

Como estabelecido na Seção 5.1.2 [^9], o valor ótimo $\hat{v}_N$ do problema SAA (5.2) [^1] é, em geral, um estimador viesado para baixo do valor ótimo verdadeiro $v^*$, ou seja, $v^* \ge \mathbb{E}[\hat{v}_N]$ [^9]. Esta propriedade fundamental sugere que $\hat{v}_N$, ou mais precisamente sua esperança $\mathbb{E}[\hat{v}_N]$, pode fornecer uma base para um limite inferior para $v^*$.

Para estimar $\mathbb{E}[\hat{v}_N]$, podemos empregar uma técnica de promediamento (averaging) [^49]. Geramos $M$ amostras independentes do vetor aleatório $\xi$, cada uma de tamanho $N$. Para cada amostra $m = 1, \dots, M$, resolvemos o problema SAA correspondente (5.2) e obtemos seu valor ótimo $\hat{v}_N^m$. O estimador da média amostral é então calculado como [^49]:
$$ \bar{v}_{N,M} := \frac{1}{M} \sum_{m=1}^M \hat{v}_N^m $$
Este $\bar{v}_{N,M}$ é um estimador não viesado de $\mathbb{E}[\hat{v}_N]$ [^49]. Como as amostras e, consequentemente, os valores ótimos $\hat{v}_N^1, \dots, \hat{v}_N^M$ são independentes e identicamente distribuídos (iid), a variância de $\bar{v}_{N,M}$ é $\text{Var}[\bar{v}_{N,M}] = M^{-1} \text{Var}[\hat{v}_N]$ [^49]. Uma estimativa da variância de $\hat{v}_N$ pode ser obtida pela variância amostral [^49]:
$$ \hat{\sigma}_{N,M}^2 := \frac{1}{M-1} \sum_{m=1}^M (\hat{v}_N^m - \bar{v}_{N,M})^2 $$
É importante notar que esta abordagem faz sentido apenas se o valor ótimo verdadeiro $v^*$ for finito [^49]. Embora a distribuição de $\hat{v}_N$ possa não ser normal, mesmo aproximadamente (conforme Teorema 5.7 [^11] e discussão subsequente [^49]), o **Teorema Central do Limite (CLT)** garante que a distribuição de $\bar{v}_{N,M}$ se aproxima da normalidade à medida que $M$ aumenta [^49]. Portanto, podemos construir um limite inferior de confiança aproximado de $100(1-\alpha)\%$ para a esperança $\mathbb{E}[\hat{v}_N]$ utilizando [^49]:
$$ L_{N,M} := \bar{v}_{N,M} - t_{\alpha, M-1} \sqrt{\frac{\hat{\sigma}_{N,M}^2}{M}} $$
onde $t_{\alpha, M-1}$ é o valor crítico $\alpha$ da distribuição t de Student com $M-1$ graus de liberdade [^49]. Como $v^* \ge \mathbb{E}[\hat{v}_N]$, $L_{N,M}$ fornece um **limite inferior estatístico** válido para o valor ótimo verdadeiro $v^*$ [^49]. Note que as amostras usadas para calcular os $\hat{v}_N^m$ não precisam ser iid internamente (poderia ser usada LH sampling), mas as $M$ replicações devem ser independentes entre si para que a estimativa de variância (5.169) seja válida [^49].

**Estimando um Limite Superior para $f(\bar{x})$**

O termo $f(\bar{x})$ na definição do gap (5.167) também precisa ser estimado, pois representa o valor esperado $\mathbb{E}[F(\bar{x}, \xi)]$. Isso pode ser feito por amostragem [^50]. Geramos uma nova amostra (independente das usadas para calcular $L_{N,M}$) de tamanho $N'$, tipicamente grande, $\xi^1, \dots, \xi^{N'}$. O estimador SAA para $f(\bar{x})$ é [^50]:
$$ f_{N'}(\bar{x}) := \frac{1}{N'} \sum_{j=1}^{N'} F(\bar{x}, \xi^j) $$
Este é um estimador não viesado de $f(\bar{x})$. Sua variância, $\text{Var}[f_{N'}(\bar{x})] = (\sigma_{N'}(\bar{x}))^2 / N'$, pode ser estimada usando a variância amostral $\hat{\sigma}_{N'}^2(\bar{x})$ [^50]:
$$ \hat{\sigma}_{N'}^2(\bar{x}) := \frac{1}{N'-1} \sum_{j=1}^{N'} [F(\bar{x}, \xi^j) - f_{N'}(\bar{x})]^2 $$
Assumindo que $N'$ é suficientemente grande, o CLT justifica a aproximação da distribuição de $f_{N'}(\bar{x})$ por uma normal. Assim, um limite superior de confiança aproximado de $100(1-\alpha)\%$ para $f(\bar{x})$ é dado por [^50]:
$$ U_{N'}(\bar{x}) := f_{N'}(\bar{x}) + z_{\alpha} \frac{\hat{\sigma}_{N'}(\bar{x})}{\sqrt{N'}} $$
onde $z_{\alpha}$ é o valor crítico $\alpha$ da distribuição normal padrão [^50]. O uso de $z_{\alpha}$ em vez de $t_{\alpha, N'-1}$ é justificado porque $N'$ é tipicamente grande [^50].

**Combinando os Limites para Estimar o Gap**

Com os limites $L_{N,M}$ e $U_{N'}(\bar{x})$ em mãos, podemos agora estimar o gap de otimalidade $gap(\bar{x}) = f(\bar{x}) - v^*$. O estimador $f_{N'}(\bar{x}) - \bar{v}_{N,M}$ é um estimador viesado para $gap(\bar{x})$, pois $\mathbb{E}[f_{N'}(\bar{x}) - \bar{v}_{N,M}] = f(\bar{x}) - \mathbb{E}[\hat{v}_N] = gap(\bar{x}) + (v^* - \mathbb{E}[\hat{v}_N])$ [^50]. Como o termo de viés $(v^* - \mathbb{E}[\hat{v}_N])$ é não-negativo [^50], $f_{N'}(\bar{x}) - \bar{v}_{N,M}$ superestima o gap em média.

Podemos construir um limite superior de confiança para o gap. A diferença $U_{N'}(\bar{x}) - L_{N,M}$ fornece um limite superior de confiança conservador de aproximadamente $100(1-\alpha)\%$ para o $gap(\bar{x})$ [^50]. Expandindo a expressão, temos:
$$ U_{N'}(\bar{x}) - L_{N,M} = (f_{N'}(\bar{x}) - \bar{v}_{N,M}) + \left( z_{\alpha} \frac{\hat{\sigma}_{N'}(\bar{x})}{\sqrt{N'}} + t_{\alpha, M-1} \sqrt{\frac{\hat{\sigma}_{N,M}^2}{M}} \right) $$
Este limite é dito "conservador" porque, de fato, ele fornece um limite de confiança de $100(1-\alpha)\%$ para a quantidade $gap(\bar{x}) + (v^* - \mathbb{E}[\hat{v}_N])$, que é maior ou igual ao $gap(\bar{x})$ [^50].

### Discussão sobre Viés e Erros

Ao usar $\bar{v}_{N,M}$ como estimador para $v^*$, incorremos em dois tipos de erro [^50]:
1.  **Viés:** O viés inerente $v^* - \mathbb{E}[\hat{v}_N]$, que é sempre não-negativo.
2.  **Variabilidade Estatística:** A variância de $\bar{v}_{N,M}$, que mede a incerteza devido à amostragem finita ($M$ replicações).

Ambos os erros podem ser reduzidos aumentando o tamanho da amostra $N$ usado em cada problema SAA. O aumento de $N$ reduz o viés $v^* - \mathbb{E}[\hat{v}_N]$ monotonicamente (Proposição 5.6 [^9]) e também tende a reduzir a variância $\text{Var}[\hat{v}_N]$. A variabilidade estatística também pode ser reduzida aumentando o número de replicações $M$ [^50]. A alocação ótima de recursos computacionais entre aumentar $N$ ou $M$ depende da complexidade computacional do problema SAA em relação a $N$ [^50]. Se o custo cresce rapidamente com $N$, pode ser vantajoso usar um $M$ maior. Se algoritmos eficientes (como métodos de subgradiente) são usados, onde o esforço cresce linearmente com $N$, usar um $N$ maior e poucas replicações $M$ pode ser preferível [^50].

A magnitude do viés $v^* - \mathbb{E}[\hat{v}_N]$ depende da estrutura do problema. Se o conjunto de soluções ótimas $S$ do problema verdadeiro não for um singleton, o viés tipicamente converge para zero a uma taxa de $O(N^{-1/2})$ e tende a ser maior para conjuntos $S$ maiores (Equação 5.29 [^11] e discussão subsequente [^50]). Por outro lado, para problemas bem-condicionados onde $S$ é um singleton, o viés é tipicamente de ordem $O(N^{-1})$ (Teorema 5.8 [^13]), sendo geralmente menor [^50]. Em problemas com solução ótima "sharp" $x^*$, o viés pode ser ainda menor, pois o evento $\hat{v}_N = f_N(x^*)$ ocorre com probabilidade que tende a 1 exponencialmente rápido (Teorema 5.23 [^38]) [^51].

### Refinamentos e Alternativas

**Estimadores Baseados em Common Random Numbers (CRN)**

Uma alternativa à computação independente dos limites superior e inferior é usar as mesmas $M$ amostras de tamanho $N$ para calcular tanto $f_N^m(\bar{x})$ quanto $\hat{v}_N^m$ para cada $m=1, \dots, M$. O gap pode então ser estimado por [^51]:
$$ \widehat{gap}_{N,M}(\bar{x}) := \frac{1}{M} \sum_{m=1}^M [f_N^m(\bar{x}) - \hat{v}_N^m] $$
O valor esperado deste estimador ainda é $f(\bar{x}) - \mathbb{E}[\hat{v}_N]$. No entanto, se a solução candidata $\bar{x}$ estiver próxima da solução ótima $x^*$, espera-se que $f_N^m(\bar{x})$ e $\hat{v}_N^m$ sejam altamente correlacionados positivamente. Neste caso, a variância de $\widehat{gap}_{N,M}(\bar{x})$ pode ser consideravelmente menor do que a variância de $f_{N'}(\bar{x}) - \bar{v}_{N,M}$ obtida com amostras independentes [^51]. Esta é a ideia por trás dos estimadores baseados em *common random numbers*.

**Uso de Limites Inferiores Determinísticos para $\hat{v}_N$**

Conforme observado na Remark 16 [^51], em vez de usar o valor ótimo $\hat{v}_N$ do problema SAA, podemos usar qualquer limite inferior determinístico para $\hat{v}_N$ para construir um limite inferior estatístico para $v^*$. Para problemas convexos, um limite inferior pode ser obtido a partir de subgradientes. Por exemplo, escolhendo pontos $x_1, \dots, x_r \in X$ e calculando subgradientes $\hat{g}_{iN} \in \partial f_N(x_i)$, o valor ótimo $\hat{\lambda}_N$ do problema [^51]:
$$ \text{Min}_{x \in X} \max_{1 \le i \le r} \{ f_N(x_i) + \hat{g}_{iN}^T (x - x_i) \} $$
satisfaz $\hat{\lambda}_N \le \hat{v}_N$ [^51]. Se este problema (5.177) for significativamente mais fácil de resolver do que o SAA original (por exemplo, se for um problema de programação linear quando $X$ é poliédrico), calcular a média de $\hat{\lambda}_N$ ao longo de $M$ replicações pode ser computacionalmente vantajoso em relação a calcular a média de $\hat{v}_N$ [^51]. A escolha dos pontos $x_1, \dots, x_r$ é crucial para a qualidade deste limite [^52].

### Conclusão

A estimativa do gap de otimalidade é um passo essencial na validação de soluções candidatas em programação estocástica. A metodologia SAA oferece uma abordagem prática para essa tarefa, permitindo a construção de limites de confiança para o gap. Isso é alcançado através da combinação de um limite inferior estatístico para o valor ótimo verdadeiro $v^*$, obtido pela média de valores ótimos de múltiplos problemas SAA ($\bar{v}_{N,M}$ e $L_{N,M}$), e um limite superior estatístico para o valor da função objetivo na solução candidata $f(\bar{x})$, obtido por amostragem direta ($f_{N'}(\bar{x})$ e $U_{N'}(\bar{x})$). A compreensão do viés inerente ao estimador SAA $\hat{v}_N$ e da variabilidade estatística dos estimadores é fundamental para a interpretação correta dos resultados e para a alocação eficiente de recursos computacionais entre o tamanho da amostra $N$ e o número de replicações $M$. Refinamentos como o uso de *common random numbers* ou limites baseados em subgradientes podem oferecer melhorias em termos de variância ou custo computacional em cenários específicos.

### Referências

[^1]: Página 155, Seção 5.1, Equações (5.1), (5.2).
[^2]: Página 156, Texto após Equação (5.3).
[^9]: Página 163, Equação (5.22) e discussão subsequente, Proposição 5.6.
[^11]: Página 165, Teorema 5.7, Equações (5.25), (5.26), (5.29).
[^13]: Página 167, Teorema 5.8.
[^38]: Página 192, Teorema 5.23.
[^48]: Página 202, Seção 5.6, Seção 5.6.1, Equação (5.167).
[^49]: Página 203, Equações (5.168), (5.169), (5.170) e texto circundante.
[^50]: Página 203-204, Equações (5.171), (5.172), (5.173), (5.174) e texto circundante.
[^51]: Página 205, Remark 16, Equação (5.175), (5.176), (5.177).
[^52]: Página 206, Texto após (5.177).
<!-- END -->