## Capítulo 5.6: Análise de Validação para Soluções Candidatas em Otimização Estocástica

### Introdução

Após a obtenção de uma solução candidata, denotada por $\\bar{x} \\in X$, para um problema de otimização estocástica, frequentemente através da resolução de um problema de Aproximação por Média Amostral (SAA) como discutido nas seções anteriores, torna-se crucial avaliar a qualidade dessa solução. A **análise de validação** é o processo dedicado a essa avaliação [^48]. Esta análise é particularmente importante para determinar a adequação da solução $\\bar{x}$ para o problema "verdadeiro" (5.1) e para guiar decisões sobre, por exemplo, o tamanho da amostra $N$ a ser utilizado em métodos de simulação ou critérios de parada para algoritmos de otimização baseados em simulação [^48].

Existem fundamentalmente duas abordagens principais para a análise de validação [^48]:
1.  Estimar o **gap de otimalidade**, definido como $f(\\bar{x}) - v^*$, onde $f(\\bar{x}) = E[F(\\bar{x}, \\xi)]$ é o valor esperado da função objetivo na solução candidata e $v^*$ é o valor ótimo verdadeiro do problema (5.1).
2.  Avaliar as **condições de otimalidade de primeira ordem (KKT)** no ponto $\\bar{x}$.

Neste capítulo, exploraremos ambas as abordagens em detalhe, baseando-nos nas propriedades estatísticas dos estimadores SAA introduzidas anteriormente. Assumiremos ao longo desta seção que o valor $f(\\bar{x})$ é finito, o que, no contexto de programação estocástica de dois estágios, implica que o problema de segundo estágio associado a $\\bar{x}$ é factível para quase toda realização dos dados aleatórios [^48].

### Estimação do Gap de Otimalidade

O gap de otimalidade, $gap(\\bar{x}) := f(\\bar{x}) - v^*$, fornece uma medida direta de quão subótima a solução candidata $\\bar{x}$ é [^48]. Por definição, para qualquer $\\bar{x}$ factível, $gap(\\bar{x}) \\ge 0$, e $gap(\\bar{x}) = 0$ se e somente se $\\bar{x}$ for uma solução ótima do problema verdadeiro [^48]. A estimação direta do gap é desafiadora pois $v^*$ não é conhecido. A estratégia consiste em obter limites estatísticos para $f(\\bar{x})$ e $v^*$.

#### Limites Estatísticos Inferiores para $v^*$

Como discutido anteriormente (ver discussão após (5.22) [^9]), o valor ótimo $\\hat{v}_N$ do problema SAA (5.2) satisfaz $v^* \\ge E[\\hat{v}_N]$. Isso significa que $\\hat{v}_N$ fornece um **limite inferior estatisticamente válido** para $v^*$ [^49]. Podemos estimar a esperança $E[\\hat{v}_N]$ através da média de múltiplas replicações independentes do problema SAA. Especificamente, resolvemos o problema SAA $M$ vezes, cada vez com uma amostra independente de tamanho $N$, obtendo os valores ótimos $\\hat{v}_N^{(1)}, \\dots, \\hat{v}_N^{(M)}$ [^49]. O estimador para $E[\\hat{v}_N]$ é então a média amostral:
$$ \\bar{v}_{N,M} := \\frac{1}{M} \\sum_{m=1}^M \\hat{v}_N^{(m)} $$
Este é um estimador não viesado de $E[\\hat{v}_N]$ [^49]. Como as replicações são independentes e identicamente distribuídas, a variância de $\\bar{v}_{N,M}$ é $M^{-1} \\text{Var}[\\hat{v}_N]$, que pode ser estimada por [^49]:
$$ \\hat{\\sigma}^2_{N,M} := \\frac{1}{M(M-1)} \\sum_{m=1}^M (\\hat{v}_N^{(m)} - \\bar{v}_{N,M})^2 $$
Embora $\\hat{v}_N$ geralmente não siga uma distribuição normal (ver Teorema 5.7 [^11]), o Teorema Central do Limite (CLT) garante que a distribuição de $\\bar{v}_{N,M}$ se aproxima da normalidade à medida que $M$ aumenta [^49]. Portanto, podemos construir um limite inferior de confiança aproximado de $100(1-\\alpha)\\%$ para $E[\\hat{v}_N]$ como [^49]:
$$ L_{N,M} := \\bar{v}_{N,M} - t_{\\alpha, M-1} \\hat{\\sigma}_{N,M} $$
onde $t_{\\alpha, M-1}$ é o valor crítico $\\alpha$ da distribuição t de Student com $M-1$ graus de liberdade [^49]. Para $M$ grande, $t_{\\alpha, M-1}$ aproxima-se do valor crítico $z_\\alpha$ da distribuição normal padrão. Este $L_{N,M}$ serve como um limite inferior de confiança para $E[\\hat{v}_N]$, que por sua vez é um limite inferior para $v^*$.

#### Limites Estatísticos Superiores para $f(\\bar{x})$

Podemos estimar $f(\\bar{x}) = E[F(\\bar{x}, \\xi)]$ usando uma média amostral baseada numa amostra independente $\\xi^1, \\dots, \\xi^{N\'}$ de tamanho $N\'$ [^49]:
$$ \\hat{f}_{N\'}(\\bar{x}) := \\frac{1}{N\'} \\sum_{j=1}^{N\'} F(\\bar{x}, \\xi^j) $$
Assumindo que $\\text{Var}[F(\\bar{x}, \\xi)]$ é finita, pelo CLT, $\\hat{f}_{N\'}(\\bar{x})$ é aproximadamente normal para $N\'$ grande. Estimando a variância $\\sigma^2(\\bar{x}) = \\text{Var}[F(\\bar{bar{x}}, \\xi)]$ pela variância amostral $\\hat{\\sigma}^2_{N\'}(\\bar{x})$ (definida analogamente a (5.21) [^9]), podemos construir um limite superior de confiança aproximado de $100(1-\\alpha)\\%$ para $f(\\bar{x})$ [^49]:
$$ U_{N\'}(\\bar{x}) := \\hat{f}_{N\'}(\\bar{x}) + z_{\\alpha} \\hat{\\sigma}_{N\'}(\\bar{x}) $$
Como $N\'$ pode ser escolhido grande (calcular $F(\\bar{x}, \\xi)$ é geralmente mais fácil do que resolver um problema SAA [^50]), a aproximação normal e o uso de $z_\\alpha$ são geralmente justificados [^49, ^50].

#### Limite Superior de Confiança Conservador para o Gap

Combinando os limites, obtemos um limite superior de confiança para o gap. Temos que [^50]:
$$ E[\\hat{f}_{N\'}(\\bar{x}) - \\bar{v}_{N,M}] = f(\\bar{x}) - E[\\hat{v}_N] = gap(\\bar{x}) + (v^* - E[\\hat{v}_N]) $$
Como $v^* - E[\\hat{v}_N] \\ge 0$, a quantidade $\\hat{f}_{N\'}(\\bar{x}) - \\bar{v}_{N,M}$ é um estimador viesado (para cima) do $gap(\\bar{x})$ [^50]. A variância deste estimador é a soma das variâncias de $\\hat{f}_{N\'}(\\bar{x})$ e $\\bar{v}_{N,M}$, assumindo que as amostras usadas para calculá-los são independentes [^50]. Um limite superior de confiança conservador de $100(1-\\alpha)\\%$ para $gap(\\bar{x})$ é dado por [^50]:

> $$ \\text{Gap}_{\\text{upper}} := \\hat{f}_{N\'}(\\bar{x}) - L_{N,M} = (\\hat{f}_{N\'}(\\bar{x}) - \\bar{v}_{N,M}) + t_{\\alpha, M-1} \\hat{\\sigma}_{N,M} $$
> ou, usando a aproximação normal e somando as variâncias estimadas:
> $$ \\text{Gap}_{\\text{upper}} \\approx (\\hat{f}_{N\'}(\\bar{x}) - \\bar{v}_{N,M}) + z_{\\alpha} \\sqrt{\\hat{\\sigma}^2_{N\'}(\\bar{x}) + \\hat{\\sigma}^2_{N,M}} $$
> Este limite é conservador porque na verdade majora $gap(\\bar{x}) + (v^* - E[\\hat{v}_N])$ com a confiança desejada [^50].

O viés $v^* - E[\\hat{v}_N]$ decresce monotonicamente com $N$ (Proposição 5.6 [^9]). A sua magnitude assintótica depende das propriedades do conjunto de soluções ótimas $S$. Se $S$ não for um singleton, o viés é tipicamente da ordem $O(N^{-1/2})$ [^50, ^11]. Se $S$ for um singleton e condições de regularidade adicionais forem satisfeitas (problemas bem condicionados), o viés é de ordem inferior, tipicamente $O(N^{-1})$ (Teorema 5.8 [^14]) [^50]. Reduzir este viés requer aumentar $N$.

#### Abordagens Alternativas para Estimar o Gap

1.  **Common Random Numbers (CRN):** Em vez de gerar amostras independentes para $\\hat{f}_{N\'}(\\bar{x})$ e $\\hat{v}_N^{(m)}$, podemos usar a *mesma* amostra $\\xi^{1,m}, \\dots, \\xi^{N,m}$ para calcular ambos $\\hat{f}_N^{(m)}(\\bar{x})$ e $\\hat{v}_N^{(m)}$ para cada replicação $m=1, \\dots, M$. O estimador do gap torna-se [^51]:
    $$ \\widehat{gap}_{N,M}(\\bar{x}) := \\frac{1}{M} \\sum_{m=1}^M [\\hat{f}_N^{(m)}(\\bar{x}) - \\hat{v}_N^{(m)}] $$
    A esperança deste estimador ainda é $f(\\bar{x}) - E[\\hat{v}_N]$. No entanto, se $\\bar{x}$ for próximo de uma solução ótima $x^*$, espera-se que $\\hat{f}_N^{(m)}(\\bar{x})$ e $\\hat{v}_N^{(m)}$ sejam positivamente correlacionados. Neste caso, a variância de $\\hat{f}_N^{(m)}(\\bar{x}) - \\hat{v}_N^{(m)}$ pode ser consideravelmente menor do que a soma das variâncias quando amostras independentes são usadas, levando a estimativas mais precisas do gap (ou limites de confiança mais apertados) [^51]. Esta é a ideia dos *common random number generated estimators* [^51].

2.  **Limite Inferior Baseado em Subgradientes (Cutting Plane):** Em vez de usar $\\hat{v}_N$ diretamente, podemos construir um limite inferior determinístico para $\\hat{v}_N$ (e, portanto, um limite inferior estatístico para $v^*$) usando subgradientes (Remark 16 [^51]). Se o problema for convexo, para qualquer $x\' \\in X$ e qualquer subgradiente $y \\in \\partial f_N(x\')$, temos $f_N(x) \\ge f_N(x\') + y^T(x-x\')$ para todo $x \\in \\mathbb{R}^n$ [^51]. Escolhendo um conjunto de pontos $x_1, \\dots, x_r \\in X$ e calculando subgradientes $\\hat{g}_{iN} \\in \\partial f_N(x_i)$, podemos obter um limite inferior para $\\hat{v}_N$ resolvendo [^51]:
    $$ \\hat{\\lambda}_N := \\min_{x \\in X} \\max_{1 \\le i \\le r} \\{ f_N(x_i) + \\hat{g}_{iN}^T(x-x_i) \\} $$
    Temos $\\hat{\\lambda}_N \\le \\hat{v}_N$ [^51]. Se $X$ for poliédrico e $f_N$ for linear por partes (como em problemas de dois estágios lineares), este problema pode ser formulado como um problema de programação linear, potencialmente mais fácil de resolver do que o problema SAA original [^51]. A qualidade do limite $\\hat{\\lambda}_N$ depende da escolha dos pontos $x_i$. Uma abordagem natural é usar soluções (quase) ótimas de uma execução SAA inicial para gerar estes cortes [^51, ^52]. A média de $\\hat{\\lambda}_N$ sobre múltiplas replicações fornece um estimador para $E[\\hat{\\lambda}_N]$, que é um limite inferior para $E[\\hat{v}_N]$ e, consequentemente, para $v^*$.

### Teste Estatístico das Condições de Otimalidade (KKT)

Uma abordagem alternativa à validação é verificar se a solução candidata $\\bar{x}$ satisfaz as condições necessárias de otimalidade de primeira ordem (KKT) para o problema verdadeiro [^48]. Esta abordagem é particularmente relevante quando o problema tem restrições explícitas.

Consideremos o problema na forma (5.185) [^53]:
$$ \\min_{x \\in \\mathbb{R}^n} f(x) \\quad \\text{s.t.} \\quad g_i(x) = 0, i=1,\\dots,q, \\quad g_i(x) \\le 0, i=q+1,\\dots,p $$
onde $f(x) = E[F(x, \\xi)]$ e as funções de restrição $g_i(x)$ são assumidas suaves (pelo menos continuamente diferenciáveis) e determinísticas [^53]. Seja $x^*$ uma solução ótima. Sob uma qualificação de restrição (como LICQ - Linear Independence Constraint Qualification), existem multiplicadores de Lagrange $\\lambda_i^*$ tais que as condições KKT são satisfeitas em $x^*$ [^53]:
1.  Estacionaridade: $\\nabla f(x^*) + \\sum_{i=1}^p \\lambda_i^* \\nabla g_i(x^*) = 0$
2.  Viabilidade Primal: $g_i(x^*) = 0$ for $i=1..q$, $g_i(x^*) \\le 0$ for $i=q+1..p$
3.  Viabilidade Dual: $\\lambda_i^* \\ge 0$ for $i=q+1..p$
4.  Complementaridade Folgada: $\\lambda_i^* g_i(x^*) = 0$ for $i=q+1..p$

Estas condições podem ser escritas de forma compacta como $\\nabla f(x^*) \\in K(x^*)$, onde $K(x^*)$ é um cone relacionado ao cone normal ao conjunto factível em $x^*$ [^53, ^54]. Por exemplo, $K(x^*) = \\{ -\\sum_{i \\in \\mathcal{J}(x^*)} \\lambda_i \\nabla g_i(x^*) : \\lambda_i \\ge 0 \\text{ for } i \\in I(x^*) \\}$ [^53], onde $\\mathcal{J}(x^*) = \\{1,\\dots,q\\} \\cup I(x^*)$ é o conjunto de índices das restrições ativas em $x^*$.

A ideia é testar estatisticamente se $\\nabla f(\\bar{x}) \\in K(\\bar{x})$ para a solução candidata $\\bar{x}$. Para isso, precisamos de um estimador para $\\nabla f(\\bar{x})$. Se $F(\\cdot, \\xi)$ for diferenciável em $\\bar{x}$ (q.c.) e a troca de gradiente e esperança for válida (cf. (5.189) [^54]), podemos usar o gradiente da média amostral como estimador [^54]:
$$ y_N(\\bar{x}) := \\nabla f_N(\\bar{x}) = \\frac{1}{N} \\sum_{j=1}^N \\nabla_x F(\\bar{x}, \\xi^j) $$
Este estimador $y_N(\\bar{x})$ é não viesado para $\\nabla f(\\bar{x})$ [^54]. Assumindo que $\\nabla_x F(\\bar{x}, \\xi)$ tem momentos de segunda ordem finitos e a amostra é iid, o CLT multivariado implica que [^54]:
$$ \\sqrt{N} (y_N(\\bar{x}) - \\nabla f(\\bar{x})) \\xrightarrow{D} \\mathcal{N}(0, \\Sigma) $$
onde $\\Sigma = \\text{Var}[\\nabla_x F(\\bar{x}, \\xi)]$ é a matriz de covariância [^54]. Podemos estimar $\\Sigma$ consistentemente pela matriz de covariância amostral $\\hat{\\Sigma}_N$, definida analogamente a (5.190) [^54].

Com base nisto, podemos testar a hipótese nula [^55]:
$H_0: \\nabla f(\\bar{x}) \\in K(\\bar{x})$
contra a alternativa
$H_1: \\nabla f(\\bar{x}) \\notin K(\\bar{x})$

O teste é baseado na distância (quadrática ponderada) entre o gradiente estimado $y_N(\\bar{x})$ e o cone $K(\\bar{x})$. A estatística de teste é [^55]:

> $$ T_N := N \\inf_{z \\in K(\\bar{x})} (y_N(\\bar{x}) - z)^T \\hat{\\Sigma}_N^{-1} (y_N(\\bar{x}) - z) = N \\min_{z \\in K(\\bar{x})} \\| y_N(\\bar{x}) - z \\|^2_{\\hat{\\Sigma}_N^{-1}} $$

O cálculo de $T_N$ envolve a minimização de uma forma quadrática sobre um cone poliédrico (assumindo que $K(\\bar{x})$ o seja), o que pode ser formulado como um problema de programação quadrática [^55].

A distribuição assintótica de $T_N$ sob $H_0$ depende da estrutura do cone $K(\\bar{x})$. Se $K(\\bar{x})$ for um subespaço linear (e.g., apenas restrições de igualdade ativas), $T_N$ segue assintoticamente uma distribuição qui-quadrado central com $n - \\dim(K(\\bar{x}))$ graus de liberdade [^55]. No caso geral com desigualdades, se LICQ e a condição de complementaridade estrita (i.e., $\\lambda_i^* > 0$ para todas as restrições de desigualdade ativas $i \\in I(\\bar{x})$) valerem em $\\bar{x}$ (assumindo que $\\bar{x}$ satisfaz $H_0$), então o cone $K(\\bar{x})$ comporta-se localmente como o subespaço gerado pelos gradientes das restrições em $\\mathcal{J}(\\bar{x})$, e $T_N$ segue assintoticamente uma distribuição $\\chi^2(\\nu)$ com $\\nu = n - |\\mathcal{J}(\\bar{x})|$ graus de liberdade [^55].

O procedimento de teste consiste em calcular $T_N$ e obter o p-valor correspondente $P(Y \\ge T_N)$ onde $Y \\sim \\chi^2(\\nu)$. Se o p-valor for menor que o nível de significância $\\alpha$ pré-definido, rejeitamos $H_0$, concluindo que há evidência estatística contra a otimalidade de $\\bar{x}$. Se o p-valor for maior que $\\alpha$, não rejeitamos $H_0$ [^55].

É crucial interpretar corretamente o resultado [^55]:
*   **Rejeitar $H_0$**: Sugere que $\\nabla f(\\bar{x})$ está estatisticamente longe do cone $K(\\bar{x})$, indicando que $\\bar{x}$ provavelmente não é ótimo.
*   **Não rejeitar $H_0$**: Não prova que $\\bar{x}$ é ótimo. Apenas indica que, dada a precisão estatística do estimador $y_N(\\bar{x})$ (refletida em $\\hat{\\Sigma}_N$), não podemos distinguir $\\nabla f(\\bar{x})$ de um vetor dentro de $K(\\bar{x})$. Se a variância $\\Sigma/N$ for grande (baixa precisão), o teste terá pouco poder para detectar desvios da otimalidade. A análise da região de confiança para $\\nabla f(\\bar{x})$ (cf. (5.192) [^54]) pode fornecer contexto adicional sobre a precisão da estimativa [^55].

### Conclusão

A análise de validação fornece ferramentas essenciais para avaliar a qualidade das soluções obtidas para problemas de otimização estocástica. A estimação do gap de otimalidade oferece uma medida quantitativa da subotimalidade, mas depende da capacidade de obter um bom limite inferior estatístico para o valor ótimo verdadeiro, sendo o viés do estimador SAA $\\hat{v}_N$ uma consideração importante. O teste das condições KKT verifica a satisfação das condições necessárias de primeira ordem, mas a sua interpretação depende da precisão estatística da estimativa do gradiente. Ambas as abordagens complementam-se e são valiosas na prática, auxiliando na seleção de parâmetros como o tamanho da amostra $N$ e no desenvolvimento de critérios de parada robustos para algoritmos de otimização baseados em simulação.

### Referências

[^1]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 155.
[^2]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 156.
[^3]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 157.
[^4]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 158.
[^5]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 159.
[^6]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 160.
[^7]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 161.
[^8]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 162.
[^9]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 163.
[^10]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 164.
[^11]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 165.
[^12]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 166.
[^13]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 167.
[^14]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 168.
[^15]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 169.
[^16]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 170.
[^17]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 171.
[^18]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 172.
[^19]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 173.
[^20]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 174.
[^21]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 175.
[^22]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 176.
[^23]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 177.
[^24]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 178.
[^25]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 179.
[^26]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 180.
[^27]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 181.
[^28]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 182.
[^29]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 183.
[^30]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 184.
[^31]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 185.
[^32]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 186.
[^33]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 187.
[^34]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 188.
[^35]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 189.
[^36]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 190.
[^37]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 191.
[^38]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 192.
[^39]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 193.
[^40]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 194.
[^41]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 195.
[^42]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 196.
[^43]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 197.
[^44]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 198.
[^45]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 199.
[^46]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 200.
[^47]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 201.
[^48]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 202.
[^49]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 203.
[^50]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 204.
[^51]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 205.
[^52]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 206.
[^53]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 207.
[^54]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 208.
[^55]: Shapiro, A. (2009/8/20). Statistical Inference. *Chapter 5*, p. 209.
<!-- END -->