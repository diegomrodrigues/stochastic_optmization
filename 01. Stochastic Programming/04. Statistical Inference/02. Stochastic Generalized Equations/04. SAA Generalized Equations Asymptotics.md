## Propriedades Assintóticas de Estimadores SAA para Equações Generalizadas Estocásticas

### Introdução

Continuando a análise das **Equações Generalizadas Estocásticas**, introduzidas na Seção 5.2 como $\\phi(x) \\in \\Gamma(x)$ [^20], e suas correspondentes aproximações via **Sample Average Approximation (SAA)**, dadas por $\\hat{\\phi}_N(x) \\in \\Gamma(x)$ [^21], este capítulo foca nas propriedades assintóticas dos estimadores $\\hat{x}_N$ que são soluções da equação SAA. Como vimos anteriormente na Seção 5.2.1, sob certas condições de regularidade e para um conjunto compacto $C$ contendo o conjunto de soluções $S$, a consistência dos estimadores SAA é estabelecida, garantindo que $D(\\hat{S}_N, S) \\to 0$ com probabilidade 1 (w.p. 1) quando $N \\to \\infty$ (Teorema 5.12) [^21]. A presente seção aprofunda essa análise, investigando a distribuição limite de $\\hat{x}_N$ à medida que o tamanho da amostra $N$ cresce indefinidamente. O objetivo é derivar a distribuição assintótica desses estimadores, conectando-a às propriedades da aplicação $\\phi(x)$ e do vetor aleatório $\\Phi(x, \\xi)$.

### Pré-requisitos: Consistência e Regularidade Forte

A análise assintótica pressupõe a convergência do estimador SAA $\\hat{x}_N$ para a verdadeira solução $x$. O Teorema 5.12 [^21] estabelece condições suficientes para essa convergência (consistência), notadamente a continuidade de $\\phi(x)$, a natureza fechada da multifunção $\\Gamma(x)$ e a convergência uniforme de $\\hat{\\phi}_N(x)$ para $\\phi(x)$ em $C$ w.p. 1.

Um conceito fundamental para a derivação das propriedades assintóticas é a **regularidade forte** da solução $x$ da equação generalizada verdadeira (5.60) [^20].

> **Definição 5.13.** *Suponha que a aplicação $\\phi(x)$ seja continuamente diferenciável. Dizemos que uma solução $x \\in S$ é **fortemente regular** se existirem vizinhanças $N_1$ e $N_2$ de $0 \\in \\mathbb{R}^n$ e $x$, respectivamente, tais que para todo $\\delta \\in N_1$ a equação generalizada (linearizada)*
> $$ \\delta + \\phi(\\bar{x}) + \\nabla\\phi(\\bar{x})(x - \\bar{x}) \\in \\Gamma(x) \\quad (5.68) $$
> *possui uma solução única em $N_2$, denotada por $x = x(\\delta)$, e $x(\\cdot)$ é Lipschitz contínua em $N_1$* [^22].

A regularidade forte implica que a solução da equação linearizada (5.68) [^22] é localmente única e se comporta de maneira Lipschitz contínua em relação à perturbação $\\delta$. No caso particular em que $\\Gamma(x) = \\{0\\}$, a regularidade forte equivale à não singularidade da matriz Jacobiana $J := \\nabla\\phi(\\bar{x})$ [^22].

O Teorema 5.14 [^22] estabelece uma conexão crucial entre a regularidade forte e a convergência dos estimadores SAA. Se $\\bar{x}$ é uma solução fortemente regular e se $\\hat{\\phi}_N$ e $\\nabla\\hat{\\phi}_N$ convergem uniformemente para $\\phi$ e $\\nabla\\phi$ numa vizinhança $V$ de $\\bar{x}$ (i.e., $||\\hat{\\phi}_N - \\phi||_{1,V} \\to 0$ w.p. 1), então, para $N$ suficientemente grande, a equação generalizada SAA (5.67) [^21] possui uma solução única $\\hat{x}_N$ numa vizinhança de $\\bar{x}$, e $\\hat{x}_N \\to \\bar{x}$ w.p. 1 [^22]. A convergência uniforme requerida, no caso de amostragem iid, é garantida pela condição (E3), que exige que $\\Phi(\\cdot, \\xi)$ seja continuamente diferenciável em $V$ e que $||\\Phi(x, \\xi)||_{x \\in V}$ e $||\\nabla_x \\Phi(x, \\xi)||_{x \\in V}$ sejam dominados por uma função integrável [^23]. O Teorema 5.15 [^23] combina essas condições para garantir a existência, unicidade local e convergência da solução SAA $\\hat{x}_N$ para uma solução única $\\bar{x}$ em $C$, sob regularidade forte e amostragem iid.

### Análise Assintótica dos Estimadores SAA

A derivação da distribuição assintótica de $\\hat{x}_N$ baseia-se na aproximação de primeira ordem dada por (5.69) [^22], que relaciona a solução da equação SAA $\\hat{x}_N = x(\\hat{\\phi}_N)$ com a solução da equação linearizada $x(\\delta)$ em torno de $\\bar{x}$, onde a perturbação é $\\delta = \\hat{\\phi}_N(\\bar{x}) - \\phi(\\bar{x})$. Sob condições de regularidade que garantem que o termo remanescente em (5.69) é de ordem $o_p(N^{-1/2})$, a análise assintótica torna-se tratável.

#### Caso 1: Equações ( $\\Gamma(x) = \\{0\\}$ )

Consideremos primeiro o caso mais simples onde a equação generalizada se reduz a uma equação padrão $\\phi(x) = 0$ [^20]. Neste cenário, a **regularidade forte** da solução $\\bar{x}$ equivale à não singularidade da matriz Jacobiana $J := \\nabla\\phi(\\bar{x})$ [^22]. A solução da equação linearizada (5.68) [^22], que neste caso é $\\delta + \\nabla\\phi(\\bar{x})(x - \\bar{x}) = 0$, é dada explicitamente por $x(\\delta) = \\bar{x} - J^{-1}\\delta$ [^23].

Substituindo $\\delta = \\hat{\\phi}_N(\\bar{x}) - \\phi(\\bar{x})$ (lembrando que $\\phi(\\bar{x})=0$) na aproximação de primeira ordem (5.69) [^22] e assumindo que o resto é $o_p(N^{-1/2})$, obtemos a relação assintótica (5.71) [^23]:
$$ N^{1/2}(\\hat{x}_N - \\bar{x}) = -J^{-1} Y_N + o_p(1), $$
onde $Y_N := N^{1/2} [\\hat{\\phi}_N(\\bar{x}) - \\phi(\\bar{x})] = N^{1/2} \\hat{\\phi}_N(\\bar{x})$ [^23].

Pelo Teorema Central do Limite (CLT), assumindo amostragem iid e momentos finitos, $Y_N$ converge em distribuição para um vetor aleatório normal multivariado, $Y_N \\xrightarrow{D} Y \\sim N(0, \\Sigma)$, onde $\\Sigma$ é a matriz de covariância do vetor aleatório $\\Phi(\\bar{x}, \\xi)$ [^23]. Combinando a relação assintótica com a convergência em distribuição de $Y_N$, concluímos que $\\hat{x}_N$ possui **distribuição assintoticamente normal** com média $\\bar{x}$ e matriz de covariância $N^{-1} J^{-1} \\Sigma J^{-T}$ [^23]:
$$ N^{1/2}(\\hat{x}_N - \\bar{x}) \\xrightarrow{D} N(0, J^{-1} \\Sigma J^{-T}). $$

#### Caso 2: Desigualdades Variacionais ( $\\Gamma(x) = N_X(x)$ )

Estendemos agora a análise para o caso de desigualdades variacionais estocásticas, onde $\\Gamma(\\cdot) = N_X(\\cdot)$ e $X$ é um conjunto não vazio, fechado, convexo e poliédrico [^24]. Seja $\\bar{x}$ uma solução fortemente regular da desigualdade variacional $\\phi(x) \\in N_X(x)$ [^20, ^21].

A análise assintótica neste caso envolve o **cone crítico** $C_X(\\bar{x})$ em $\\bar{x}$, definido como [^24]:
$$ C_X(\\bar{x}) := \\{y \\in T_X(\\bar{x}) : y^T \\phi(\\bar{x}) = 0\\} \\quad (5.72) $$
onde $T_X(\\bar{x})$ é o cone tangente a $X$ em $\\bar{x}$.

A solução $x(\\delta)$ da desigualdade variacional linearizada (5.68) [^22] está relacionada à solução $d(\\delta)$ da seguinte desigualdade variacional definida no cone crítico [^24]:
$$ \\delta + Jd \\in N_{C_X(\\bar{x})}(d) \\quad (5.73) $$
onde $J = \\nabla\\phi(\\bar{x})$ e $N_{C_X(\\bar{x})}(d)$ é o cone normal ao cone crítico $C_X(\\bar{x})$ em $d$. Especificamente, $x(\\delta) - \\bar{x}$ coincide com $d(\\delta)$ para $\\delta$ suficientemente próximo de 0 [^24]. A aplicação $d(\\cdot)$ representa a derivada direcional de Hadamard da aplicação solução $x(u)$ em $u = \\phi$ [^24].

Aplicando o Teorema Delta (funcional), sob condições de regularidade apropriadas que garantem o CLT funcional $N^{1/2}(\\hat{\\phi}_N - \\phi) \\xrightarrow{D} Y$ (onde $Y$ é um processo Gaussiano, mas aqui focamos no vetor aleatório $Y \\sim N(0, \\Sigma)$ correspondente a $Y(\\bar{x})$), obtemos a distribuição assintótica [^24]:
$$ N^{1/2}(\\hat{x}_N - \\bar{x}) \\xrightarrow{D} d(Y) \\quad (5.74) $$
onde $Y \\sim N(0, \\Sigma)$ e $\\Sigma$ é a matriz de covariância de $\\Phi(\\bar{x}, \\xi)$.

A distribuição limite $d(Y)$ é normal multivariada se, e somente se, a aplicação $d(\\cdot)$ for linear. Isso ocorre precisamente quando o cone crítico $C_X(\\bar{x})$ é um subespaço linear [^24]. Neste caso, a desigualdade variacional (5.73) [^24] pode ser reescrita como uma equação linear projetada [^24]:
$$ P\\delta + PJd = 0 \\quad (5.75) $$
onde $P$ é a matriz de projeção ortogonal sobre o subespaço $C_X(\\bar{x})$. A condição de regularidade forte equivale, então, à invertibilidade (não singularidade) da aplicação $PJ$ restrita ao subespaço $C_X(\\bar{x})$ [^24].

#### Aplicação às Condições KKT

A estrutura de equação generalizada é particularmente útil para analisar as condições de otimalidade de primeira ordem (Karush-Kuhn-Tucker, KKT) de problemas de otimização com restrições. Considerando o problema (5.1) [^1] com o conjunto viável $X$ definido por restrições da forma (5.62) [^20], as condições KKT podem ser escritas como uma desigualdade variacional $\\phi(z) \\in N_K(z)$ [^20, ^24]. Aqui, $z = (x, \\lambda) \\in \\mathbb{R}^{n+p}$ inclui as variáveis primais $x$ e os multiplicadores de Lagrange $\\lambda$. A aplicação $\\phi(z)$ é definida a partir do gradiente da Lagrangiana e das funções de restrição (veja (5.66) [^21]), e $K = \\mathbb{R}^n \\times \\mathbb{R}^q_+ \\times \\mathbb{R}^{p-q}_+$ define o conjunto viável para $z$, com $N_K(z)$ sendo o cone normal correspondente (veja (5.64) [^21]).

Uma solução KKT $\\bar{z} = (\\bar{x}, \\bar{\\lambda})$ é **fortemente regular** se a qualificação de restrição de independência linear (LICQ) vale em $\\bar{x}$ e se as condições suficientes de segunda ordem (strong form) são satisfeitas (veja (5.79) [^25]) [^25]. A condição LICQ garante que o conjunto de multiplicadores de Lagrange $\\Lambda(\\bar{x})$ é um singleton $\\{\\bar{\\lambda}\\}$ [^15]. A condição suficiente de segunda ordem (5.79) [^25] exige que o Hessiano da Lagrangiana $\\nabla^2_{xx} L(\\bar{x}, \\bar{\\lambda})$ seja positivo definido no subespaço linear gerado pelo cone crítico $C_X(\\bar{x})$ (definido em (5.80) [^25]).

Neste contexto, o cone crítico associado à desigualdade variacional KKT, $C_K(\\bar{z})$, é um subespaço linear se, e somente se, a condição de **complementaridade estrita** vale em $\\bar{x}$, ou seja, se $\\bar{\\lambda}_i > 0$ para todas as restrições de desigualdade ativas ($i \\in I(\\bar{x})$) [^25]. Sob LICQ, condições suficientes de segunda ordem e complementaridade estrita, a solução KKT $\\bar{z}$ é fortemente regular e o estimador SAA $\\hat{z}_N = (\\hat{x}_N, \\hat{\\lambda}_N)$ é assintoticamente normal [^25]. A distribuição limite é $N(0, J^{-1} \\Sigma J^{-T})$, onde $J = \\nabla\\phi(\\bar{z})$ é dado por (5.81) [^25] e $\\Sigma$ é a matriz de covariância de $\\Phi(\\bar{z}, \\xi)$ definida em (5.63) [^20], conforme (5.82) [^25].

### Conclusão

Este capítulo detalhou as propriedades assintóticas dos estimadores $\\hat{x}_N$ obtidos a partir da resolução de Equações Generalizadas Estocásticas via SAA. Demonstramos que, sob a condição crucial de **regularidade forte** da solução verdadeira $\\bar{x}$ e condições apropriadas de convergência uniforme para $\\hat{\\phi}_N$ e seu Jacobiano, o estimador SAA $\\hat{x}_N$ converge w.p. 1 para $\\bar{x}$ e sua distribuição assintótica pode ser caracterizada.

No caso de equações padrão ($\\Gamma(x) = \\{0\\}$), o estimador $\\hat{x}_N$ é **assintoticamente normal**, com média $\\bar{x}$ e matriz de covariância determinada pelo Jacobiano $J = \\nabla\\phi(\\bar{x})$ e pela covariância $\\Sigma$ do termo estocástico $\\Phi(\\bar{x}, \\xi)$. Para desigualdades variacionais ($\\Gamma(x) = N_X(x)$), a distribuição limite é dada por $d(Y)$, onde $d(\\cdot)$ resolve uma desigualdade variacional no **cone crítico** $C_X(\\bar{x})$. A normalidade assintótica ocorre se e somente se o cone crítico for um subespaço linear, condição esta que, no contexto das condições KKT, está ligada à **complementaridade estrita**. Esses resultados fornecem uma base teórica para a inferência estatística sobre soluções de equações generalizadas estocásticas obtidas via SAA.

### Referências

[^1]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 155)
[^2]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 156)
[^3]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 157)
[^4]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 158)
[^5]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 159)
[^6]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 160)
[^7]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 161)
[^8]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 162)
[^9]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 163)
[^10]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 164)
[^11]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 165)
[^12]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 166)
[^13]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 167)
[^14]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 168)
[^15]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 169)
[^16]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 170)
[^17]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 171)
[^18]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 172)
[^19]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 173)
[^20]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 174)
[^21]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 175)
[^22]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 176)
[^23]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 177)
[^24]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 178)
[^25]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 179)
[^26]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 180)
[^27]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 181)
[^28]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 182)
[^29]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 183)
[^30]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 184)
[^31]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 185)
[^32]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 186)
[^33]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 187)
[^34]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 188)
[^35]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 189)
[^36]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 190)
[^37]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 191)
[^38]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 192)
[^39]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 193)
[^40]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 194)
[^41]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 195)
[^42]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 196)
[^43]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 197)
[^44]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 198)
[^45]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 199)
[^46]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 200)
[^47]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 201)
[^48]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 202)
[^49]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 203)
[^50]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 204)
[^51]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 205)
[^52]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 206)
[^53]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 207)
[^54]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 208)
[^55]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 209)
[^56]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 210)
[^57]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 211)
[^58]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 212)
[^59]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 213)
[^60]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 214)
[^61]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 215)
[^62]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 216)
[^63]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 217)
[^64]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 218)
[^65]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 219)
[^66]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 220)
[^67]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 221)
[^68]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 222)
[^69]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 223)
[^70]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 224)
[^71]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 225)
[^72]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 226)
[^73]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 227)
[^74]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 228)
[^75]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 229)
[^76]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 230)
[^77]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 231)
[^78]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 232)
[^79]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 233)
[^80]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 234)
[^81]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 235)
[^82]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 236)
[^83]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 237)
[^84]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 238)
[^85]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 239)
[^86]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 240)
[^87]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (p. 241)
[^88]: Shapiro, A. (2009). Chapter 5: Statistical Inference. In *Lectures on Stochastic Programming: Modeling and Theory*. MPS/SIAM Series on Optimization. (