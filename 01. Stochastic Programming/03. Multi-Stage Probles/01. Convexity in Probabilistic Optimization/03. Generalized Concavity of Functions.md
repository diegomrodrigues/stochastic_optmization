## Capítulo 4.2: Concavidade Generalizada em Otimização Probabilística

### Introdução

Questões fundamentais para qualquer modelo de otimização dizem respeito à convexidade do conjunto viável, bem como à continuidade e diferenciabilidade das funções de restrição [^context_intro_ref]. A análise de modelos com funções de probabilidade baseia-se em propriedades específicas das distribuições de probabilidade subjacentes [^context_intro_ref]. Em particular, a teoria da **concavidade generalizada** desempenha um papel central na otimização probabilística, pois facilita a aplicação de ferramentas poderosas da análise convexa [^context_intro_ref]. Este capítulo explora a concavidade generalizada de funções e medidas, definindo funções **α-concave** e suas propriedades, como **log-concavidade** e **quasi-concavidade**, e discute sua relevância no contexto de restrições probabilísticas e de ordenação estocástica introduzidas anteriormente neste capítulo [^28-30].

### Concavidade Generalizada de Funções

Consideramos várias transformações não lineares de funções $f : \\Omega \\to \\mathbb{R}_+$ definidas em um conjunto convexo $\\Omega \\subseteq \\mathbb{R}^n$ [^1].

> **Definição 4.7.** Uma função não negativa $f(x)$ definida em um conjunto convexo $\\Omega \\subseteq \\mathbb{R}^n$ é dita **α-concave**, onde $\\alpha \\in [-\\infty, +\\infty]$, se para todo $x, y \\in \\Omega$ e todo $\\lambda \\in [0, 1]$ a seguinte desigualdade se mantém [^1]:
> $$ f(\\lambda x + (1 - \\lambda)y) \\ge m_\\alpha(f(x), f(y), \\lambda) $$
> onde $m_\\alpha: \\mathbb{R}_+ \\times \\mathbb{R}_+ \\times [0, 1] \\to \\mathbb{R}$ é definida como [^2]:
> $$ m_\\alpha(a, b, \\lambda) = 0 \\quad \\text{se } ab = 0, $$
> e se $a > 0, b > 0, 0 \\le \\lambda \\le 1$, então [^2]:
> $$ m_\\alpha(a, b, \\lambda) = \\begin{cases} a^\\lambda b^{1-\\lambda} & \\text{se } \\alpha = 0, \\\\ \\max\\{a, b\\} & \\text{se } \\alpha = \\infty, \\\\ \\min\\{a, b\\} & \\text{se } \\alpha = -\\infty, \\\\ (\\lambda a^\\alpha + (1 - \\lambda)b^\\alpha)^{1/\\alpha} & \\text{caso contrário.} \\end{cases} $$

No caso de $\\alpha = 0$, a função $f$ é chamada **logaritmicamente côncava** ou **log-concave** porque $\\ln f(\\cdot)$ é uma função côncava [^3]. No caso de $\\alpha = 1$, a função $f$ é simplesmente **côncava** [^4]. É importante notar que se $f$ e $g$ são duas funções mensuráveis, então a função $m_\\alpha(f(\\cdot), g(\\cdot), \\lambda)$ é uma função mensurável para todo $\\alpha$ e todo $\\lambda \\in (0, 1)$ [^5]. Além disso, $m_\\alpha(a, b, \\lambda)$ possui a seguinte propriedade importante [^6]:

**Lemma 4.8.** A aplicação $\\alpha \\mapsto m_\\alpha(a, b, \\lambda)$ é não decrescente e contínua [^6].

*Prova.* Primeiro mostramos a continuidade da aplicação em $\\alpha = 0$. Temos a seguinte cadeia de equações [^7]:
$$ \\ln m_\\alpha(a, b, \\lambda) = \\ln(\\lambda a^\\alpha + (1-\\lambda)b^\\alpha)^{1/\\alpha} = \\frac{1}{\\alpha} \\ln (\\lambda e^{\\alpha \\ln a} + (1-\\lambda)e^{\\alpha \\ln b}) $$
$$ = \\frac{1}{\\alpha} \\ln (1 + \\alpha(\\lambda \\ln a + (1-\\lambda)\\ln b) + o(\\alpha^2)) $$
Aplicando a regra de l'Hôpital ao lado direito para calcular seu limite quando $\\alpha \\to 0$, obtemos [^7]:
$$ \\lim_{\\alpha \\to 0} \\ln m_\\alpha(a, b, \\lambda) = \\lim_{\\alpha \\to 0} \\frac{\\lambda \\ln a + (1-\\lambda)\\ln b + o(\\alpha)}{1 + \\alpha(\\lambda \\ln a + (1-\\lambda)\\ln b) + o(\\alpha^2)} $$
$$ = \\lim_{\\alpha \\to 0} \\frac{\\ln(a^\\lambda b^{(1-\\lambda)}) + o(\\alpha)}{1 + \\alpha \\ln(a^\\lambda b^{(1-\\lambda)}) + o(\\alpha^2)} = \\ln(a^\\lambda b^{(1-\\lambda)}). $$
Agora passamos à monotonicidade da aplicação. Primeiro, considere o caso $0 < \\alpha < \\beta$. Definimos [^7]:
$$ h(\\alpha) = m_\\alpha(a, b, \\lambda) = \\exp\\left\\{\\frac{1}{\\alpha} \\ln [\\lambda a^\\alpha + (1-\\lambda)b^\\alpha]\\right\\} $$
Calculando sua derivada, obtemos [^7]:
$$ h'(\\alpha) = h(\\alpha)\\left( \\frac{1}{\\alpha} \\frac{\\lambda a^\\alpha \\ln a + (1-\\lambda)b^\\alpha \\ln b}{\\lambda a^\\alpha + (1-\\lambda)b^\\alpha} - \\frac{1}{\\alpha^2} \\ln[\\lambda a^\\alpha + (1-\\lambda)b^\\alpha] \\right) $$
Temos que demonstrar que a expressão do lado direito é não negativa. Substituindo $x = a^\\alpha$ e $y = b^\\alpha$, obtemos [^7]:
$$ h'(\\alpha) = \\frac{1}{\\alpha^2} h(\\alpha) \\left( \\frac{\\lambda x \\ln x + (1-\\lambda)y \\ln y}{\\lambda x + (1-\\lambda)y} - \\ln[\\lambda x + (1-\\lambda)y] \\right) $$
Usando o fato de que a função $z \\mapsto z \\ln z$ é convexa para $z > 0$ e que ambos $x, y > 0$, temos que [^7]:
$$ \\frac{\\lambda x \\ln x + (1-\\lambda)y \\ln y}{\\lambda x + (1-\\lambda)y} - \\ln[\\lambda x + (1-\\lambda)y] \\ge 0. $$
Como $h(\\alpha) > 0$, concluímos que $h(\\cdot)$ é não decrescente neste caso. Se $\\alpha < \\beta < 0$, temos a seguinte cadeia de relações [^8]:
$$ m_\\alpha(a, b, \\lambda) = \\left[ m_{-\\alpha}\\left(\\frac{1}{a}, \\frac{1}{b}, \\lambda\\right) \\right]^{-1} \\le \\left[ m_{-\\beta}\\left(\\frac{1}{a}, \\frac{1}{b}, \\lambda\\right) \\right]^{-1} = m_\\beta(a, b, \\lambda). $$
No caso $0 = \\alpha < \\beta$, podemos selecionar uma sequência $\\{\\alpha_k\\}$ tal que $\\alpha_k > 0$ e $\\lim_{k \\to \\infty} \\alpha_k = 0$. Usamos a monotonicidade de $h(\\cdot)$ para argumentos positivos e a continuidade em 0 para obter a asserção desejada. No caso $\\alpha < \\beta = 0$, procedemos da mesma forma, escolhendo uma sequência apropriada que se aproxima de 0 [^9]. Se $\\alpha < 0 < \\beta$, então a desigualdade $m_\\alpha(a, b, \\lambda) \\le m_0(a, b, \\lambda) \\le m_\\beta(a, b, \\lambda)$ segue dos dois casos anteriores [^9]. Resta investigar como a aplicação se comporta quando $\\alpha \\to \\infty$ ou $\\alpha \\to -\\infty$. Observamos que [^9]:
$$ \\max\\{\\lambda^{1/\\alpha}a, (1-\\lambda)^{1/\\alpha}b\\} \\le m_\\alpha(a, b, \\lambda) \\le \\max\\{a, b\\}. $$
Passando ao limite, obtemos [^9]:
$$ \\lim_{\\alpha \\to \\infty} m_\\alpha(a, b, \\lambda) = \\max\\{a, b\\}. $$
Também concluímos que [^9]:
$$ \\lim_{\\alpha \\to -\\infty} m_\\alpha(a, b, \\lambda) = \\lim_{\\alpha \\to \\infty} [m_{-\\alpha}(1/a, 1/b, \\lambda)]^{-1} = [\\max\\{1/a, 1/b\\}]^{-1} = \\min\\{a, b\\}. $$
Isso completa a prova. $\\blacksquare$ [^9]

Esta afirmação tem a implicação muito importante de que a **α-concavidade implica β-concavidade para todo β < α** [^10]. Portanto, todas as funções α-concave são (-∞)-concave, ou seja, **quasi-concave** [^11].

**Exemplo 4.9.** Considere a função de densidade de uma distribuição normal multivariada não degenerada em $\\mathbb{R}^s$ [^12]:
$$ \\theta(x) = \\frac{1}{\\sqrt{(2\\pi)^s \\det(\\Sigma)}} \\exp\\left\\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right\\} $$
onde $\\Sigma$ é uma matriz simétrica positiva definida de dimensão $s \\times s$, $\\det(\\Sigma)$ denota o determinante da matriz $\\Sigma$, e $\\mu \\in \\mathbb{R}^s$. Observamos que [^12]:
$$ \\ln \\theta(x) = -\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu) - \\ln(\\sqrt{(2\\pi)^s \\det(\\Sigma)}) $$
é uma função côncava. Portanto, concluímos que $\\theta$ é **0-concave**, ou **log-concave** [^12]. $\\blacksquare$

**Exemplo 4.10.** Considere um corpo convexo (um conjunto convexo compacto com interior não vazio) $\\Omega \\subset \\mathbb{R}^s$. A distribuição uniforme sobre este conjunto tem densidade definida como [^13]:
$$ \\theta(x) = \\begin{cases} \\frac{1}{V_s(\\Omega)}, & x \\in \\Omega, \\\\ 0, & x \\notin \\Omega, \\end{cases} $$
onde $V_s(\\Omega)$ denota a medida de Lebesgue de $\\Omega$. A função $\\theta(x)$ é **quasi-concave** em $\\mathbb{R}^s$ e **+∞-concave** em $\\Omega$ [^13]. $\\blacksquare$

**Exemplo 4.18 (Dirichlet Distribution).** Considere números positivos $\\alpha_1, \\dots, \\alpha_s$ e o simplex $S = \\{ x \\in \\mathbb{R}^s_+ : \\sum_{i=1}^s x_i \\le 1, x_i \\ge 0, i=1,\\dots,s \\}$. A função de densidade da distribuição de Dirichlet com parâmetros $\\alpha_1, \\dots, \\alpha_s$ é definida como [^26]:
$$ \\theta(x) = \\begin{cases} \\frac{\\Gamma(\\alpha_1 + \\dots + \\alpha_s)}{\\Gamma(\\alpha_1)\\dots\\Gamma(\\alpha_s)} x_1^{\\alpha_1-1} \\dots x_s^{\\alpha_s-1} & \\text{se } x \\in \\text{int } S, \\\\ 0 & \\text{caso contrário.} \\end{cases} $$
Assumindo que $x \\in \\text{int } S$, consideramos [^26]:
$$ \\ln \\theta(x) = \\sum_{i=1}^s (\\alpha_i - 1) \\ln x_i + \\ln \\Gamma(\\alpha_1 + \\dots + \\alpha_s) - \\sum_{i=1}^s \\ln \\Gamma(\\alpha_i). $$
Se $\\alpha_i \\ge 1$ para todo $i=1, \\dots, s$, então $\\ln \\theta(\\cdot)$ é uma função côncava no interior de $S$ e, portanto, $\\theta(x)$ é **log-concave** em cl $S$. Se todos os parâmetros satisfazem $\\alpha_i \\le 1$, então $\\theta(x)$ é log-convexa em cl $(S)$ [^27]. Para outros conjuntos de parâmetros, esta função de densidade não possui nenhuma propriedade de concavidade generalizada [^27]. $\\blacksquare$

Os resultados a seguir fornecem regras de cálculo para funções α-concave.

**Teorema 4.19.** Se a função $f : \\mathbb{R}^n \\to \\mathbb{R}_+$ é α-concave e a função $g : \\mathbb{R}^n \\to \\mathbb{R}_+$ é β-concave, onde $\\alpha, \\beta \\ge 1$, então a função $h : \\mathbb{R}^n \\to \\mathbb{R}$, definida como $h(x) = f(x) + g(x)$ é γ-concave com $\\gamma = \\min\\{\\alpha, \\beta\\}$ [^28].

*Prova.* Dados pontos $x_1, x_2 \\in \\mathbb{R}^n$ e um escalar $\\lambda \\in (0, 1)$, definimos $x_\\lambda = \\lambda x_1 + (1-\\lambda)x_2$. Ambas as funções $f$ e $g$ são γ-concave em virtude do Lemma 4.8. Usando a desigualdade de Minkowski, que vale para $\\gamma \\ge 1$, obtemos [^29]:
$$ f(x_\\lambda) + g(x_\\lambda) \\ge \\left[ (\\lambda(f(x_1))^\\gamma + (1-\\lambda)(f(x_2))^\\gamma)^{1/\\gamma} + (\\lambda(g(x_1))^\\gamma + (1-\\lambda)(g(x_2))^\\gamma)^{1/\\gamma} \\right] $$
$$ \\ge \\left[ \\lambda(f(x_1) + g(x_1))^\\gamma + (1-\\lambda)(f(x_2) + g(x_2))^\\gamma \\right]^{1/\\gamma}. $$
Isso completa a prova. $\\blacksquare$ [^29]

**Teorema 4.20.** Seja $f$ uma função côncava definida em um conjunto convexo $C \\subseteq \\mathbb{R}^s$ e $g: \\mathbb{R} \\to \\mathbb{R}$ seja uma função α-concave não negativa e não decrescente, $\\alpha \\in [-\\infty, \\infty]$. Então a função $g \\circ f$ é α-concave [^30].

*Prova.* Dados $x, y \\in \\mathbb{R}^s$ e um escalar $\\lambda \\in (0, 1)$, consideramos $z = \\lambda x + (1-\\lambda)y$. Temos $f(z) \\ge \\lambda f(x) + (1-\\lambda)f(y)$. Pela monotonicidade e α-concavidade de $g$, obtemos a seguinte cadeia de desigualdades [^31]:
$$ [g \\circ f](z) \\ge g(\\lambda f(x) + (1-\\lambda)f(y)) \\ge m_\\alpha(g(f(x)), g(f(y)), \\lambda). $$
Isso prova a asserção. $\\blacksquare$ [^31]

**Teorema 4.21.** Seja a função $f : \\mathbb{R}^m \\times \\mathbb{R}^s \\to \\mathbb{R}_+$ tal que para todo $y \\in Y \\subseteq \\mathbb{R}^s$ a função $f(\\cdot, y)$ é α-concave ($\\alpha \\in [-\\infty, \\infty]$) no conjunto convexo $X \\subseteq \\mathbb{R}^m$. Então a função $\\varphi(x) = \\inf_{y \\in Y} f(x, y)$ é α-concave em $X$ [^32].

*Prova.* Sejam $x_1, x_2 \\in X$ e um escalar $\\lambda \\in (0, 1)$ dados. Definimos $z = \\lambda x_1 + (1-\\lambda)x_2$. Podemos encontrar uma sequência de pontos $y_k \\in Y$ tal que [^33]:
$$ \\varphi(z) = \\inf_{y \\in Y} f(z, y) = \\lim_{k \\to \\infty} f(z, y_k). $$
Usando a α-concavidade da função $f(\\cdot, y)$, concluímos que [^33]:
$$ f(z, y_k) \\ge m_\\alpha(f(x_1, y_k), f(x_2, y_k), \\lambda). $$
A aplicação $(a, b) \\mapsto m_\\alpha(a, b, \\lambda)$ é monotônica para $a$ e $b$ não negativos e $\\lambda \\in (0, 1)$. Portanto, temos que [^33]:
$$ f(z, y_k) \\ge m_\\alpha(\\varphi(x_1), \\varphi(x_2), \\lambda). $$
Passando ao limite, obtemos a asserção. $\\blacksquare$ [^33]

**Lemma 4.22.** Se $\\alpha_i > 0, i = 1, \\dots, m$, e $\\sum_{i=1}^m \\alpha_i = 1$, então a função $f : \\mathbb{R}^m_+ \\to \\mathbb{R}$, definida como $f(x) = \\prod_{i=1}^m x_i^{\\alpha_i}$ é côncava [^34].

*Prova.* Mostraremos a afirmação para o caso $m=2$. Para pontos $x, y \\in \\mathbb{R}^2_+$ e um escalar $\\lambda \\in (0, 1)$, consideramos $\\lambda x + (1-\\lambda)y$. Definimos as quantidades $a_1 = (\\lambda x_1)^{\\alpha_1}$, $a_2 = ((1-\\lambda)y_1)^{\\alpha_1}$, $b_1 = (\\lambda x_2)^{\\alpha_2}$, $b_2 = ((1-\\lambda)y_2)^{\\alpha_2}$. Usando a desigualdade de Hölder, obtemos o seguinte [^35]:
$$ f(\\lambda x + (1-\\lambda)y) = (a_1^{1/\\alpha_1} + a_2^{1/\\alpha_1})^{\\alpha_1} (b_1^{1/\\alpha_2} + b_2^{1/\\alpha_2})^{\\alpha_2} $$
$$ \\ge a_1 b_1 + a_2 b_2 = \\lambda x_1^{\\alpha_1} x_2^{\\alpha_2} + (1-\\lambda) y_1^{\\alpha_1} y_2^{\\alpha_2}. $$
A asserção no caso geral segue por indução. $\\blacksquare$ [^35]

**Teorema 4.23.** Se as funções $f_i : \\mathbb{R}^n \\to \\mathbb{R}_+, i=1, \\dots, m$, são $\\alpha_i$-concave e $\\alpha_i$ são tais que $\\sum_{i=1}^m \\alpha_i^{-1} > 0$, então a função $g : \\mathbb{R}^{nm} \\to \\mathbb{R}_+$, definida como $g(x) = \\prod_{i=1}^m f_i(x_i)$ é γ-concave com $\\gamma = (\\sum_{i=1}^m \\alpha_i^{-1})^{-1}$ [^36].

*Prova.* Fixe pontos $x_1, x_2 \\in \\mathbb{R}^{nm}$, um escalar $\\lambda \\in (0, 1)$ e defina $x_\\lambda = \\lambda x_1 + (1-\\lambda)x_2$. Pela concavidade generalizada das funções $f_i, i=1, \\dots, m$, temos a seguinte desigualdade [^37]:
$$ \\prod_{i=1}^m f_i(x_{\\lambda i}) \\ge \\prod_{i=1}^m \\left( \\lambda f_i(x_{1i})^{\\alpha_i} + (1-\\lambda)f_i(x_{2i})^{\\alpha_i} \\right)^{1/\\alpha_i}. $$
Denotamos $y_{ij} = f_i(x_{ji})^{\\alpha_i}, j=1, 2$. Substituindo na última desigualdade exibida e elevando ambos os lados à potência $\\gamma$, obtemos [^37]:
$$ \\left( \\prod_{i=1}^m f_i(x_{\\lambda i}) \\right)^\\gamma \\ge \\prod_{i=1}^m (\\lambda y_{i1} + (1-\\lambda)y_{i2})^{\\gamma/\\alpha_i}. $$
Continuamos a cadeia de desigualdades usando o Lemma 4.22 [^37]:
$$ \\prod_{i=1}^m (\\lambda y_{i1} + (1-\\lambda)y_{i2})^{\\gamma/\\alpha_i} \\ge \\lambda \\prod_{i=1}^m [y_{i1}]^{\\gamma/\\alpha_i} + (1-\\lambda) \\prod_{i=1}^m [y_{i2}]^{\\gamma/\\alpha_i}. $$
Juntando as desigualdades e usando as substituições no lado direito da última desigualdade, concluímos que [^37]:
$$ \\left[ \\prod_{i=1}^m f_i(x_{\\lambda i}) \\right]^\\gamma \\ge \\lambda \\left[ \\prod_{i=1}^m f_i(x_{1i}) \\right]^\\gamma + (1-\\lambda) \\left[ \\prod_{i=1}^m f_i(x_{2i}) \\right]^\\gamma, $$
como requerido. $\\blacksquare$ [^37]

No caso especial em que as funções $f_i : \\mathbb{R}^n \\to \\mathbb{R}, i=1, \\dots, k$, são côncavas ($\\alpha_i=1$), podemos aplicar o Teorema 4.23 consecutivamente para concluir que $f_1 f_2$ é (1/2)-concave e $f_1 \\dots f_k$ é (1/k)-concave [^38].

**Lemma 4.24.** Se A é uma matriz simétrica positiva definida de tamanho $n \\times n$, então a função $A \\mapsto \\det(A)$ é (1/n)-concave [^39].

*Prova.* Considere duas matrizes simétricas positivas definidas $A, B$ de $n \\times n$ e $\\gamma \\in (0, 1)$. Poderíamos aplicar a desigualdade de Minkowski para matrizes [^40]:
$$ [\\det(A+B)]^{1/n} \\ge [\\det(A)]^{1/n} + [\\det(B)]^{1/n}, $$
o que implica a (1/n)-concavidade da função. Como a desigualdade (4.14) não é bem conhecida, fornecemos uma prova dela. Primeiro, consideramos o caso de matrizes diagonais. Neste caso, os determinantes de A e B são produtos de seus elementos diagonais e a desigualdade (4.14) segue do Lemma 4.22. No caso geral, seja $A^{1/2}$ a raiz quadrada simétrica positiva definida de A e $A^{-1/2}$ sua inversa. Temos [^40]:
$$ \\det(A+B) = \\det(A^{1/2} A^{-1/2} (A+B) A^{-1/2} A^{1/2}) $$
$$ = \\det(A^{-1/2} (A+B) A^{-1/2}) \\det(A) = \\det(I + A^{-1/2} B A^{-1/2}) \\det(A). $$
Note que $A^{-1/2} B A^{-1/2}$ é simétrica positiva definida e, portanto, podemos escolher uma matriz ortogonal $R$ de $n \\times n$, que a diagonaliza. Obtemos [^40]:
$$ \\det(I + A^{-1/2} B A^{-1/2}) = \\det(R^T (I + A^{-1/2} B A^{-1/2}) R) = \\det(I + R^T A^{-1/2} B A^{-1/2} R). $$
No lado direito da equação, temos uma soma de duas matrizes diagonais e podemos aplicar a desigualdade (4.14) para este caso. Concluímos que [^40]:
$$ [\\det(I + A^{-1/2} B A^{-1/2})]^{1/n} = [\\det(I + R^T A^{-1/2} B A^{-1/2} R)]^{1/n} $$
$$ \\ge 1 + [\\det(R^T A^{-1/2} B A^{-1/2} R)]^{1/n} = 1 + [\\det(B)]^{1/n} [\\det(A)]^{-1/n}. $$
Combinando esta desigualdade com (4.15), obtemos (4.14) no caso geral. $\\blacksquare$ [^40]

**Exemplo 4.25 (Dirichlet Distribution Continued).** Retornamos ao Exemplo 4.18. Vemos que as funções $x_i \\mapsto x_i^{\\beta_i}$ são $1/\\beta_i$-concave, desde que $\\beta_i > 0$. Portanto, a função de densidade da distribuição de Dirichlet é um produto de funções $(1/(\\alpha_i-1))$-concave, dado que todos os parâmetros $\\alpha_i > 1$. Em virtude do Teorema 4.23, obtemos que esta densidade é γ-concave com $\\gamma = (\\sum_{i=1}^m (\\alpha_i-1))^{-1}$ desde que $\\alpha_i > 1, i=1, \\dots, m$. Devido ao Corolário 4.16, a distribuição de Dirichlet é uma medida de probabilidade $\\alpha = (\\sum_{i=1}^m (\\alpha_i-1))^{-1} / (1 + s (\\sum_{i=1}^m (\\alpha_i-1))^{-1})$-concave [^41]. $\\blacksquare$


### Concavidade Generalizada de Medidas e Distribuições

Estendemos agora o conceito de α-concavidade para medidas de probabilidade.

> **Definição 4.11.** Uma medida de probabilidade $P$ definida nos subconjuntos mensuráveis de Lebesgue de um conjunto convexo $\\Omega \\subseteq \\mathbb{R}^s$ é dita **α-concave** se para quaisquer conjuntos mensuráveis de Borel $A, B \\subset \\Omega$ e para todo $\\lambda \\in [0, 1]$ temos a desigualdade [^14]:
> $$ P(\\lambda A + (1-\\lambda)B) \\ge m_\\alpha(P(A), P(B), \\lambda), $$
> onde $\\lambda A + (1-\\lambda)B = \\{\\lambda x + (1-\\lambda)y : x \\in A, y \\in B\\}$.

Dizemos que um vetor aleatório $Z$ com valores em $\\mathbb{R}^s$ tem uma **distribuição α-concave** se a medida de probabilidade $P_Z$ induzida por $Z$ em $\\mathbb{R}^s$ é α-concave [^15].

**Lemma 4.12.** Se um vetor aleatório $Z$ induz uma medida de probabilidade α-concave em $\\mathbb{R}^s$, então sua função de distribuição acumulada $F_Z$ é uma função α-concave [^16].

*Prova.* De fato, para dados pontos $z^1, z^2 \\in \\mathbb{R}^s$ e $\\lambda \\in [0, 1]$, definimos [^17]:
$$ A = \\{z \\in \\mathbb{R}^s : z_i \\le z^1_i, i=1, \\dots, s\\} \\quad \\text{e} \\quad B = \\{z \\in \\mathbb{R}^s : z_i \\le z^2_i, i=1, \\dots, s\\}. $$
Então $\\lambda A + (1-\\lambda)B = \\{z \\in \\mathbb{R}^s : z_i \\le \\lambda z^1_i + (1-\\lambda)z^2_i, i=1, \\dots, s\\}$. A desigualdade para $F_Z$ segue da desigualdade na Definição 4.11. $\\blacksquare$ [^17]

**Lemma 4.13.** Se um vetor aleatório $Z$ tem componentes independentes com distribuições marginais log-concave, então $Z$ tem uma distribuição log-concave [^18].

*Prova.* Para dois conjuntos de Borel $A, B \\subset \\mathbb{R}^s$ e $\\lambda \\in (0, 1)$, definimos o conjunto $C = \\lambda A + (1-\\lambda)B$. Denote as projeções de $A, B$ e $C$ no eixo coordenado $i$ por $A_i, B_i$ e $C_i$, $i=1, \\dots, s$, respectivamente. Para qualquer número $r \\in C_i$ existe $c \\in C$ tal que $c_i = r$, o que implica que temos $a \\in A$ e $b \\in B$ com $\\lambda a + (1-\\lambda)b = c$ e $r = \\lambda a_i + (1-\\lambda)b_i$. Em outras palavras, $r \\in \\lambda A_i + (1-\\lambda)B_i$, e concluímos que $C_i \\subseteq \\lambda A_i + (1-\\lambda)B_i$. Por outro lado, se $r \\in \\lambda A_i + (1-\\lambda)B_i$, então temos $a \\in A$ e $b \\in B$ tais que $r = \\lambda a_i + (1-\\lambda)b_i$. Definindo $c = \\lambda a + (1-\\lambda)b$, concluímos que $r \\in C_i$. Obtemos [^19]:
$$ \\ln[P_Z(C)] = \\sum_{i=1}^s \\ln[P_{Z_i}(C_i)] = \\sum_{i=1}^s \\ln[P_{Z_i}(\\lambda A_i + (1-\\lambda)B_i)] $$
$$ \\ge \\sum_{i=1}^s (\\lambda \\ln[P_{Z_i}(A_i)] + (1-\\lambda)\\ln[P_{Z_i}(B_i)]) $$
$$ = \\lambda \\ln[P_Z(A)] + (1-\\lambda)\\ln[P_Z(B)]. \\blacksquare $$ [^19]

Podemos relacionar a propriedade de α-concavidade de uma medida com a concavidade generalizada de sua densidade.

**Teorema 4.15.** Seja $\\Omega$ um subconjunto convexo de $\\mathbb{R}^s$ e seja $m > 0$ a dimensão do menor subespaço afim $L$ contendo $\\Omega$. A medida de probabilidade $P$ em $\\Omega$ é γ-concave com $\\gamma \\in [-\\infty, 1/m]$ se e somente se sua função de densidade de probabilidade $\\theta$ com respeito à medida de Lebesgue em $L$ é α-concave com [^21]:
$$ \\alpha = \\begin{cases} \\gamma / (1 - m\\gamma) & \\text{se } \\gamma \\in (-\\infty, 1/m), \\\\ -1/m & \\text{se } \\gamma = -\\infty, \\\\ +\\infty & \\text{se } \\gamma = 1/m. \\end{cases} $$

**Corolário 4.16.** Seja uma função integrável $\\theta(x)$ definida e positiva em um conjunto convexo não degenerado $\\Omega \\subset \\mathbb{R}^s$. Denote $c = \\int_\\Omega \\theta(x) dx$. Se $\\theta(x)$ é α-concave com $\\alpha \\in [-1/s, \\infty]$ e positiva no interior de $\\Omega$, então a medida $P$ em $\\Omega$ definida por $P(A) = \\frac{1}{c} \\int_A \\theta(x) dx, A \\subset \\Omega$, é γ-concave com [^22]:
$$ \\gamma = \\begin{cases} \\alpha / (1 + s\\alpha) & \\text{se } \\alpha \\in (-1/s, \\infty), \\\\ 1/s & \\text{se } \\alpha = \\infty, \\\\ -\\infty & \\text{se } \\alpha = -1/s. \\end{cases} $$
Em particular, se uma medida $P$ em $\\mathbb{R}^s$ tem uma função de densidade $\\theta(x)$ tal que $\\theta^{-1/s}$ é convexa, então $P$ é quasi-concave [^23].

**Exemplo 4.17.** Observamos no Exemplo 4.10 que a densidade da distribuição uniforme em um corpo convexo $\\Omega$ é uma função ∞-concave. Portanto, ela gera uma medida 1/s-concave em $\\Omega$ [^24]. Por outro lado, a densidade da distribuição normal (Exemplo 4.9) é log-concave (0-concave) e, portanto, gera uma medida de probabilidade log-concave (γ=0) [^25]. $\\blacksquare$

**Exemplo 4.27 (Gamma Distribution).** Uma distribuição gama univariada é dada pela seguinte função de densidade de probabilidade [^44]:
$$ f(z)