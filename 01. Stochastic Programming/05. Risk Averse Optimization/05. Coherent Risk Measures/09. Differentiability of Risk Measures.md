## Differentiability Properties of Risk Measures

### Introdução
Este capítulo explora as propriedades de diferenciabilidade das **risk measures**, com foco no **subdiferencial** $\\partial \\rho(Z)$ e na diferenciabilidade direcional de Hadamard. As conexões com funções compostas e a regra da cadeia serão detalhadas [^contexto]. A análise da diferenciabilidade é crucial para a otimização e caracterização de risk measures, permitindo a aplicação de técnicas de otimização baseadas em gradientes e a análise de sensibilidade das soluções ótimas.

### Conceitos Fundamentais

#### Subdiferencial de Risk Measures
O **subdiferencial** $\\partial \\rho(Z)$ de uma risk measure $\\rho$ em um ponto $Z$ é definido como o conjunto de todos os subgradientes de $\\rho$ em $Z$ [^6.3.1]. Formalmente,
$$\\partial \\rho(Z) = \\{\\zeta \\in Z^* : \\langle \\zeta, Z' - Z \\rangle \\leq \\rho(Z') - \\rho(Z), \\forall Z' \\in Z \\}$$\nonde $Z^*$ é o espaço dual de $Z$ e $\\langle \\cdot, \\cdot \\rangle$ denota o produto escalar entre $Z$ e $Z^*$. O subdiferencial generaliza o conceito de gradiente para funções não diferenciáveis.

**Proposição:** Se $\\rho$ é convexa, própria e lower semicontinuous, então
$$\\partial \\rho(Z) = \\arg \\max_{\\zeta \\in A} \\{\\langle \\zeta, Z \\rangle - \\rho^*(\\zeta)\\}$$\nonde $A$ é o domínio da função conjugada $\\rho^*$ [^6.3.1].

#### Diferenciabilidade Direcional de Hadamard
Uma risk measure $\\rho$ é dita **Hadamard directionally differentiable** em $Z$ se existe um funcional $\\rho'(Z, H)$ tal que para qualquer $H \\in Z$,
$$\\lim_{t \\to 0, H' \\to H} \\frac{\\rho(Z + tH') - \\rho(Z)}{t} = \\rho'(Z, H)$$\nonde a convergência $H' \\to H$ é na norma de $Z$. A diferenciabilidade direcional de Hadamard implica a existência de derivadas direcionais em todas as direções, e a função $\\rho'(Z, \\cdot)$ é homogênea e contínua [^6.3.1].

#### Conexões com Funções Compostas e a Regra da Cadeia
Frequentemente, encontramos risk measures aplicadas a funções compostas, como $\\rho(F(x))$, onde $F: \\mathbb{R}^n \\to Z$ é um mapeamento [^6.3.1]. A diferenciabilidade da função composta $\\phi(x) = \\rho(F(x))$ é crucial para problemas de otimização.

**Teorema (Regra da Cadeia):** Se $F: \\mathbb{R}^n \\to Z$ é uma função convexa e $\\rho: Z \\to \\mathbb{R}$ é uma risk measure convexa, finita e contínua em $Z = F(x)$, então $\\phi = \\rho \\circ F$ é directionally differentiable em $x$, e
$$\\phi'(x, h) = \\sup_{\\zeta \\in \\partial \\rho(Z)} \\langle \\zeta, F'(x, h) \\rangle$$\nonde $F'(x, h)$ é a derivada direcional de $F$ em $x$ na direção $h$ [^6.3.1].

#### Subdiferenciabilidade de Funções Compostas
Sob certas condições, a subdiferenciabilidade da função composta $\\phi(x) = \\rho(F(x))$ também pode ser estabelecida [^6.3.1].

**Teorema:** Se $F: \\mathbb{R}^n \\to Z$ é uma função convexa e $\\rho$ satisfaz as condições (R1) e (R2) e é finita e contínua em $Z := F(x)$, então a função composta $\\phi = \\rho \\circ F$ é subdiferenciável em $x$ e
$$\\partial \\phi(x) = \\text{cl} \\left( \\bigcup_{\\zeta \\in \\partial \\rho(Z)} \\int_\\Omega \\nabla f(x, \\omega) \\zeta(\\omega) dP(\\omega) \\right)$$\nonde $\\text{cl}$ denota o fecho topológico [^6.3.1].

### Conclusão
A análise da diferenciabilidade das risk measures fornece ferramentas essenciais para a otimização e caracterização dessas medidas. O subdiferencial e a diferenciabilidade direcional de Hadamard oferecem generalizações do conceito de gradiente para funções não diferenciáveis, permitindo a aplicação de técnicas de otimização baseadas em gradientes. A regra da cadeia e os resultados sobre subdiferenciabilidade de funções compostas são cruciais para problemas de otimização envolvendo risk measures aplicadas a funções de variáveis de decisão.

### Referências
[^6.3.1]: Ruszczyński, A., & Shapiro, A. (2006). *Risk-Averse Optimization*. Mathematical Programming Series.
[^contexto]: Conteúdo do contexto fornecido.

<!-- END -->