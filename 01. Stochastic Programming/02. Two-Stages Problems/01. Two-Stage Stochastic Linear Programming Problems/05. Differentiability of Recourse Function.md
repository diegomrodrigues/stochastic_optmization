## Capítulo 3: Propriedades de Diferenciabilidade e Subdiferenciais em Programação Estocástica Linear de Dois Estágios

### Introdução

Continuando a análise de problemas de programação estocástica linear de dois estágios, introduzidos no formato geral (2.1) [^1], este capítulo aprofunda-se nas propriedades analíticas da função de recurso do segundo estágio, **Q(x, ξ)**, e da função de custo esperado de recurso, **φ(x)**. Como vimos, Q(x, ξ) representa o valor ótimo do problema do segundo estágio (2.2) [^1] para uma dada decisão de primeiro estágio *x* e uma realização *ξ* dos parâmetros aleatórios. A compreensão das propriedades de diferenciabilidade destas funções é crucial para o desenvolvimento e análise de algoritmos de otimização. Focaremos em caracterizar estas propriedades através do conceito de **subdiferencial**, estabelecendo uma ligação fundamental entre as soluções do problema primal do segundo estágio e seu dual. Investigaremos também o caso específico onde o vetor aleatório **ξ** possui uma distribuição discreta, analisando a estrutura do custo esperado e suas implicações.

### Conceitos Fundamentais

#### Caracterização do Subdiferencial de Q(x, ξ)

A função de valor do segundo estágio, Q(x, ξ), definida como o mínimo de $q^T y$ sujeito a $Tx + Wy = h, y \\ge 0$ [^1], exibe propriedades estruturais importantes. Conforme estabelecido na Proposição 2.1, para um dado ξ, a função Q(·, ξ) é convexa [^3]. Se adicionalmente o conjunto $\\{π : W^T π \\le q\\}$ é não vazio e o problema (2.2) é factível para ao menos um *x*, então Q(·, ξ) é poliédrica [^3].

As propriedades de diferenciabilidade de Q(·, ξ) são elegantemente descritas através de seu subdiferencial. Consideremos o problema dual do segundo estágio (2.3) [^2]:
$$ \text{Max}_{\pi} \pi^T (h - Tx) \quad \text{s.t.} \quad W^T \pi \le q $$
Seja **D(x, ξ)** o conjunto das soluções ótimas deste problema dual (2.3) [^3]. A Proposição 2.2 estabelece uma relação direta entre o subdiferencial de Q no ponto x₀ e o conjunto D(x₀, ξ) [^3].

> **Proposição 2.2:** Suponha que para um dado $x = x_0$ e $ξ ∈ Ξ$, o valor $Q(x_0, ξ)$ é finito. Então $Q(·, ξ)$ é subdiferenciável em $x_0$ e
> $$ ∂Q(x_0, ξ) = -T^T D(x_0, ξ), $$
> onde $D(x, ξ) := \text{arg max}_{\pi \in \Pi(q)} \pi^T (h - Tx)$ é o conjunto das soluções ótimas do problema dual (2.3) [^3].

Esta proposição é central, pois conecta o comportamento local da função de custo do segundo estágio (via subdiferencial) com as soluções ótimas do problema dual associado. O fator $-T^T$ surge da aplicação da regra da cadeia para subdiferenciação, uma vez que $Q(x, ξ)$ pode ser vista como a composição da função $s_q(z) = \inf \{q^T y : Wy = z, y \ge 0\}$ [^2] com a transformação afim $z = h - Tx$. Especificamente, $Q(x, ξ) = s_q(h - Tx)$ [^2]. O subdiferencial de $s_q$ em $z_0 = h - Tx_0$ é precisamente o conjunto $D(x_0, ξ)$ [^4], e a fórmula (2.7) [^3] segue da regra da cadeia [^4]. A função $s_q(·)$ é a função suporte do conjunto poliédrico $\Pi(q) = \{\pi : W^T \pi \le q\}$ [^2], e sua subdiferenciabilidade está garantida quando $Q(x_0, ξ)$ é finito, o que implica que $\Pi(q)$ é não vazio [^4]. A natureza poliédrica de $\Pi(q)$ implica que $s_q(·)$ é linear por partes (*piecewise linear*) [^4], o que por sua vez implica que $Q(·, ξ)$ também o é.

#### Custo Esperado de Recurso para Distribuições Discretas

Consideremos agora o caso em que o vetor aleatório ξ possui suporte finito, $Ξ = \{ξ_1, ..., ξ_K\}$, com probabilidades $p_k > 0$, $k = 1, ..., K$ [^5]. A função de custo esperado de recurso é então dada por:
$$ φ(x) := E[Q(x, ξ)] = \sum_{k=1}^K p_k Q(x, ξ_k) $$
Esta função representa o valor esperado do custo ótimo do segundo estágio [^5]. Para um *x* fixo, o cálculo de $φ(x)$ equivale a resolver o problema de programação linear (2.14) [^5]. O problema de dois estágios completo (2.1) pode ser reformulado como um problema de programação linear de grande escala (2.15) [^5]:
$$ \text{Min}_{x, y_1, ..., y_K} c^T x + \sum_{k=1}^K p_k q_k^T y_k $$
$$ \text{s.t.} \quad T_k x + W_k y_k = h_k, \quad k = 1, ..., K $$
$$ \quad Ax = b $$
$$ \quad x \ge 0, \quad y_k \ge 0, \quad k = 1, ..., K $$
As propriedades da função de custo esperado $φ(x)$ derivam diretamente das propriedades de $Q(x, ξ_k)$ para cada cenário *k*.

> **Proposição 2.3:** Suponha que a distribuição de ξ tenha suporte finito $Ξ = \{ξ_1, ..., ξ_K\}$ e que o custo esperado de recurso $φ(·)$ tenha valor finito em pelo menos um ponto $x ∈ \mathbb{R}^n$. Então a função $φ(·)$ é poliédrica, e para qualquer $x_0 ∈ \text{dom } φ$,
> $$ ∂φ(x_0) = \sum_{k=1}^K p_k ∂Q(x_0, ξ_k). $$ [^5]

Como cada função $Q(·, ξ_k)$ é poliédrica (assumindo finitude, seguindo da Proposição 2.1 [^3]), a combinação linear convexa $φ(x) = \sum p_k Q(x, ξ_k)$ também é poliédrica [^6]. A fórmula (2.16) [^5] para o subdiferencial de $φ(x_0)$ é um caso particular do teorema de Moreau-Rockafellar [^6], que estabelece que o subdiferencial da soma de funções convexas é a soma dos subdiferenciais (sob certas condições de regularidade, que são satisfeitas aqui devido à natureza poliédrica das funções $Q_k(\cdot) := Q(\cdot, \xi_k)$ [^6]). Substituindo a expressão para $∂Q(x_0, ξ_k)$ dada por (2.18) [^6] (que é idêntica a (2.7) [^3]), obtemos:
$$ ∂φ(x_0) = \sum_{k=1}^K p_k (-T_k^T D(x_0, ξ_k)) = -\sum_{k=1}^K p_k T_k^T D(x_0, ξ_k) $$
Este resultado mostra que o subdiferencial do custo esperado é a soma ponderada (pelas probabilidades $p_k$) dos subdiferenciais da função de recurso para cada cenário, transformados pela respectiva matriz $-T_k^T$.

Uma consequência direta desta caracterização é a condição para a diferenciabilidade da função de custo esperado $φ(x)$ em um ponto $x_0$.

> **Corolário:** A função de custo esperado $φ(x)$ é diferenciável em $x_0$ se e somente se, para cada cenário $ξ = ξ_k$, $k = 1, ..., K$, o problema dual do segundo estágio (correspondente a (2.18) [^6]) possui uma solução ótima única.

Se cada conjunto $D(x_0, ξ_k)$ for um singleton, digamos $D(x_0, ξ_k) = \{\pi_k\}$, então cada $∂Q(x_0, ξ_k)$ será um singleton $\{-T_k^T \pi_k\}$. Consequentemente, $∂φ(x_0)$ será também um singleton, contendo apenas o gradiente $\nabla φ(x_0) = -\sum_{k=1}^K p_k T_k^T \pi_k$. A unicidade da solução dual para *todos* os cenários é, portanto, a condição chave para a diferenciabilidade da função de custo esperado no caso discreto [^6].

### Conclusão

Este capítulo detalhou as propriedades de diferenciabilidade da função de recurso $Q(x, ξ)$ e da função de custo esperado $φ(x)$ em problemas de programação estocástica linear de dois estágios. Demonstramos que o subdiferencial de $Q(x, ξ)$ é caracterizado pelo conjunto de soluções ótimas do problema dual, $D(x, ξ)$, e pela transposta da matriz de tecnologia $T$, conforme $∂Q(x_0, ξ) = -T^T D(x_0, ξ)$ [^3]. Para o caso de distribuições discretas, a função de custo esperado $φ(x)$ é poliédrica e seu subdiferencial é a soma ponderada dos subdiferenciais de cada cenário, $∂φ(x_0) = \sum p_k ∂Q(x_0, ξ_k)$ [^5]. A diferenciabilidade de $φ(x)$ em um ponto $x_0$ está condicionada à unicidade da solução ótima dual para cada um dos cenários $k=1, ..., K$ [^6]. Estas caracterizações são fundamentais para a análise teórica e para o projeto de métodos de solução para programas estocásticos.

### Referências

[^1]: Page 27, Equations (2.1), (2.2).
[^2]: Page 28, Equations (2.3), (2.4), (2.5), (2.6).
[^3]: Page 28, Proposition 2.1, Proposition 2.2, Equation (2.7), Definition of D(x, ξ).
[^4]: Page 29, Proof of Proposition 2.2, Equation (2.8), Discussion of pos W and piecewise linearity.
[^5]: Page 30, Equations (2.12), (2.13), (2.14), (2.15), Proposition 2.3, Equation (2.16).
[^6]: Page 31, Proof of Proposition 2.3, Moreau-Rockafellar reference, Equation (2.18), Differentiability condition discussion.

<!-- END -->