## Modelos de Seleção de Portfólio Multiestágio: Formulação e Solução via Programação Dinâmica

### Introdução

Este capítulo aprofunda a análise de problemas de seleção de portfólio, transitando dos modelos estáticos, como os abordados na Seção 1.4.1 [^13], para uma formulação dinâmica multiestágio. **Modelos de seleção de portfólio multiestágio** permitem o rebalanceamento do portfólio ao longo de múltiplos períodos de tempo, ajustando a alocação de capital entre diferentes ativos com base nos retornos observados em períodos anteriores [^16]. Especificamente, consideramos um horizonte de planejamento de $T$ períodos, onde as decisões de rebalanceamento podem ser tomadas nos instantes $t = 1, ..., T - 1$. O objetivo fundamental é maximizar a utilidade esperada da riqueza no final do horizonte de planejamento, $W_T$, formalizado como $\\max E[U(W_T)]$, onde $U(\\cdot)$ é uma função de utilidade apropriada, tipicamente côncava e não decrescente para modelar a aversão ao risco [^14]. A metodologia central para resolver tais problemas é a **programação dinâmica** (*dynamic programming*), que aborda o problema de otimização de forma recursiva, retrocedendo no tempo (*backward in time*) [^17]. A otimização é realizada sobre o conjunto de todas as **políticas implementáveis** (*implementable policies*) e **factíveis** (*feasible policies*) [^17]. Como veremos, sob certas condições, notavelmente a independência estagiária (*stagewise independence*), a política ótima pode exibir um comportamento **miópico** (*myopic*) [^20].

### Conceitos Fundamentais

#### Formulação do Problema Multiestágio

Consideremos um investidor com uma riqueza inicial $W_0$ no instante $t=0$. O horizonte de investimento é dividido em $T$ períodos. Em cada período $t = 1, ..., T$, os $n$ ativos disponíveis geram taxas de retorno $R_{it}$, $i=1, ..., n$. Definimos o processo aleatório (vetorial) $\\xi_t = (\\xi_{1t}, ..., \\xi_{nt})$, onde $\\xi_{it} = 1 + R_{it}$ representa o fator de retorno bruto do ativo $i$ no período $t$ [^17]. Assume-se que a distribuição de probabilidade conjunta do processo $\\{\\xi_1, ..., \\xi_T\\}$ é conhecida [^17].

No início de cada período $t$ (começando com $t=0$), antes da realização de $\\xi_t$ (para $t \\ge 1$), o investidor decide como alocar a riqueza disponível $W_t$ entre os $n$ ativos. Seja $x_{it}$ a quantia investida no ativo $i$ durante o período $t+1$ (ou seja, a decisão é tomada no instante $t$). O vetor de decisão é $x_t = (x_{1t}, ..., x_{nt})$ [^17]. Uma política de investimento é uma sequência de funções de decisão $x_t(\\cdot)$ que determinam a alocação em cada estágio.

> Uma **política implementável** (*implementable policy*) é uma sequência de funções $x_t = x_t(\\xi_{[t]})$, $t = 0, ..., T-1$, onde a decisão $x_t$ no instante $t$ depende apenas da informação disponível até aquele momento, ou seja, das realizações passadas do processo de retornos $\\xi_{[t]} = (\\xi_1, ..., \\xi_t)$ (com $x_0$ sendo constante, pois depende apenas de $W_0$) [^17]. Isso reflete a restrição de **não antecipatividade** (*nonanticipativity constraint*) [^7].

> Uma política implementável é dita **factível** (*feasible*) se satisfaz as restrições do modelo com probabilidade 1 (w.p. 1). No contexto de seleção de portfólio, as restrições típicas são a não negatividade dos investimentos e a restrição orçamentária [^17]:
> $$ x_{it}(\\xi_{[t]}) \\ge 0, \\quad i = 1, ..., n, \\quad t = 0, ..., T-1 $$\
> $$ \\sum_{i=1}^n x_{it}(\\xi_{[t]}) = W_t, \\quad t = 0, ..., T-1 $$\
> onde $W_t$ é a riqueza no início do período $t$.

A riqueza evolui ao longo do tempo de acordo com a dinâmica:
$$ W_t = \\sum_{i=1}^n \\xi_{it} x_{i, t-1}(\\xi_{[t-1]}), \\quad t = 1, ..., T $$\
com $W_0$ sendo a riqueza inicial [^17]. O objetivo é encontrar uma política implementável e factível que maximize a utilidade esperada da riqueza terminal:
$$ \\max_{x_0, ..., x_{T-1}} E[U(W_T)] $$\
onde a maximização é sobre todas as políticas implementáveis e factíveis [^17].

#### Solução via Programação Dinâmica

A estrutura sequencial do problema sugere o uso de programação dinâmica. Procedemos de forma recursiva, de trás para frente no tempo.

**Estágio Final (t = T-1):** No início do período $T-1$, a riqueza $W_{T-1}$ e a história dos retornos $\\xi_{[T-1]}$ são conhecidas. A decisão $x_{T-1}$ deve ser tomada para maximizar a utilidade esperada de $W_T = \\sum_{i=1}^n \\xi_{iT} x_{i,T-1}$. O problema a ser resolvido é:
$$ Q_{T-1}(W_{T-1}, \\xi_{[T-1]}) = \\max_{x_{T-1}} E \\left\\{ U\\left(\\sum_{i=1}^n \\xi_{iT} x_{i,T-1}\\right) \\bigg| \\xi_{[T-1]} \\right\\} $$\
sujeito a $\\sum_{i=1}^n x_{i,T-1} = W_{T-1}$ e $x_{i,T-1} \\ge 0$ para todo $i$ [^17, ^18]. A função $Q_{T-1}(\\cdot, \\cdot)$ é a **função valor** (*value function*) ou função de custo-a-seguir (*cost-to-go function*) no estágio $T-1$. A esperança é condicional à informação $\\xi_{[T-1]}$ [^18].

**Estágios Intermediários (t = T-2, ..., 1):** No início do período $t$, com riqueza $W_t$ e história $\\xi_{[t]}$ conhecidas, a decisão $x_t$ é tomada para maximizar a esperança condicional da função valor do próximo estágio, $Q_{t+1}$. O problema recursivo é:
$$ Q_t(W_t, \\xi_{[t]}) = \\max_{x_t} E \\left\\{ Q_{t+1}\\left(W_{t+1}, \\xi_{[t+1]}\\right) \\bigg| \\xi_{[t]} \\right\\} $$\
sujeito a $W_{t+1} = \\sum_{i=1}^n \\xi_{i,t+1} x_{it}$, $\\sum_{i=1}^n x_{it} = W_t$ e $x_{it} \\ge 0$ para todo $i$ [^18].

**Estágio Inicial (t = 0):** No instante inicial, a riqueza $W_0$ é conhecida. A decisão $x_0$ é tomada para maximizar a esperança (incondicional) da função valor do primeiro estágio:
$$ Q_0(W_0) = \\max_{x_0} E [ Q_1(W_1, \\xi_1) ] $$\
sujeito a $W_1 = \\sum_{i=1}^n \\xi_{i1} x_{i0}$, $\\sum_{i=1}^n x_{i0} = W_0$ e $x_{i0} \\ge 0$ para todo $i$ [^18]. O valor $Q_0(W_0)$ é o valor ótimo esperado da utilidade terminal para o problema original (1.49) [^17].

A solução das equações de programação dinâmica (1.50)-(1.52) [^17, ^18] fornece a **política ótima** $\\bar{x}_t(W_t, \\xi_{[t]})$.

#### Simplificação sob Independência Estagiária (*Stagewise Independence*)

A resolução das equações de programação dinâmica pode ser extremamente complexa, pois as funções valor $Q_t$ dependem da história completa $\\xi_{[t]}$ e da riqueza $W_t$. Uma simplificação significativa ocorre sob a hipótese de **independência estagiária** (*stagewise independence*) [^7, ^18].

> A hipótese de **independência estagiária** assume que o vetor de retornos $\\xi_t$ em cada período $t$ é estocasticamente independente dos vetores de retornos passados $\\xi_1, ..., \\xi_{t-1}$, para $t = 2, ..., T$ [^18].

Sob esta hipótese, a esperança condicional em cada estágio se torna uma esperança incondicional com respeito aos retornos do próximo período. Consequentemente, as funções valor não dependem mais da história passada $\\xi_{[t]}$, mas apenas da riqueza atual $W_t$: $Q_t(W_t, \\xi_{[t]}) = Q_t(W_t)$ para $t = 1, ..., T-1$ [^18]. As equações recursivas simplificam-se para:
$$ Q_t(W_t) = \\max_{x_t} E [ Q_{t+1}(W_{t+1}) ] $$\
sujeito a $W_{t+1} = \\sum_{i=1}^n \\xi_{i,t+1} x_{it}$, $\\sum_{i=1}^n x_{it} = W_t$ e $x_{it} \\ge 0$.
E no último estágio:
$$ Q_{T-1}(W_{T-1}) = \\max_{x_{T-1}} E \\left\\{ U\\left(\\sum_{i=1}^n \\xi_{iT} x_{i,T-1}\\right) \\right\\} $$\
sujeito a $\\sum_{i=1}^n x_{i,T-1} = W_{T-1}$ e $x_{i,T-1} \\ge 0$ [^18].

#### Casos Especiais de Funções de Utilidade

A estrutura da política ótima depende crucialmente da forma da função de utilidade $U(\\cdot)$. Duas formas são particularmente tratáveis sob independência estagiária, levando a políticas ótimas miópicas.

**Utilidade Logarítmica ($U(W) = \\ln W$):**
Para a função de utilidade logarítmica, definida para $W > 0$, pode-se mostrar que a função valor possui uma estrutura aditiva [^18]. Utilizando a propriedade $\\ln(aW) = \\ln a + \\ln W$, demonstra-se recursivamente que $Q_t(W_t) = v_t + \\ln W_t$ para $t=1, ..., T-1$, onde $v_t$ é uma constante que não depende de $W_t$ (sob independência estagiária) [^19]. A equação de Bellman torna-se:
$$ v_t + \\ln W_t = \\max_{x_t} E [ v_{t+1} + \\ln W_{t+1} ] = v_{t+1} + \\max_{x_t} E \\left\\{ \\ln \\left( \\sum_{i=1}^n \\xi_{i,t+1} x_{it} \\right) \\right\\} $$\
sujeito a $\\sum_{i=1}^n x_{it} = W_t$ e $x_{it} \\ge 0$.
Substituindo $x_{it} = w_{it} W_t$, onde $\\sum w_{it} = 1$ e $w_{it} \\ge 0$ são as frações de riqueza alocadas, temos $\\ln W_{t+1} = \\ln (\\sum \\xi_{i,t+1} w_{it} W_t) = \\ln W_t + \\ln (\\sum \\xi_{i,t+1} w_{it})$. A maximização se separa:
$$ v_t = v_{t+1} + \\max_{w_t} E \\left\\{ \\ln \\left( \\sum_{i=1}^n \\xi_{i,t+1} w_{it} \\right) \\right\\} $$\
sujeito a $\\sum_{i=1}^n w_{it} = 1$ e $w_{it} \\ge 0$.
Isso implica que a decisão ótima sobre as frações $w_{it}^*$ em cada estágio $t$ depende apenas da distribuição de $\\xi_{t+1}$ e é independente do nível de riqueza $W_t$ e dos estágios futuros (além de $t+1$). A otimização é feita de forma **completamente miópica** (*completely myopic fashion*) [^19]. A política ótima é da forma $\\bar{x}_t(W_t) = W_t w_t^*$, onde $w_t^*$ é a solução do problema de maximização de um período (Eq. 1.59 para $W_t=1$) [^19, ^20]. $\\blacksquare$

**Utilidade Potência ($U(W) = W^\\gamma$, $0 < \\gamma < 1$):**
Para a função de utilidade potência (CRRA - Constant Relative Risk Aversion), definida para $W \\ge 0$, e assumindo independência estagiária, pode-se mostrar por indução retroativa que a função valor tem a forma $Q_t(W_t) = \\eta_t W_t^\\gamma$, onde $\\eta_t$ é uma constante positiva [^20]. A relação $Q_{T-1}(W_{T-1}) = W_{T-1}^\\gamma Q_{T-1}(1)$ é um passo chave na derivação [^20]. A equação de Bellman torna-se:
$$ \\eta_t W_t^\\gamma = \\max_{x_t} E [ \\eta_{t+1} W_{t+1}^\\gamma ] = \\eta_{t+1} \\max_{x_t} E \\left\\{ \\left( \\sum_{i=1}^n \\xi_{i,t+1} x_{it} \\right)^\\gamma \\right\\} $$\
sujeito a $\\sum_{i=1}^n x_{it} = W_t$ e $x_{it} \\ge 0$.
Novamente, substituindo $x_{it} = w_{it} W_t$, temos $W_{t+1}^\\gamma = (W_t \\sum \\xi_{i,t+1} w_{it})^\\gamma = W_t^\\gamma (\\sum \\xi_{i,t+1} w_{it})^\\gamma$. A maximização se separa:
$$ \\eta_t = \\eta_{t+1} \\max_{w_t} E \\left\\{ \\left( \\sum_{i=1}^n \\xi_{i,t+1} w_{it} \\right)^\\gamma \\right\\} $$\
sujeito a $\\sum_{i=1}^n w_{it} = 1$ e $w_{it} \\ge 0$.
Similarmente ao caso logarítmico, a decisão ótima sobre as frações $w_{it}^*$ em cada estágio $t$ é independente do nível de riqueza $W_t$ e dos estágios futuros.

> Sob a hipótese de **independência estagiária**, para a função de utilidade potência $U(W)=W^\\gamma$ ($0<\\gamma<1$), a política ótima $\\bar{x}_t = \\bar{x}_t(W_t)$ é obtida de maneira **miópica** (*myopic way*) como a solução ótima do problema:
> $$ \\max_{x_t} E \\left\\{ \\left( \\sum_{i=1}^n \\xi_{i,t+1} x_{it} \\right)^\\gamma \\right\\} \\quad \\text{s.t.} \\sum_{i=1}^n x_{it} = W_t, x_{it} \\ge 0 $$\
> Esta é equivalente a encontrar as frações ótimas $w_t^*$ resolvendo o problema (1.60) para $W_t=1$, e então a política ótima é $\\bar{x}_t(W_t) = W_t w_t^*$ [^20]. $\\blacksquare$

#### Políticas de Rebalanceamento (Decision Rules)

As políticas ótimas derivadas para as utilidades logarítmica e potência sob independência estagiária pertencem a uma classe de regras de decisão conhecidas como **políticas de mistura fixa** (*fixed mix policies*) [^21].

> Uma **política de mistura fixa** é definida por $x_t(W_t) = W_t x^*$, para $t=0, ..., T-1$, onde $x^* = (x_1^*, ..., x_n^*)$ é um vetor de frações constantes tal que $x_i^* \\ge 0$ e $\\sum_{i=1}^n x_i^* = 1$ [^21].

Como visto, tais políticas são ótimas para $U(W) = \\ln W$ (com $x^*$ sendo a solução de (1.59) para $W_t=1$) e $U(W) = W^\\gamma$ (com $x^*$ sendo a solução de (1.60) para $W_t=1$) sob independência estagiária [^21]. Embora possam não ser ótimas em outros cenários, sua simplicidade as torna implementáveis [^21]. Sob independência estagiária, a riqueza evolui como $W_{t+1} = W_t \\sum_{i=1}^n \\xi_{i,t+1} x_i^*$ [^21]. A riqueza esperada segue $E[W_{t+1}] = E[W_t] (x^{*T} \\mu_{t+1})$ [^21], e a variância relativa $(\\text{Var}[W_t]) / (E[W_t])^2$ pode ser analisada recursivamente (Eq. 1.67, 1.68), mostrando que a razão do desvio padrão para a riqueza esperada cresce na ordem de $O(\\sqrt{T})$ se os retornos médios e variâncias forem de ordem similar ao longo do tempo [^21, ^22].

### Conclusão

Este capítulo apresentou a formulação de problemas de seleção de portfólio multiestágio, cujo objetivo é maximizar a utilidade esperada da riqueza terminal através de decisões sequenciais de rebalanceamento. A metodologia padrão para a solução é a programação dinâmica, que opera recursivamente de trás para frente no tempo. Foi demonstrado que, sob a hipótese crucial de independência estagiária dos retornos e para classes específicas de funções de utilidade (logarítmica e potência), a política ótima exibe um comportamento miópico, simplificando consideravelmente a determinação das decisões ótimas. A política ótima nesses casos envolve manter uma mistura fixa de ativos (proporções constantes da riqueza) ao longo do tempo. Contudo, é fundamental ressaltar que esse comportamento miópico é bastante excepcional e tende a desaparecer em cenários mais realistas, como na presença de custos de transação [^20] ou quando a independência estagiária não se verifica.

### Referências

[^7]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 7.
[^8]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 8.
[^13]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 13.
[^14]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 14.
[^16]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 16.
[^17]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 17.
[^18]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 18.
[^19]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 19.
[^20]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 20.
[^21]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 21.
[^22]: Ruszczyński, A., & Shapiro, A. (2009). Stochastic Programming Models. Chapter 1, Page 22.
<!-- END -->