## Capítulo X: Programação Estocástica Multiestágio: Formulação e Solução via Programação Dinâmica

### Introdução

Este capítulo aprofunda-se nos **modelos de programação estocástica multiestágio** (*multistage stochastic programming models*), que são fundamentais para abordar problemas de tomada de decisão sequencial sob incerteza ao longo de múltiplos períodos de tempo [^1]. Diferentemente dos modelos de dois estágios, onde uma decisão inicial é seguida por uma ação de *recourse* após a revelação da incerteza [^2], os modelos multiestágio envolvem uma sequência de decisões e observações. Uma característica central desses modelos é a incorporação da **restrição de não-antecipatividade** (*nonanticipativity constraint*), que estipula que as decisões em qualquer estágio só podem depender da informação disponível até aquele momento, e não de observações futuras [^2]. Exploraremos como as equações de **programação dinâmica** (*dynamic programming*) fornecem uma estrutura poderosa para decompor e resolver esses problemas complexos, utilizando o conceito de **funções de valor** (*value functions*) para conectar as decisões entre os estágios [^5], [^7]. Adicionalmente, discutiremos a hipótese simplificadora de **independência estagial** (*stagewise independence*) [^8] e como ela pode levar a políticas ótimas com estruturas especiais, como a **política basestock** (*basestock policy*) em problemas de controle de estoque [^13].

### Conceitos Fundamentais

#### Definição do Problema Multiestágio e Não-Antecipatividade

Um problema de programação estocástica multiestágio envolve tomar decisões em múltiplos períodos $t = 1, ..., T$, onde a informação sobre parâmetros incertos (como demanda ou retornos de ativos) é revelada progressivamente ao longo do tempo [^1]. Considere, por exemplo, um problema de gerenciamento de estoque ao longo de $T$ períodos [^1]. A cada período $t$, uma empresa observa o nível de estoque atual $y_t$ e decide uma quantidade a pedir para elevar o estoque ao nível $x_t$ (ou seja, pede $x_t - y_t$, assumindo $x_t \\ge y_t$) [^1]. Subsequentemente, a demanda $D_t$ para o período $t$ é realizada, determinando o nível de estoque $y_{t+1} = x_t - D_t$ para o início do próximo período [^1]. A demanda $\\{D_t\\}_{t=1}^T$ é um processo estocástico [^1].

De forma crucial, a decisão sobre o nível de estoque $x_t$ no estágio $t$ deve ser tomada com base apenas na informação disponível naquele momento [^2]. Denotando o histórico do processo de demanda até o tempo $t$ como $D_{[t]} = (D_1, ..., D_t)$ e uma realização particular como $d_{[t]} = (d_1, ..., d_t)$ [^2], a decisão $x_t$ só pode depender da realização observada $d_{[t-1]}$ [^2]. Esta é a **restrição de não-antecipatividade**: as decisões não podem antecipar realizações futuras da incerteza [^2]. Assume-se, contudo, que a distribuição de probabilidade do processo de demanda é conhecida, incluindo as distribuições condicionais de $D_t$ dado $D_{[t-1]} = d_{[t-1]}$ [^2]. Uma sequência de regras de decisão $x_t = x_t(d_{[t-1]})$ que satisfaz a não-antecipatividade para $t=1,...,T$ (com $x_1$ sendo determinístico, pois $d_{[0]}$ é vazio) é chamada de **política implementável** (*implementable policy*) [^10]. Uma política é **factível** (*feasible*) se satisfizer outras restrições do problema (como $x_t \\ge y_t$ no exemplo do estoque) com probabilidade 1 [^10]. O objetivo é encontrar uma política implementável e factível que otimize um critério esperado, como minimizar o custo total esperado [^1], [^11] ou maximizar a utilidade esperada [^21].

#### Formulação via Programação Dinâmica

A estrutura sequencial dos problemas multiestágio presta-se naturalmente à solução via **programação dinâmica** (*dynamic programming*), que decompõe o problema em uma série de subproblemas menores resolvidos recursivamente para trás no tempo (*backward in time*) [^5].

1.  **Último Estágio (T):** No último estágio $t=T$, dado o estado do sistema (e.g., nível de estoque $y_T$) e o histórico de informação $d_{[T-1]}$, toma-se a decisão final $x_T$. O problema é otimizar o custo (ou utilidade) imediato mais o valor esperado de quaisquer custos finais, condicionado à informação $d_{[T-1]}$ [^3]. Por exemplo, no problema de estoque, resolve-se:\
    $$ \\min_{x_T \\ge y_T} c_T(x_T - y_T) + \\mathbb{E}\\{b_T[D_T - x_T]_+ + h_T[x_T - D_T]_+ | D_{[T-1]} = d_{[T-1]}\\} $$\
    O valor ótimo deste problema é a **função de valor** (*value function*) no último estágio, denotada por $Q_T(y_T, d_{[T-1]})$ [^3]. Ela representa o custo mínimo esperado a partir do estado $(y_T, d_{[T-1]})$ até o final do horizonte.

2.  **Recursão para Estágios Anteriores (t < T):** Para um estágio $t$ ($1 \\le t < T$), dado o estado $y_t$ e o histórico $d_{[t-1]}$, a decisão $x_t$ é tomada para minimizar o custo imediato mais o valor esperado da função de valor do próximo estágio $Q_{t+1}$. A equação de programação dinâmica é [^5]:\
    $$ Q_t(y_t, d_{[t-1]}) = \\min_{x_t \\ge y_t} c_t(x_t - y_t) + \\mathbb{E}\\{b_t[D_t - x_t]_+ + h_t[x_t - D_t]_+ + Q_{t+1}(x_t - D_t, D_{[t]}) | D_{[t-1]} = d_{[t-1]}\\} $$\
    Note que $D_{[t]}$ dentro da esperança inclui $D_t$ e $d_{[t-1]}$. A esperança é tomada sobre a distribuição (condicional) de $D_t$ dado $d_{[t-1]}$ [^5]. Esta equação é resolvida iterativamente para $t = T-1, T-2, ..., 1$ [^5]. A função $Q_t(y_t, d_{[t-1]})$ representa o custo ótimo esperado do estágio $t$ até o final do horizonte, começando no estado $y_t$ e com histórico $d_{[t-1]}$ [^4], [^7].

3.  **Primeiro Estágio (t=1):** Finalmente, no primeiro estágio, dado o estado inicial $y_1$, resolve-se [^6]:\
    $$ \\min_{x_1 \\ge y_1} c_1(x_1 - y_1) + \\mathbb{E}\\{b_1[D_1 - x_1]_+ + h_1[x_1 - D_1]_+ + Q_2(x_1 - D_1, D_1)\\} $$\
    A solução $x_1^*$ deste problema é a decisão ótima inicial. As decisões ótimas para os estágios subsequentes $t > 1$ são encontradas resolvendo as respectivas minimizações nas equações de DP (passo 2) para a realização $d_{[t-1]}$ observada [^11].

A principal dificuldade computacional reside no cálculo e representação das funções de valor $Q_t(y_t, d_{[t-1]})$, pois elas podem depender não apenas do estado atual $y_t$, mas também de todo o histórico de informações $d_{[t-1]}$ [^7]. Para processos estocásticos gerais, o espaço de estados efetivo cresce exponencialmente com o número de estágios, um fenômeno conhecido como a **maldição da dimensionalidade** (*curse of dimensionality*) [^14]. Frequentemente, as funções de valor precisam ser aproximadas numericamente [^7], o que pode ser extremamente desafiador [^14]. Uma alternativa é discretizar o processo estocástico usando uma *scenario tree* [^14], embora esta também possa se tornar proibitivamente grande [^14].

#### Independência Estagial (Stagewise Independence)

Uma hipótese que simplifica drasticamente os modelos multiestágio é a **independência estagial** (*stagewise independence*) [^8], [^24]. Esta hipótese assume que a variável aleatória $D_t$ (ou $\\xi_t$ no contexto de portfólio) em cada estágio $t$ é estocasticamente independente do histórico das realizações passadas $D_{[t-1]}$ (ou $\\xi_{[t-1]}$) para $t=2, ..., T$ [^8], [^24].

> **Impacto da Independência Estagial:**
> Sob independência estagial, as esperanças condicionais nas equações de programação dinâmica (e.g., (1.18)-(1.21)) tornam-se esperanças incondicionais [^8], [^25]. Consequentemente, as funções de valor $Q_t$ deixam de depender do histórico $d_{[t-1]}$ e passam a depender apenas do estado atual do sistema, como o nível de estoque $y_t$ ou a riqueza $W_t$ [^9], [^25]. Ou seja, $Q_t(y_t, d_{[t-1]})$ torna-se $Q_t(y_t)$ [^9].

Essa simplificação é imensa do ponto de vista computacional. Como as funções de valor agora dependem apenas de uma variável de estado de baixa dimensão (univariada $y_t$ no exemplo do estoque [^9] ou $W_t$ no portfólio [^25]), elas podem ser calculadas e representadas com precisão, por exemplo, através de discretização do espaço de estados $y_t$ [^9], [^14]. Isso evita a maldição da dimensionalidade associada à dependência do histórico completo [^14].

#### Política Basestock (Basestock Policy)

No contexto de problemas de controle de estoque com custos lineares e sob a hipótese de independência estagial, a programação dinâmica leva a uma política ótima de estrutura simples conhecida como **política basestock** (*basestock policy*) [^13]. Para cada estágio $t$, existe um nível crítico $x_t^*$ que é o minimizador (irrestrito) da função de custo de um período mais o custo futuro esperado [^13]:
$$ x_t^* = \\arg\\min_{x_t} \\left( c_t x_t + \\mathbb{E}\\{b_t[D_t - x_t]_+ + h_t[x_t - D_t]_+ + Q_{t+1}(x_t - D_t)\\} \\right) $$
onde $Q_{t+1}(\\cdot)$ é a função de valor (que depende apenas do estado $y_{t+1} = x_t - D_t$ devido à independência estagial) e $Q_{T+1}(\\cdot) \\equiv 0$ [^13]. A política ótima implementável é então dada por [^13]:
$$ \\mathbf{x}_t(y_t) = \\max\\{y_t, x_t^*\\} $$
Isso significa que, se o nível de estoque atual $y_t$ estiver abaixo do nível basestock $x_t^*$, um pedido é feito para trazer o nível de estoque exatamente para $x_t^*$; caso contrário, nenhum pedido é feito [^13]. É importante notar que um resultado similar (uma política do tipo $\\max\\{y_t, x_t^*\\}$) pode valer mesmo sem independência estagial, mas nesse caso o nível crítico $x_t^*$ dependeria do histórico $d_{[t-1]}$ [^13].

A independência estagial também pode levar a **políticas míopes** (*myopic policies*) em certos casos, como em problemas de seleção de portfólio com funções de utilidade logarítmica ou de potência [^26]. Nesses casos excepcionais, a decisão ótima em cada estágio maximiza apenas a utilidade esperada do próximo período (ou uma função relacionada), sem considerar explicitamente os estágios futuros além do próximo [^26]. No entanto, essa miopia é geralmente destruída pela introdução de fatores mais realistas, como custos de transação [^20].

### Conclusão

Os modelos de programação estocástica multiestágio fornecem uma estrutura rigorosa para a tomada de decisão sequencial sob incerteza. A **restrição de não-antecipatividade** [^2] é um pilar desses modelos, garantindo que as decisões sejam baseadas apenas na informação passada e presente. A **programação dinâmica** oferece a principal ferramenta metodológica para a solução, decompondo o problema através de uma **recursão para trás** (*backward recursion*) que utiliza **funções de valor** para encapsular o impacto futuro das decisões atuais [^5], [^7]. Embora a aplicação geral da programação dinâmica enfrente desafios computacionais significativos devido à **maldição da dimensionalidade** [^14], a hipótese de **independência estagial** [^8], [^24] simplifica consideravelmente o problema, tornando as funções de valor dependentes apenas do estado atual [^9], [^25]. Essa simplificação permite a derivação de políticas ótimas com estruturas tratáveis em certos casos, como a **política basestock** para controle de estoque [^13]. Compreender essas formulações e as condições sob as quais elas podem ser simplificadas é crucial para aplicar a programação estocástica a problemas práticos complexos em finanças, operações e outras áreas.

### Referências

[^1]: (p. 6, Sec 1.2.3) Suppose now that the company has a planning horizon of T periods. We model the demand as a random process Dt indexed by the time t = 1, . . ., T. At the beginning, at t = 1, there is (known) inventory level y1. At each period t = 1, . . ., T, the company first observes the current inventory level yt and then places an order to replenish the inventory level to xt. This results in order quantity xt − yt, which clearly should be nonnegative, i.e., xt ≥ yt. After the inventory is replenished, demand dt is realized, and hence the next inventory level, at the beginning of period t + 1, becomes yt+1 = xt − dt. We allow backlogging, and the inventory level yt may become negative. The total cost incurred in period t is ct(xt − yt) + bt[dt − xt]+ + ht[xt − dt]+, where ct, bt, ht are the ordering, backorder penalty, and holding costs per unit, respectively, at time t. We assume that bt > ct > 0 and ht > 0, t = 1, . . ., T. The objective is to minimize the expected value of the total cost over the planning horizon. This can be written as the following optimization problem: Min_{xt≥yt} E{∑_{t=1}^T [ct(xt - yt) + bt[Dt − xt]+ + ht[xt – Dt]+]} s.t. yt+1 = xt - Dt, t = 1, ..., T – 1. (1.17)
[^2]: (p. 7, Sec 1.2.3) Consider the demand process Dt, t = 1, . . ., T. We denote by D[t] := (D1, . . ., Dt) the history of the demand process up to time t, and by d[t] := (d1, ..., dt) its particular realization. At each period (stage) t, our decision about the inventory level xt should depend only on information available at the time of the decision, i.e., on an observed realization d[t-1] of the demand process, and not on future observations. This principle is called the nonanticipativity constraint. We assume, however, that the probability distribution of the demand process is known. That is, the conditional probability distribution of Dt, given D[t-1] = d[t-1], is assumed to be known.
[^3]: (p. 7, Sec 1.2.3) At the last stage t = T, for observed inventory level yT, we need to solve the problem Min_{xT≥yT} cT(xT - yT) + E {bT[DT - xT]+ + hT[xT - DT]+ | D[T−1] = d[T−1]} . (1.18) The expectation in (1.18) is conditional on the realization d[T−1] of the demand process prior to the considered time T. The optimal value (and the set of optimal solutions) of problem (1.18) depends on yT and d[T-1] and is denoted QT (yT, d[T−1]).
[^4]: (p. 7, Sec 1.2.3) At stage t = T – 1 we solve the problem Min_{xT-1≥yT-1} cT-1(xT−1 − yT−1) + E{bT-1[DT-1 - xT-1]+ + hT-1[xT-1 - DT-1]+ + QT (xT−1 - DT-1, D[T-1]) | D[T−2] = d[T−2]} . (1.19) Its optimal value is denoted QT-1(yT−1, d[T−2]).
[^5]: (p. 7, Sec 1.2.3) Proceeding in this way backward in time, we write the following dynamic programming equations: Qt(yt, d[t-1]) = min_{xt≥yt} ct(xt - yt) + E{bt[Dt – xt]+ + ht[xt – Dt]+ + Qt+1 (xt - Dt, D[t]) | D[t-1] = d[t-1]}, t = T - 1, . . ., 2.
[^6]: (p. 7, Sec 1.2.3) Finally, at the first stage we need to solve the problem Min_{x1≥y1} c1(x1 - y1) + E{b1[D1 − x1]+ + h1[x1 − D1]+ + Q2 (x1 - D1, D1) }. (1.21)
[^7]: (p. 7, Sec 1.2.3) Let us take a closer look at the above decision process. We need to understand how the dynamic programming equations (1.19)–(1.21) could be solved and what is the meaning of the solutions. Starting with the last stage, t = T, we need to calculate the value functions Qt (yt, d[t−1]) going backward in time. In the present case, the value functions cannot be calculated in a closed form and should be approximated numerically. For a generally distributed demand process, this could be very difficult or even impossible.
[^8]: (p. 7, Sec 1.2.3) The situation simplifies dramatically if we assume that the random process Dt is stagewise independent, that is, if Dt is independent of D[t−1], t = 2,..., T. Then the conditional expectations in equations (1.18)–(1.19) become the corresponding unconditional expectations.
[^9]: (p. 7, Sec 1.2.3) Consequently, the value functions Qt (yt) do not depend on demand realizations and become functions of the respective univariate variables yt only. In that case, by discretization of yt and the (one-dimensional) distribution of Dt, these value functions can be calculated in a recursive way.
[^10]: (p. 8, Sec 1.2.3) Suppose now that somehow we can solve the dynamic programming equations (1.19)–(1.21). ... We see that xt is a function of yt and d[t-1] for t = 2, ..., T, while the first stage (optimal) decision x1 is independent of the data. ... Therefore, we may think about a sequence of possible decisions xt = xt(d[t−1]), t = 1, ..., T, as functions of realizations of the demand process available at the time of the decision (with the convention that x1 is independent of the data). Such a sequence of decisions xt(d[t-1]) is called an implementable policy, or simply a policy. That is, an implementable policy is a rule which specifies our decisions, based on information available at the current stage, for any possible realization of the demand process. By definition, an implementable policy xt = xt(d[t-1]) satisfies the nonanticipativity constraint. A policy is said to be feasible if it satisfies other constraints with probability one (w.p. 1). In the present case, a policy is feasible if xt ≥ yt, t = 1, ..., T, for almost every realization of the demand process.
[^11]: (p. 8, Sec 1.2.3) We can now formulate the optimization problem (1.17) as the problem of minimization of the expectation in (1.17) with respect to all implementable feasible policies. An optimal solution of such problem will give us an optimal policy. We have that a policy xt is optimal if it is given by optimal solutions of the respective dynamic programming equations.
[^12]: (p. 8, Sec 1.2.3) Note again that under the assumption of stagewise independence, an optimal policy xt = xt(yt) is a function of yt alone.
[^13]: (p. 8, Sec 1.2.3) Moreover, in that case it is possible to give the following characterization of the optimal policy. Let x*t be an (unconstrained) minimizer of ct xt + E{bt[Dt − xt]+ + ht[xt − Dt]+ + Qt+1 (xt − Dt) }, t = T, ..., 1, (1.22) with the convention that QT+1(·) = 0. Since Qt+1(·) is nonnegative valued and ct +ht > 0, we have that the function in (1.22) tends to +∞ if xt → +∞. Similarly, as bt > ct, it also tends to +∞ if xt → -∞. Moreover, this function is convex and continuous (as long as it is real valued) and hence attains its minimal value. Then by using convexity of the value functions, it is not difficult to show that xt = max{yt, x*t} is an optimal policy. Such policy is called the basestock policy. A similar result holds without the assumption of stagewise independence, but then the critical values x*t depend on realizations of the demand process up to time t − 1.
[^14]: (p. 8, Sec 1.2.3) As mentioned above, if the stagewise independence condition is satisfied, then each value function Qt (yt) is a function of the variable yt. In that case, we can accurately represent Qt(.) by discretization, i.e., by specifying its values at a finite number of points on the real line. Consequently, the corresponding dynamic programming equations can be accurately solved recursively going backward in time. The situation starts to change dramatically with an increase of the number of variables on which the value functions depend, like in the example discussed in the next section. The discretization approach may still work with several state variables, but it quickly becomes impractical when the dimension of the state vector increases. This is called the "curse of dimensionality." As we shall see it later, stochastic programming approaches the problem in a different way, by exploring convexity of the underlying problem and thus attempting to solve problems with a state vector of high dimension. This is achieved by means of discretization of the random process Dt in a form of a scenario tree, which may also become prohibitively large.
[^15]: (p. 12, Sec 1.3.3) Consider now the situation when the manufacturer has a planning horizon of T periods. The demand is modeled as a stochastic process Dt, t = 1,...,T, where each Dt = (D1t, ..., Dnt) is a random vector of demands for the products. ... Let us denote by xt−1 = (xt−1,1, ..., xt−1,n) the vector of quantities ordered at the beginning of stage t, before the demand vector Dt becomes known. The numbers of units produced in stage t will be denoted by zt and the inventory level of parts at the end of stage t by yt for t = 1,..., T. We use the subscript t − 1 for the order quantity to stress that it may depend on the past demand realizations D[t-1] but not on Dt, while the production and storage variables at stage t may depend on D[t], which includes Dt.
[^16]: (p. 12, Sec 1.3.3) Suppose T > 1 and consider the last stage t = T, after the demand DT has been observed. At this time, all inventory levels yT-1 of the parts, as well as the last order quantities xT-1, are known. The problem at stage T is therefore identical to the second-stage problem (1.23) of the two-stage formulation: Min_{zT,yT} (l – q)T zT – sT yT s.t. yT = yT−1 + xT-1 - ATzT, 0 ≤ zT ≤ dT, yT ≥ 0, (1.30) where dT is the observed realization of DT. Denote by QT (xT−1, yT−1, dT) the optimal value of (1.30).
[^17]: (p. 12, Sec 1.3.3) This optimal value depends on the latest inventory levels, order quantities, and the present demand. At stage T − 1 we know realization d[T-1] of D[T-1], and thus we are concerned with the conditional expectation of the last stage cost, that is, the function Q̄T (xT−1, yT−1, d[T−1]) := E{QT(xT−1, yT−1, DT) | D[T−1] = d[T−1]}. At stage T-1 we solve the problem Min_{zT-1, yT-1, xT-1} (l − q)T zT−1 + hT yT−1 + cT xT−1 + Q̄T (xT−1, yT−1, d[T−1]) s.t. yT−1 = yT−2 + xT-2 - ATzT−1, 0 ≤ zT-1 ≤ dT-1, yT-1 ≥ 0. (1.31)
[^18]: (p. 13, Sec 1.3.3) Its optimal value is denoted by QT−1(xT−2, yT−2, d[T-1]). Generally, the problem at stage t = T - 1, ..., 1 has the form Min_{zt, yt, xt} (l - q)T zt + hT yt + cT xt + Q̄t+1(xt, yt, d[t]) s.t. yt = yt−1 + xt−1 - ATzt, 0 ≤ zt ≤ dt, yt ≥ 0, (1.32) with Q̄t+1(xt, yt, d[t]) := E{Qt+1(xt, yt, D[t+1]) | D[t] = d[t]}. The optimal value of problem (1.32) is denoted by Qt(xt−1, yt-1, d[t]), and the backward recursion continues.
[^19]: (p. 13, Sec 1.3.3) At stage t = 1, the symbol y0 represents the initial inventory levels of the parts, and the optimal value function Q1(x0, d1) depends only on the initial order x0 and realization d1 of the first demand D1. The initial problem is to determine the first order quantities x0. It can be written as Min_{x0≥0} cT x0 + E[Q1(x0, D1)]. (1.33) Although the first-stage problem (1.33) looks similar to the first-stage problem (1.24) of the two-stage formulation, it is essentially different since the function Q1(x0, d1) is not given in a computationally accessible form but in itself is a result of recursive optimization.
[^20]: (p. 17, Sec 1.4.2) At time period t = 1 we can rebalance the portfolio by specifying the amounts x1 = (x11, ..., xn1) invested in the respective assets. At that time, we already know the actual returns in the first period, so it is reasonable to use this information in the rebalancing decisions. Thus, our second-stage decisions, at time t = 1, are actually functions of realizations of the random data vector ξ1, i.e., x1 = x1(ξ1). Similarly, at time t our decision xt = (x1t, ..., xnt) is a function xt = xt(ξ[t]) of the available information given by realization ξ[t] = (ξ1, ..., ξt) of the data process up to time t. A sequence of specific functions xt = xt(ξ[t]), t = 0, 1, . . ., T − 1, with x0 being constant, defines an implementable policy of the decision process. It is said that such policy is feasible if it satisfies w.p. 1 the model constraints, i.e., the nonnegativity constraints xit(ξ[t]) ≥ 0, i = 1, ..., n, t = 0, . . ., T − 1, and the balance of wealth constraints ∑_{i=1}^n xit(ξ[t]) = Wt.
[^21]: (p. 17, Sec 1.4.2) Suppose our objective is to maximize the expected utility of this wealth at the last period, that is, we consider the problem Max E[U(WT)]. (1.49) It is a multistage stochastic programming problem, where stages are numbered from t = 0 to t = T - 1. Optimization is performed over all implementable and feasible policies. Of course, in order to complete the description of the problem, we need to define the probability distribution of the random process R1,..., RT. This can be done in many different ways. For example, one can construct a particular scenario tree defining time evolution of the process.
[^22]: (p. 17, Sec 1.4.2) In order to write dynamic programming equations, let us consider the above multistage problem backward in time. At the last stage t = T-1, a realization ξ[T-1] = (ξ1,..., ξT−1) of the random process is known and xT-2 has been chosen. Therefore, we have to solve the problem Max_{xT-1≥0, WT} E{U[WT] | ξ[T-1]} s.t. WT = ∑_{i=1}^n ξiT xi,T-1, ∑_{i=1}^n xi,T-1 = WT-1. (1.50)
[^23]: (p. 18, Sec 1.4.2) where E{U[WT]|ξ[T−1]} denotes the conditional expectation of U[WT] given ξ[T−1]. The optimal value of the above problem (1.50) depends on WT−1 and ξ[T−1] and is denoted QT-1(WT-1, ξ[T−1]). Continuing in this way, at stage t = T - 2, ..., 1, we consider the problem Max_{xt≥0, Wt+1} E[Qt+1(Wt+1, ξ[t+1])|ξ[t]] s.t. Wt+1 = ∑_{i=1}^n ξi,t+1 xi,t, ∑_{i=1}^n xi,t = Wt, (1.51) whose optimal value is denoted Qt(Wt, ξ[t]). Finally, at stage t = 0 we solve the problem Max_{x0≥0, W1} E[Q1(W1, ξ1)] s.t. W1 = ∑_{i=1}^n ξi1 xi0, ∑_{i=1}^n xi0 = W0. (1.52)
[^24]: (p. 18, Sec 1.4.2) For a general distribution of the data process ξt, it may be hard to solve these dynamic programming equations. The situation simplifies dramatically if the process ξt is stagewise independent, i.e., ξt is (stochastically) independent of ξ1, ..., ξt−1 for t = 2,..., T. Of course, the assumption of stagewise independence is not very realistic in financial models, but it is instructive to see the dramatic simplifications it allows.
[^25]: (p. 18, Sec 1.4.2) In that case, the corresponding conditional expectations become unconditional expectations, and the cost-to-go (value) function Qt(Wt), t = 1, . . .,T − 1, does not depend on ξ[t]. That is, QT-1(WT-1) is the optimal value of the problem Max_{xT-1≥0, WT} E{U[WT]} s.t. WT = ∑_{i=1}^n ξiT xi,T-1, ∑_{i=1}^n xi,T-1 = WT-1, and Qt(Wt) is the optimal value of Max_{xt≥0, Wt+1} E{Qt+1(Wt+1)} s.t. Wt+1 = ∑_{i=1}^n ξi,t+1 xi,t, ∑_{i=1}^n xi,t = Wt for t = T − 2, ..., 1.
[^26]: (p. 18-20) The other relevant question is what utility function to use. Let us consider the logarithmic utility function U(W) := ln W. ... For the logarithmic utility function, this implies the following relation between the optimal values ... QT-1(αw, ξ[T−1]) = QT-1(w, ξ[T−1]) + ln a. (1.53) ... it suffices to solve at each stage t = T – 1, . . ., 1, 0, the corresponding optimization problem Max_{xt≥0} E[ln(∑_{i=1}^n ξi,t+1 xi,t) | ξ[t]] s.t. ∑_{i=1}^n xit = Wt (1.56) in a completely myopic fashion. ... Consider now the power utility function U(W) := Wγ with 1 > γ > 0 ... Suppose again that the random process ξt is stagewise independent. ... it is not difficult to show that QT-1(WT-1) = WγT−1 QT-1(1), and so on. The optimal policy xt = xt(Wt) is obtained in a myopic way as an optimal solution of the problem Max_{xt≥0} E[(∑_{i=1}^n ξi,t+1 xi,t)γ] s.t. ∑_{i=1}^n xit = Wt. (1.60) ... The above myopic behavior of multistage stochastic programs is rather exceptional. A more realistic situation occurs in the presence